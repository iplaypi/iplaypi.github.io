<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>虾丸派</title>
  
  <subtitle>烂笔头</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.playpi.org/"/>
  <updated>2020-03-02T12:03:44.000Z</updated>
  <id>https://www.playpi.org/</id>
  
  <author>
    <name>虾丸派</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>在 Elasticsearch 中指定查询返回的字段</title>
    <link href="https://www.playpi.org/2020030201.html"/>
    <id>https://www.playpi.org/2020030201.html</id>
    <published>2020-03-02T12:03:44.000Z</published>
    <updated>2020-03-02T12:03:44.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>在 <code>Elasticsearch</code> 中，有参数可以指定查询结果返回的字段，这样可以使查询结果更简约，看起来更清晰。如果是大批量 <code>scroll</code> 取数，还可以减少数据在网络中的传输，从而降低网络 <code>IO</code>。本文使用简单的查询来举例，演示环境基于 <code>Elasticsearch v5.6.8</code>。</p><a id="more"></a><h1 id="演示"><a href="# 演示" class="headerlink" title="演示"></a>演示 </h1><p> 我的演示环境里面有一个索引 <code>my-index-user</code>，里面是用户的信息，字段有姓名、年龄、性别、城市等。</p><p>现在我根据用户 <code>id</code> 查询数据，使用 <code>_source</code> 参数指定返回 4 个字段：<code>item_id</code>、<code>gender</code>、<code>city</code>、<code>birthday</code>。</p><p>查询条件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">POST my-index-user/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;terms&quot;: &#123;</span><br><span class="line">      &quot;item_id&quot;: [</span><br><span class="line">        &quot;63639783663&quot;,</span><br><span class="line">        &quot;59956667929&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;_source&quot;: [</span><br><span class="line">    &quot;item_id&quot;,</span><br><span class="line">    &quot;gender&quot;,</span><br><span class="line">    &quot;city&quot;,</span><br><span class="line">    &quot;birthday&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查询结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;took&quot;: 2,</span><br><span class="line">  &quot;timed_out&quot;: false,</span><br><span class="line">  &quot;_shards&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 3,</span><br><span class="line">    &quot;successful&quot;: 3,</span><br><span class="line">    &quot;skipped&quot;: 0,</span><br><span class="line">    &quot;failed&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;hits&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 2,</span><br><span class="line">    &quot;max_score&quot;: 7.937136,</span><br><span class="line">    &quot;hits&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">        &quot;_type&quot;: &quot;user&quot;,</span><br><span class="line">        &quot;_id&quot;: &quot;23c659fde1a2c02b3618eaa92fcd7106&quot;,</span><br><span class="line">        &quot;_score&quot;: 7.937136,</span><br><span class="line">        &quot;_source&quot;: &#123;</span><br><span class="line">          &quot;birthday&quot;: &quot;1994-01-01&quot;,</span><br><span class="line">          &quot;city&quot;: &quot; 成都 & quot;,</span><br><span class="line">          &quot;item_id&quot;: &quot;63639783663&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">        &quot;_type&quot;: &quot;user&quot;,</span><br><span class="line">        &quot;_id&quot;: &quot;75e3db1f4ab288d38de3ab80bfba8ecd&quot;,</span><br><span class="line">        &quot;_score&quot;: 7.937136,</span><br><span class="line">        &quot;_source&quot;: &#123;</span><br><span class="line">          &quot;birthday&quot;: &quot;1982-01-01&quot;,</span><br><span class="line">          &quot;gender&quot;: &quot;1&quot;,</span><br><span class="line">          &quot;city&quot;: &quot; 渭南 & quot;,</span><br><span class="line">          &quot;item_id&quot;: &quot;59956667929&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200302205417.png" alt="查询结果指定字段" title="查询结果指定字段"></p><p>可以看到，查到的数据只返回了 4 个字段。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 除了 <code>_source</code> 参数外，还有其它的参数也可以达到同样的效果，在 <code>v2.4</code> 以及之前的版本，可以使用 <code>fields</code> 参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">POST my-index-user/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;terms&quot;: &#123;</span><br><span class="line">      &quot;item_id&quot;: [</span><br><span class="line">        &quot;63639783663&quot;,</span><br><span class="line">        &quot;59956667929&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;fields&quot;: [</span><br><span class="line">    &quot;user_name&quot;,</span><br><span class="line">    &quot;item_id&quot;,</span><br><span class="line">    &quot;gender&quot;,</span><br><span class="line">    &quot;city&quot;,</span><br><span class="line">    &quot;birthday&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下图是我找了一个低版本 <code>Elasticsearch</code> 集群测试了一下：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200302205758.png" alt="fields 参数过滤字段" title="fields 参数过滤字段"></p><p>不过在 <code>v5.x</code> 以及之后的版本不再支持这个参数：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200302205500.png" alt="不支持 fields 参数" title="不支持 fields 参数"></p><p>异常信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The field [fields] is no longer supported, please use [stored_fields] to retrieve stored fields or _source filtering if the field is not stored</span><br></pre></td></tr></table></figure><p>注意这里提及的 <code>stored_fields</code> 参数用处有点鸡肋，还是需要 <code>_source</code> 参数配合。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在 &lt;code&gt;Elasticsearch&lt;/code&gt; 中，有参数可以指定查询结果返回的字段，这样可以使查询结果更简约，看起来更清晰。如果是大批量 &lt;code&gt;scroll&lt;/code&gt; 取数，还可以减少数据在网络中的传输，从而降低网络 &lt;code&gt;IO&lt;/code&gt;。本文使用简单的查询来举例，演示环境基于 &lt;code&gt;Elasticsearch v5.6.8&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="fields" scheme="https://www.playpi.org/tags/fields/"/>
    
      <category term="source" scheme="https://www.playpi.org/tags/source/"/>
    
      <category term="stored_fields" scheme="https://www.playpi.org/tags/stored-fields/"/>
    
  </entry>
  
  <entry>
    <title>Spark 警告：Not enough space to cache rdd in memory</title>
    <link href="https://www.playpi.org/2020012201.html"/>
    <id>https://www.playpi.org/2020012201.html</id>
    <published>2020-01-21T17:58:33.000Z</published>
    <updated>2020-01-21T17:58:33.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>在常规的 <code>Spark</code> 任务中，出现警告：<code>Not enough space to cache rdd_0_255 in memory! (computed 8.3 MB so far)</code>，接着任务就卡住，等了很久最终 <code>Spark</code> 任务失败。排查到原因是 <code>RDD</code> 缓存的时候内存不够，无法继续处理数据，等待资源释放，最终导致假死现象。本文中的开发环境基于 <code>Spark v1.6.2</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 在服务器上面执行一个简单的 <code>Spark</code> 任务，代码逻辑里面有 <code>rdd.cache ()</code> 操作，结果在日志中出现类似如下的警告：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">01:35:42.207 [Executor task launch worker-4] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_28 as it would require dropping another block from the same RDD</span><br><span class="line">01:35:42.211 [Executor task launch worker-4] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_28 in memory! (computed 340.2 MB so far)</span><br><span class="line">01:35:42.213 [Executor task launch worker-4] INFO  org.apache.spark.storage.MemoryStore - Memory use = 8.3 KB (blocks) + 4.9 GB (scratch space shared across 106 tasks (s)) = 4.9 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:35:49.104 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_15 as it would require dropping another block from the same RDD</span><br><span class="line">01:35:49.105 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_15 in memory! (computed 341.4 MB so far)</span><br><span class="line">01:35:49.105 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 8.3 KB (blocks) + 4.9 GB (scratch space shared across 106 tasks (s)) = 4.9 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:35:51.375 [Executor task launch worker-11] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_33 as it would require dropping another block from the same RDD</span><br><span class="line">01:35:51.375 [Executor task launch worker-11] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_33 in memory! (computed 341.4 MB so far)</span><br><span class="line">01:35:51.376 [Executor task launch worker-11] INFO  org.apache.spark.storage.MemoryStore - Memory use = 8.3 KB (blocks) + 4.9 GB (scratch space shared across 106 tasks (s)) = 4.9 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:35:52.188 [Executor task launch worker-12] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_48 as it would require dropping another block from the same RDD</span><br><span class="line">01:35:52.188 [Executor task launch worker-12] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_48 in memory! (computed 341.4 MB so far)</span><br><span class="line">01:35:52.189 [Executor task launch worker-12] INFO  org.apache.spark.storage.MemoryStore - Memory use = 8.3 KB (blocks) + 4.9 GB (scratch space shared across 106 tasks (s)) = 4.9 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:35:52.213 [Executor task launch worker-6] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_58 as it would require dropping another block from the same RDD</span><br><span class="line">01:35:52.213 [Executor task launch worker-6] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_58 in memory! (computed 342.6 MB so far)</span><br><span class="line">01:35:52.214 [Executor task launch worker-6] INFO  org.apache.spark.storage.MemoryStore - Memory use = 8.3 KB (blocks) + 4.9 GB (scratch space shared across 106 tasks (s)) = 4.9 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:35:56.619 [Executor task launch worker-2] INFO  org.apache.spark.storage.MemoryStore - Block rdd_0_41 stored as values in memory (estimated size 378.7 MB, free 378.7 MB)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200122021258.png" alt="Storage 内存不足警告" title="Storage 内存不足警告"></p><p>看起来这只是一个警告，显示 <code>Storage</code> 内存不足，无法进行 <code>rdd.cache ()</code>，等待一段时间之后，<code>Spark</code> 任务的部分 <code>Task</code> 可以接着运行。</p><p>但是后续还是会发生同样的事情：内存不足，导致 <code>Task</code> 一直在等待，最后假死【或者说 <code>Spark</code> 任务基本卡住不动】。</p><p>里面有一个明显的提示：<code>Storage limit = 5.0 GB.</code>，也就是 <code>Storage</code> 的上限是 <code>5GB</code>。</p><h1 id="问题分析解决"><a href="# 问题分析解决" class="headerlink" title="问题分析解决"></a>问题分析解决 </h1><p> 查看业务代码，里面有一个：<code>rdd.cache ();</code> 操作，显然会占用大量的内存。</p><p>查看官方文档的配置：<a href="https://spark.apache.org/docs/1.6.2/configuration.html" target="_blank" rel="noopener">1.6.2-configuration</a> ，里面有一个重要的参数：<code>spark.storage.memoryFraction</code>，它是一个系数，决定着缓存上限的大小【基数是 <code>spark.excutor.memory</code>】。</p><blockquote><p>(deprecated) This is read only if spark.memory.useLegacyMode is enabled. Fraction of Java heap to use for Spark’s memory cache. This should not be larger than the “old” generation of objects in the JVM, which by default is given 0.6 of the heap, but you can increase it if you configure your own old generation size.</p></blockquote><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200122023358.png" alt="memoryFraction 参数" title="memoryFraction 参数"></p><p>另外还有 2 个相关的参数，读者也可以了解一下。</p><p>读者可以注意到，官方是不建议使用这个参数的，也就是不建议变更。当然如果你非要使用也是可以的，可以提高系数的值，这样的话缓存的空间就会变多。显然这样做不合理。</p><p>那有没有别的方法了呢？有！当然有。</p><p>主要是从缓存的方式入手，不要直接使用 <code>rdd.cache ()</code>，而是通过序列化 <code>RDD</code> 数据：<code>rdd.persist (StorageLevel.MEMORY_ONLY_SER)</code>，减少空间的占用，或者直接缓存一部分数据到磁盘：<code>rdd.persist (StorageLevel.MEMORY_AND_DISK)</code>，避免内存不足。</p><p>我下面演示使用后者，即直接缓存一部分数据到磁盘，当然，使用这种方式，<code>Spark</code> 任务执行速度肯定是慢了不少。</p><p>我这里测试后，得到的结果：耗时是以前的 3 倍【可以接受】。</p><p>再接着执行 <code>Spark</code> 任务，日志中还是会出现上述警告：<code>Not enough space to cache rdd in memory!</code>，但是接着会提示数据被缓存到磁盘了：<code>Persisting partition rdd_0_342 to disk instead.</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">01:55:24.414 [Executor task launch worker-3] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_342 as it would require dropping another block from the same RDD</span><br><span class="line">01:55:24.414 [Executor task launch worker-3] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_342 in memory! (computed 96.8 MB so far)</span><br><span class="line">01:55:24.414 [Executor task launch worker-3] INFO  org.apache.spark.storage.MemoryStore - Memory use = 3.0 GB (blocks) + 2.0 GB (scratch space shared across 339 tasks (s)) = 5.0 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:55:24.414 [Executor task launch worker-3] WARN  org.apache.spark.CacheManager - Persisting partition rdd_0_342 to disk instead.</span><br><span class="line">01:55:33.262 [Executor task launch worker-12] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_229 as it would require dropping another block from the same RDD</span><br><span class="line">01:55:33.262 [Executor task launch worker-12] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_229 in memory! (computed 342.6 MB so far)</span><br><span class="line">01:55:33.262 [Executor task launch worker-12] INFO  org.apache.spark.storage.MemoryStore - Memory use = 3.0 GB (blocks) + 2.0 GB (scratch space shared across 339 tasks (s)) = 5.0 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:55:33.262 [Executor task launch worker-12] WARN  org.apache.spark.CacheManager - Persisting partition rdd_0_229 to disk instead.</span><br><span class="line">01:55:40.247 [Executor task launch worker-13] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_254 as it would require dropping another block from the same RDD</span><br><span class="line">01:55:40.248 [Executor task launch worker-13] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_254 in memory! (computed 18.0 MB so far)</span><br><span class="line">01:55:40.248 [Executor task launch worker-13] INFO  org.apache.spark.storage.MemoryStore - Memory use = 3.0 GB (blocks) + 2.0 GB (scratch space shared across 339 tasks (s)) = 5.0 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:55:40.248 [Executor task launch worker-13] WARN  org.apache.spark.CacheManager - Persisting partition rdd_0_254 to disk instead.</span><br><span class="line">01:56:28.062 [dispatcher-event-loop-9] INFO  o.a.spark.storage.BlockManagerInfo - Added rdd_0_255 on disk on localhost:55066 (size: 146.4 MB)</span><br><span class="line">01:56:28.194 [Executor task launch worker-1] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_255 as it would require dropping another block from the same RDD</span><br><span class="line">01:56:28.194 [Executor task launch worker-1] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_255 in memory! (computed 8.3 MB so far)</span><br><span class="line">01:56:28.194 [Executor task launch worker-1] INFO  org.apache.spark.storage.MemoryStore - Memory use = 3.0 GB (blocks) + 2.0 GB (scratch space shared across 339 tasks (s)) = 5.0 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:56:28.194 [Executor task launch worker-1] INFO  o.apache.spark.storage.BlockManager - Found block rdd_0_255 locally</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200122024437.png" alt="一部分数据被缓存到磁盘" title="一部分数据被缓存到磁盘"></p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 综上所述，有三种方式可以解决这个问题：</p><ul><li>提高缓存空间系数：<code>spark.storage.memoryFraction</code>【或者增大 <code>spark.excutor.memory</code>，不建议】</li><li>使用序列化 <code>RDD</code> 数据的方式：<code>rdd.persist (StorageLevel.MEMORY_ONLY_SER)</code></li><li>使用磁盘缓存的方式：<code>rdd.persist (StorageLevel.MEMORY_AND_DISK)</code></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在常规的 &lt;code&gt;Spark&lt;/code&gt; 任务中，出现警告：&lt;code&gt;Not enough space to cache rdd_0_255 in memory! (computed 8.3 MB so far)&lt;/code&gt;，接着任务就卡住，等了很久最终 &lt;code&gt;Spark&lt;/code&gt; 任务失败。排查到原因是 &lt;code&gt;RDD&lt;/code&gt; 缓存的时候内存不够，无法继续处理数据，等待资源释放，最终导致假死现象。本文中的开发环境基于 &lt;code&gt;Spark v1.6.2&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Spark" scheme="https://www.playpi.org/tags/Spark/"/>
    
      <category term="cache" scheme="https://www.playpi.org/tags/cache/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch 的 Reindex API 详解</title>
    <link href="https://www.playpi.org/2020011601.html"/>
    <id>https://www.playpi.org/2020011601.html</id>
    <published>2020-01-16T12:15:12.000Z</published>
    <updated>2020-01-16T12:15:12.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --><p>在业务中只要有使用到 <code>Elasticsearch</code> 的场景，那么有时候会遇到需要重构索引的情况，例如 <code>mapping</code> 被污染了、某个字段需要变更类型等。如果对 <code>reindex API</code> 不熟悉，那么在遇到重构的时候，必然事倍功半，效率低下。但是如果熟悉了，就可以方便地进行索引重构，省时省力。</p><p>本文演示内容基于 <code>Elasticsearch v5.6.8</code>，在以后会不断补充完善。</p><a id="more"></a><h1 id="常用方式"><a href="# 常用方式" class="headerlink" title="常用方式"></a>常用方式 </h1><p> 提前声明，在开始演示具体的 <code>API</code> 的时候，有一点读者必须知道，<code>reindex</code> 不会尝试自动设置目标索引，它也不会复制源索引的设置。读者应该在运行 <code>reindex</code> 操作之前设置好目标索引的参数，包括映射、分片数、副本数等等。目标索引如果不设置 <code>mapping</code>，则会使用默认的配置，默认配置不会自动处理一些有特殊要求的字段【例如分词字段、数值类型字段】，则会引发字段类型错误的结果。</p><p>当然，关于设置索引，最常见的做法不是手动设置索引信息，而是使用索引模版【使用动态模版：<code>dynamic_templates</code>】，只要索引模版的匹配形式可以匹配上源索引和目标索引，我们不需要去考虑索引配置的问题，模版会为我们解决对应的问题。当然，关于的索引的分片数、副本数，还是需要考虑的。</p><p>最后，关于版本的说明，以下内容只针对 <code>v5.x</code> 以及之后的版本，更多版本的使用方式读者可以参考文末的备注信息。</p><h2 id="迁移数据简单示例"><a href="# 迁移数据简单示例" class="headerlink" title="迁移数据简单示例"></a>迁移数据简单示例 </h2><p> 涉及到 <code>_reindex</code> 关键字，简单示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行返回结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;took&quot;: 9991,</span><br><span class="line">  &quot;timed_out&quot;: false,</span><br><span class="line">  &quot;total&quot;: 12505,</span><br><span class="line">  &quot;updated&quot;: 12505,</span><br><span class="line">  &quot;created&quot;: 0,</span><br><span class="line">  &quot;deleted&quot;: 0,</span><br><span class="line">  &quot;batches&quot;: 13,</span><br><span class="line">  &quot;version_conflicts&quot;: 0,</span><br><span class="line">  &quot;noops&quot;: 0,</span><br><span class="line">  &quot;retries&quot;: &#123;</span><br><span class="line">    &quot;bulk&quot;: 0,</span><br><span class="line">    &quot;search&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;throttled_millis&quot;: 0,</span><br><span class="line">  &quot;requests_per_second&quot;: -1,</span><br><span class="line">  &quot;throttled_until_millis&quot;: 0,</span><br><span class="line">  &quot;failures&quot;: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200203234351.png" alt="演示结果" title="演示结果"></p><p>演示结果迁移了 12505 条数据，耗时 9991 毫秒。</p><p>以上只是一个非常基础的示例，还有一些可以优化的参数没有指定，全部使用的是默认值，例如看到 <code>batches</code>、<code>total</code> 对应的数值，就可以算出一批的数据大小默认为 1000，12505 条数据需要 13 批次才能迁移完成。再看到 <code>updated</code> 的数值，就可以猜测是更新了目标索引的数据，而不是创建数据，说明目标索引本来就有数据，被重新覆盖了。而这些内容在后续的小节中都会逐一解释，并再次演示。</p><p>注意一点，如果 <code>Elasticsearch</code> 是 <code>v6.x</code> 以及以下的版本，会涉及到索引的 <code>type</code>，如果源索引只有一个 <code>type</code>，则可以省略 <code>type</code> 这个参数，即不需要指定【数据会迁移到目标索引的同名 <code>type</code> 里面】。但是，如果涉及到多个 <code>type</code> 的数据迁移，肯定是要指定的【例如把多个 <code>type</code> 的数据迁移到同一个 <code>type</code> 中，或者仅仅把某个 <code>type</code> 的数据迁移到另外一个 <code>type</code> 中】。因此，为了准确无误，最好还是指定 <code>type</code>【当然再高的版本就没有 <code>type</code> 的概念了，无需指定】。</p><p>如果根据上面的示例，再添加 <code>type</code> 参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;user&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="version-type- 参数"><a href="#version-type- 参数" class="headerlink" title="version_type 参数"></a>version_type 参数 </h2><p> 就像 <code>_update_by_query</code> 里面的逻辑一样，<code>_reindex</code> 会生成源索引的快照【<code>snapshot</code>】，但是它的目标索引必须是一个不同的索引【另外创建一个】，以便避免版本冲突问题。同时，针对 <code>dest index</code> 可以像 <code>index API</code>【索引数据】 一样进行配置，以乐观锁控制并发写入【并发写入相同的数据，通过 <code>version</code> 来控制，有冲突时会写入失败，可以重试】。</p><p>像上面那样最简单的方式，不设置 <code>version_type</code> 参数【默认为 <code>internal</code>】或显式设置它为 <code>internal</code>，效果都一样。此时，<code>Elasticsearch</code> 将会直接将文档转储到 <code>dest index</code> 中，直接覆盖任何具有相同类型和 <code>id</code> 的 <code>document</code>，不会产生冲突问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;version_type&quot;: &quot;internal&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果把 <code>version_type</code> 设置为 <code>external</code>，则 <code>Elasticsearch</code> 会从 <code>source</code> 读取 <code>version</code> 字段，当遇到具有相同类型和 <code>id</code> 的 <code>document</code> 时，只会保留 <code>newer verion</code>，即最新的 <code>version</code> 对应的数据。此时可能会有冲突产生【例如把 <code>op_type</code> 设置为 <code>create</code>】，对于产生的冲突现象，返回体中的 <code>failures</code> 会携带冲突的数据信息【类似详细的日志可以查看】。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;version_type&quot;: &quot;external&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的说法看起来似乎有点不好理解，再简单直观点来说，就是在 <code>redinex</code> 的时候，我们的 <code>dest index</code> 可以不是一个新创建的不包含数据的 <code>index</code>，而是已经包含有数据的。如果我们的 <code>source index</code> 和 <code>dest index</code> 里面有相同类型和 <code>id</code> 的 <code>document</code>【一模一样的数据】，对于使用 <code>internal</code> 来说，就是直接覆盖，而使用 <code>external</code> 的话，只有当 <code>source index</code> 的数据的 <code>version</code> 比 <code>dest index</code> 数据的 <code>version</code> 更加新的时候，才会去更新【即保留最新的 <code>version</code> 对应的数据】。</p><p>再说明一下，<code>internal</code> 可以理解为使用内部版本号，即 <code>Elasticsearch</code> 不会单独比较版本号，对于 <code>dest index</code> 来说，无论是索引数据还是更新数据，写入时都按部就班把版本号累加，所以也就不会有冲突问题【从 <code>source index</code> 出来的数据是不携带版本信息的】，但是有可能会出现版本号不合法的问题，参考后面的 <strong>使用脚本配置 </strong>小节【使用脚本时人为变更版本号】。</p><p>另一方面，<code>external</code> 表示外部版本号，即 <code>Elasticsearch</code> 会单独比较版本号再决定写入的流程，对于 <code>dest index</code> 来说，无论是索引数据还是更新数据，写入时会先比较版本号，只保留版本号最大的数据【如果是来自不同索引的数据，版本号会不一致；如果是来自不同集群的数据，版本号规则可能也不一致】。</p><h2 id="op-type- 参数"><a href="#op-type- 参数" class="headerlink" title="op_type 参数"></a>op_type 参数 </h2><p><code>op_type</code> 参数控制着写入数据的冲突处理方式，如果把 <code>op_type</code> 设置为 <code>create</code>【默认值】，在 <code>_reindex API</code> 中，表示写入时只在 <code>dest index</code> 中添加不存在的 <code>doucment</code>，如果相同的 <code>document</code> 已经存在，则会报 <code>version confilct</code> 的错误，那么索引操作就会失败。【这种方式与使用 <code>_create API</code> 时效果一致】</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;op_type&quot;: &quot;create&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 如果这样设置了，也就不存在更新数据的场景了【冲突数据无法写入】，则 <code>version_type</code> 参数的设置也就无所谓了【但是返回体的 <code>failures</code> 中还是会携带冲突信息】。</p><p>另外也可以把 <code>op_type</code> 设置为 <code>index</code>，表示所有的数据全部重新索引创建。</p><p>再总结一下，如果把 <code>version_type</code> 设置为 <code>external</code>，无论 <code>op_type</code> 怎么设置，都有可能产生冲突【会比较版本】；如果把 <code>version_type</code> 设置为 <code>internal</code>，则在 <code>op_type</code> 为 <code>index</code> 的时候不会产生冲突，在 <code>op_type</code> 为 <code>create</code> 的时候可能有冲突。</p><h2 id="conflicts- 配置"><a href="#conflicts- 配置" class="headerlink" title="conflicts 配置"></a>conflicts 配置 </h2><p> 默认情况下，当发生 <code>version conflict</code> 的时候，<code>_reindex</code> 会被 <code>abort</code>，任务终止【此时数据还没有 <code>reindex</code> 完成】，在返回体中的 <code>failures</code> 指标中会包含冲突的数据【有时候数据会非常多】，除非把 <code>conflicts</code> 设置为 <code>proceed</code>。</p><p>关于 <code>abort</code> 的说明，如果产生了 <code>abort</code>，已经执行的数据【例如更新写入的】仍然存在于目标索引，此时任务终止，还会有数据没有被执行，也就是漏数了。换句话说，该执行过程不会回滚，只会终止。如果设置了 <code>proceed</code>，任务在检测到数据冲突的情况下，不会终止，会跳过冲突数据继续执行，直到所有数据执行完成，此时不会漏掉正常的数据，只会漏掉有冲突的数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;op_type&quot;: &quot;create&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;conflicts&quot;: &quot;proceed&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>故意把 <code>op_type</code> 设置为 <code>create</code>，人为制造数据冲突的场景，测试时更容易观察到冲突现象。</p><p>如果把 <code>conflicts</code> 设置为 <code>proceed</code>，在返回体结果中不会再出现 <code>failures</code> 的信息，但是通过 <code>version_conflicts</code> 指标可以看到具体的数量。</p><h2 id="query- 配置"><a href="#query- 配置" class="headerlink" title="query 配置"></a>query 配置 </h2><p> 迁移数据的时候，肯定有只是复制源索引中部分数据的场景，此时就需要配置查询条件【使用 <code>query</code> 参数】，只复制命中条件的部分数据，而不是全部，这和查询 <code>Elasticsearch</code> 数据的逻辑一致。</p><p>如下示例，通过 <code>query</code> 参数，把需要 <code>_reindex</code> 的 <code>document</code> 限定在一定的范围，我这里是限定更新时间 <code>update_timestamp</code> 在 1546272000000【20190101000000】之后。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">      &quot;bool&quot;: &#123;</span><br><span class="line">        &quot;must&quot;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;range&quot;: &#123;</span><br><span class="line">              &quot;update_timestamp&quot;: &#123;</span><br><span class="line">                &quot;gte&quot;: 1546272000000</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="批次大小配置"><a href="# 批次大小配置" class="headerlink" title="批次大小配置"></a>批次大小配置 </h2><p> 如果在 <code>query</code> 参数的同一层次【即 <code>source</code> 参数中】再添加 <code>size</code> 参数，并不是表示随机获取部分数据，而是表示 <code>scroll size</code> 的大小【会影响批次的次数，进而影响整体的速度】，这个有点迷惑人。另外，直接在 <code>query</code> 中添加 <code>size</code> 参数是不被允许的。</p><p>如果不显式设置，默认是一批 1000 条数据，在一开始的简单示例中也看到了。</p><p>如下，设置 <code>scroll size</code> 为 100：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">      &quot;bool&quot;: &#123;</span><br><span class="line">        &quot;must&quot;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;range&quot;: &#123;</span><br><span class="line">              &quot;update_timestamp&quot;: &#123;</span><br><span class="line">                &quot;gte&quot;: 1546272000000</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;size&quot;: 100</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200205001736.png" alt="返回结果" title="返回结果"></p><p>根据返回结果可以看到，实际迁移 12505 条数据，<code>batches</code> 为 126【可以算出每次 <code>batch</code> 为 100 条数据】，耗时为 31868 毫秒，是前面简单示例耗时的 3 倍【前面简单示例的耗时是 10 秒左右】。</p><p>注意，千万不要用错 <code>size</code> 参数的位置，可以继续参考下面的 <strong>随机 size 配置 </strong>小节。</p><h2 id="多对一迁移"><a href="# 多对一迁移" class="headerlink" title="多对一迁移"></a>多对一迁移 </h2><p> 如果需要把多个索引的数据或者多个 <code>type</code> 的数据共同迁移到同一个目标索引中，则需要在 <code>source</code> 参数中指定多个索引。</p><p>把同一个索引中的不同 <code>type</code> 的数据共同迁移到同一个索引中，例如把 <code>my-index-user</code> 下的 <code>user</code>、<code>user2</code> 数据共同迁移到 <code>my-index-user-v2</code> 中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: [</span><br><span class="line">      &quot;my-index-user&quot;,</span><br><span class="line">      &quot;my-index-user&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;type&quot;: [</span><br><span class="line">      &quot;user&quot;,</span><br><span class="line">      &quot;user2&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>把不同索引中的数据共同迁移到同一个索引中，例如把 <code>my-index-user</code> 下的 <code>user</code>、<code>my-index-user-v2</code> 下的 <code>user2</code> 数据共同迁移到 <code>my-index-user-v3</code> 中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: [</span><br><span class="line">      &quot;my-index-user&quot;,</span><br><span class="line">      &quot;my-index-user-v2&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;type&quot;: [</span><br><span class="line">      &quot;user&quot;,</span><br><span class="line">      &quot;user2&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v3&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里要注意的是，对于上面的第二个示例，如果 <code>my-index-user</code> 和 <code>my-index-user-v2</code> 中有 <code>document</code> 的 <code>id</code> 是一样的，则无法保证最终出现在 <code>my-index-user-v3</code> 里面的 <code>document</code> 是哪个，因为迭代是随机的。按照默认的配置【即前面的 <code>version_type</code>、<code>opt_type</code> 等参数】，最后一条数据会覆盖前一条数据。</p><p>当然，对于第一个示例也会有这个问题。</p><p>另外，不要想着一对多迁移、多对多迁移等操作，不支持，因为写入时必须指定唯一确定的索引，否则 <code>Elasticsearch</code> 不知道数据要往哪个索引里面写入。</p><h2 id="随机 -size- 配置"><a href="# 随机 -size- 配置" class="headerlink" title="随机 size 配置"></a>随机 size 配置 </h2><p> 有时候想提前测试一下迁移结果是否准确，或者使用少量数据做一下性能测试，则需要随机取数的配置，可以使用 <code>size</code> 参数。注意，<code>size</code> 参数的位置是与 <code>source</code>、<code>dest</code> 在同一层级的，即在最外层。</p><p>例如随机取数 100 条，示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200205004107.png" alt="随机 100 条数据结果" title="随机 100 条数据结果"></p><p>可以看到，随机迁移了 100 条数据，耗时 223 毫秒。</p><p>注意这里，千万不要用错 <code>size</code> 参数的位置，一个是表示 <code>scroll size</code> 的大小【如上面的 <strong>query 配置 </strong>中，配置在与 <code>query</code> 同一个层级，在 <code>source</code> 里面】，一个是表示随机抽取多少条【本小节示例，配置在最外层】。</p><p>我曾经就把 <code>size</code> 设置错误，放在与 <code>query</code> 同一层级，也就是误把 <code>scroll size</code> 设置为 10 了【本来是想先迁移 10 条数据看看结果】，导致 <code>reindex</code> 速度非常慢，30 分钟才 20 万数据量【我还在疑惑为什么设置的随机 10 条不生效，把全部数据都迁移了】。</p><p>后来发现了，把任务取消，重新提交 <code>reindex</code> 任务，准确地把随机 <code>size</code> 设置为 10 万，把 <code>scroll size</code> 设置为 2000。测试后才正式开始迁移数据，速度达到了 30 分钟 500 万，和前面的误操作比较明显提升了很多。</p><h2 id="排序配置"><a href="# 排序配置" class="headerlink" title="排序配置"></a>排序配置 </h2><p> 好，上面有了随机数据条数的设置，但是如果我们想根据某个字段进行排序，获取 <code>top 100</code>，有没有办法呢？有，当然有，可以使用排序的功能，关键字为 <code>sort</code>，使用方式和查询 <code>Elasticsearch</code> 时一致。</p><p>例如按照更新时间 <code>update_timestamp</code> 的降序排列，获取前 100 条数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;sort&quot;: &#123;</span><br><span class="line">      &quot;update_timestamp&quot;: &quot;desc&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200205005139.png" alt="返回结果" title="返回结果"></p><h2 id="指定字段配置"><a href="# 指定字段配置" class="headerlink" title="指定字段配置"></a>指定字段配置 </h2><p> 如果迁移数据时只需要特定的字段，可以使用 <code>_source</code> 参数指定字段，字段少了迁移速度也可以提升。下面的示例指定了 3 个字段：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;_source&quot;: [</span><br><span class="line">      &quot;id&quot;,</span><br><span class="line">      &quot;city_level&quot;,</span><br><span class="line">      &quot;task_ids&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200205235937.png" alt="返回结果" title="返回结果"></p><p>可以看到，迁移 12505 条数据，耗时 3611 毫秒，比前面的简单示例快了不少。</p><p>查看目标数据，可以看到只有 3 个字段【注意，<code>_id</code> 字段是 <code>document</code> 的主键，会自动携带】。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206001202.png" alt="有 3 个字段的数据示例" title="有 3 个字段的数据示例"></p><p>但是有一点需要注意，如果 <code>Elasticsearch</code> 的索引设置中，使用 <code>_source、excludes</code> 排除了部分字段的存储【为了节省磁盘空间】，实际上没有存储字段，只是做了索引，则不可以直接迁移，会丢失这些字段。</p><h2 id="使用脚本配置"><a href="# 使用脚本配置" class="headerlink" title="使用脚本配置"></a>使用脚本配置 </h2><p> 如果集群开启了允许使用 <code>script</code> 的功能【在配置文件 <code>elasticsearch.yml</code> 中使用 <code>script.inline: true</code> 开启】，就可以使用 <code>script</code> 做一些简单的数据转换。</p><p>例如把满足条件的数据做一个 <code>_version</code> 增加，并且移除指定的字段，在写入目标索引时，利用 <code>version_type</code> 参数保留最新版本的数据。</p><p>以下示例为了方便查看结果，只获取 3 个字段，脚本逻辑：对于 <code>city_level</code> 等于 1 的数据【<code>city</code> 为北京、上海、广州、深圳】，做一个版本自增，并且把 <code>city_level</code> 字段移除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;_source&quot;: [</span><br><span class="line">      &quot;id&quot;,</span><br><span class="line">      &quot;city&quot;,</span><br><span class="line">      &quot;city_level&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;version_type&quot;: &quot;external&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;script&quot;: &#123;</span><br><span class="line">    &quot;source&quot;: &quot;if (ctx._source.city_level == &apos;1&apos;) &#123;ctx._version++; ctx._source.remove (&apos;city_level&apos;)&#125;&quot;,</span><br><span class="line">    &quot;lang&quot;: &quot;painless&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>数据结果查看：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206012210.png" alt="数据结果查看" title="数据结果查看"></p><p>数据结果中可以看到数据的 <code>city_level</code> 字段已经消失了，只剩下 2 个字段。</p><p>再执行一次，如果数据相同，会有冲突问题，因为设置了 <code>version_type</code> 为 <code>external</code>，会比较版本【数据的版本不够新从而无法写入】。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">         &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">         &quot;type&quot;: &quot;user&quot;,</span><br><span class="line">         &quot;id&quot;: &quot;AW3zlTvZa9C6UomAXwqT&quot;,</span><br><span class="line">         &quot;cause&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,</span><br><span class="line">            &quot;reason&quot;: &quot;[user][AW3zlTvZa9C6UomAXwqT]: version conflict, current version [3] is higher or equal to the one provided [1]&quot;,</span><br><span class="line">            &quot;index_uuid&quot;: &quot;tJienWj1T_udvoJQTcDzyg&quot;,</span><br><span class="line">            &quot;shard&quot;: &quot;1&quot;,</span><br><span class="line">            &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">         &#125;,</span><br><span class="line">         &quot;status&quot;: 409</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206012415.png" alt="冲突现象" title="冲突现象"></p><p>如果把 <code>version_type</code> 设置为 <code>internal</code>，同时指定 <code>op_type</code> 为 <code>index</code>【默认是 <code>create</code>】，则会出现版本号不合法的异常。因为在脚本中手动自增了版本号，不符合按照 <code>index</code> 方式索引数据的要求。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">action_request_validation_exception</span><br><span class="line">Validation Failed: 1: illegal version value [0] for version type [INTERNAL];</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206012755.png" alt="版本号不合法" title="版本号不合法"></p><p>在此引申一下脚本的内容，<code>Elasticsearch</code> 提供了脚本的支持，可以通过 <code>Groovy</code> 外置脚本【已经过时，<code>v6.x</code> 以及之后的版本，不建议使用】、内置 <code>painless</code> 脚本实现各种复杂的操作【类似于写逻辑代码，对数据进行 <code>ETL</code> 操作】，如上面的示例。</p><p><code>painless</code> 有轻便之意，使用时直接在语法中调用即可，无需外置，也就是不支持通过外部文件存储 <code>painless</code> 脚本来调用。</p><blockquote><p>默认的脚本语言是 Groovy，一种快速表达的脚本语言，在语法上与 JavaScript 类似。它在 Elasticsearch v1.3.0 版本首次引入并运行在沙盒中，然而 Groovy 脚本引擎存在漏洞，允许攻击者通过构建 Groovy 脚本，在 Elasticsearch Java VM 运行时脱离沙盒并执行 shell 命令。<br>因此，在版本 v1.3.8、v1.4.3 和 v1.5.0 及更高的版本中，它已经被默认禁用。此外，您可以通过设置集群中的所有节点的 config/elasticsearch.yml 文件来禁用动态 Groovy 脚本：script.groovy.sandbox.enabled: false，这将关闭 Groovy 沙盒，从而防止动态 Groovy 脚本作为请求的一部分被接受。</p></blockquote><p>当然，对于常用的脚本，可以通过 <code>_scripts/calculate-score</code> 接口创建后缓存起来【也需要集群的配置：<code>script.store: true</code>】，会生成一个唯一 <code>id</code>，下次可以直接使用【就像声明了一个方法】，还支持参数传递。</p><h2 id="使用 -Ingest-Node- 配置"><a href="# 使用 -Ingest-Node- 配置" class="headerlink" title="使用 Ingest Node 配置"></a>使用 Ingest Node 配置 </h2><p><code>Ingest</code> 其实就是定义了一些预处理的规则，可以预处理数据，提升性能，主要依靠 <code>Pipeline</code>、<code>Processors</code>。当然前提还是需要集群支持，定义一些 <code>Ingest</code> 节点、预处理流程，可以通过配置 <code>elasticsearch.xml</code> 文件中的 <code>node.ingest: true</code> 来开启 <code>Ingest</code> 节点。</p><p> 这个功能应该说是最好用的了，当你的 <code>source</code> 因为不合理的结构，需要重新结构化所有的数据时，通过 <code>Ingest Node</code>，可以很方便地在新的 <code>index</code> 中获得不一样的 <code>mapping</code> 和 <code>value</code>【例如把数值类型转为字符串类型，或者把值替换掉】。</p><p>使用方式也很简单【需要提前创建 <code>pipeline</code>】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;pipeline&quot;: &quot;some_ingest_pipeline&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果没有创建 <code>pipeline</code>，会报错，在返回体的 <code>failures</code> 中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;type&quot;: &quot;illegal_argument_exception&quot;,</span><br><span class="line">&quot;reason&quot;: &quot;pipeline with id [some_ingest_pipeline] does not exist&quot;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206014248.png" alt="报错信息" title="报错信息"></p><h2 id="迁移远程集群数据到当前环境"><a href="# 迁移远程集群数据到当前环境" class="headerlink" title="迁移远程集群数据到当前环境"></a>迁移远程集群数据到当前环境 </h2><p> 有时候需要跨集群迁移数据，例如把 <code>A</code> 集群的数据复制到 <code>B</code> 集群中，只要 <code>A</code> 集群的节点开放了 <code>ip</code>、端口，就可以使用 <code>remote</code> 参数。</p><p>在 <code>B</code> 集群中也需要设置白名单，在 <code>elasticsearch.xml</code> 文件中配置 <code>reindex.remote.whitelist: otherhost:9200</code> 参数即可，多个使用英文逗号隔开。</p><p>使用示例【如果有认证机制则还需要带上用户名、密码信息】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;remote&quot;: &#123;</span><br><span class="line">      &quot;host&quot;: &quot;http://otherhost:9200&quot;,</span><br><span class="line">      &quot;username&quot;: &quot;username&quot;,</span><br><span class="line">      &quot;password&quot;: &quot;password&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里需要注意，对于复制在其他集群上的 <code>index</code> 数据来说，就不存在直接从本地镜像复制的便利【速度快】，需要从网络上下载数据再写到本地。默认的设置，<code>buffer</code> 的 <code>size</code> 是 <code>100Mb</code>，在 <code>scroll size</code> 是 1000 的情况下【默认值】，如果单个 <code>document</code> 的平均大小超过 <code>100Kb</code>，则会报错，数据过大。</p><p>因此在遇到非常大的 <code>document</code> 时，需要减小 <code>batch</code> 的 <code>size</code>【尽管会导致 <code>batch</code> 变多，迁移速度变慢，但是安全】，使用 <code>size</code> 参数【参考前面的 <strong>批次大小配置 </strong>小节】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;remote&quot;: &#123;</span><br><span class="line">      &quot;host&quot;: &quot;http://otherhost:9200&quot;,</span><br><span class="line">      &quot;username&quot;: &quot;username&quot;,</span><br><span class="line">      &quot;password&quot;: &quot;password&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;size&quot;: 100</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="返回体"><a href="# 返回体" class="headerlink" title="返回体"></a>返回体 </h2><p> 在提交迁移数据任务后，如果耐心等待，会有执行的结果返回，正常情况下，返回的结果格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;took&quot;: 9675,</span><br><span class="line">  &quot;timed_out&quot;: false,</span><br><span class="line">  &quot;total&quot;: 12505,</span><br><span class="line">  &quot;updated&quot;: 12505,</span><br><span class="line">  &quot;created&quot;: 0,</span><br><span class="line">  &quot;deleted&quot;: 0,</span><br><span class="line">  &quot;batches&quot;: 13,</span><br><span class="line">  &quot;version_conflicts&quot;: 0,</span><br><span class="line">  &quot;noops&quot;: 0,</span><br><span class="line">  &quot;retries&quot;: &#123;</span><br><span class="line">    &quot;bulk&quot;: 0,</span><br><span class="line">    &quot;search&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;throttled_millis&quot;: 0,</span><br><span class="line">  &quot;requests_per_second&quot;: -1,</span><br><span class="line">  &quot;throttled_until_millis&quot;: 0,</span><br><span class="line">  &quot;failures&quot;: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206015432.png" alt="返回信息" title="返回信息"></p><p>下面挑选几个指标解释说明一下：</p><ul><li><code>took</code>，整个操作从开始到结束的毫秒数 </li><li><code>total</code>，已成功处理的文档数</li><li><code>updated</code>，已成功更新的文档数</li><li><code>deleted</code>，已成功删除的文档数</li><li><code>batches</code>，由查询更新拉回的滚动响应数【结合 <code>total</code> 可以算出 <code>scroll size</code>】，与 <code>scroll size</code> 有关</li><li><code>version_conflicts</code>，按查询更新的版本冲突数【涉及到版本的比较判断】</li><li><code>retries</code>，逐个更新尝试的重试次数，<code>bulk</code> 是批量操作的重试次数，<code>search</code> 是搜索操作的重试次数</li><li><code>failures</code>，如果在此过程中存在任何不可恢复的错误，发生 <code>abort</code>，则会返回故障信息数组【内容可能会比较多】</li></ul><p> 这里需要注意的是 <code>failures</code> 信息，如果里面的信息不为空，则表示本次 <code>_reindex</code> 是失败的，是被中途 <code>abort</code>，一般都是因为发生了 <code>conflicts</code>。前面已经描述过如何合理设置【业务场景可接受的方式，例如把 <code>conflicts</code> 设置为 <code>proceed</code>】，可以确保在发生 <code>conflict</code> 的时候还能继续运行。但是，这样设置后任务不会被 <code>abort</code>，可以正常执行完成，则最终也就不会返回 <code>failures</code> 信息了，但是通过 <code>version_conflicts</code> 指标可以看到具体的数量。</p><h2 id="查看任务进度以及取消任务"><a href="# 查看任务进度以及取消任务" class="headerlink" title="查看任务进度以及取消任务"></a>查看任务进度以及取消任务 </h2><p> 一般来说，如果我们的 <code>source index</code> 很大【比如几百万数据量】，则可能需要比较长的时间来完成 <code>_reindex</code> 的工作，可能需要几十分钟。而在此期间不可能一直等待结果返回，可以去做其它事情，如果中途需要查看进度，可以通过 <code>_tasks API</code> 进行查看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET _tasks?detailed=true&amp;actions=*reindex</span><br></pre></td></tr></table></figure><p>返回结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;nodes&quot;: &#123;</span><br><span class="line">    &quot;ietwyYpJRo-gz_C1tCbpgQ&quot;: &#123;</span><br><span class="line">      &quot;name&quot;: &quot;dev4_xx&quot;,</span><br><span class="line">      &quot;transport_address&quot;: &quot;xx.xx.xx.204:9308&quot;,</span><br><span class="line">      &quot;host&quot;: &quot;xx.xx.xx.204&quot;,</span><br><span class="line">      &quot;ip&quot;: &quot;xx.xx.xx.204:9308&quot;,</span><br><span class="line">      &quot;roles&quot;: [</span><br><span class="line">        &quot;master&quot;,</span><br><span class="line">        &quot;data&quot;,</span><br><span class="line">        &quot;ingest&quot;</span><br><span class="line">      ],</span><br><span class="line">      &quot;tasks&quot;: &#123;</span><br><span class="line">        &quot;ietwyYpJRo-gz_C1tCbpgQ:420711&quot;: &#123;</span><br><span class="line">          &quot;node&quot;: &quot;ietwyYpJRo-gz_C1tCbpgQ&quot;,</span><br><span class="line">          &quot;id&quot;: 420711,</span><br><span class="line">          &quot;type&quot;: &quot;transport&quot;,</span><br><span class="line">          &quot;action&quot;: &quot;indices:data/write/reindex&quot;,</span><br><span class="line">          &quot;status&quot;: &#123;</span><br><span class="line">            &quot;total&quot;: 12505,</span><br><span class="line">            &quot;updated&quot;: 0,</span><br><span class="line">            &quot;created&quot;: 0,</span><br><span class="line">            &quot;deleted&quot;: 0,</span><br><span class="line">            &quot;batches&quot;: 5,</span><br><span class="line">            &quot;version_conflicts&quot;: 4000,</span><br><span class="line">            &quot;noops&quot;: 0,</span><br><span class="line">            &quot;retries&quot;: &#123;</span><br><span class="line">              &quot;bulk&quot;: 0,</span><br><span class="line">              &quot;search&quot;: 0</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;throttled_millis&quot;: 0,</span><br><span class="line">            &quot;requests_per_second&quot;: -1,</span><br><span class="line">            &quot;throttled_until_millis&quot;: 0</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;description&quot;: &quot;reindex from [my-index-user] to [my-index-user-v2]&quot;,</span><br><span class="line">          &quot;start_time_in_millis&quot;: 1580958992770,</span><br><span class="line">          &quot;running_time_in_nanos&quot;: 1441736539,</span><br><span class="line">          &quot;cancellable&quot;: true</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206113250.png" alt="查看任务状态" title="查看任务状态"></p><p>根据任务的各项指标，就可以预估完成进度，从而预估完成时间，做到心中有数。</p><p>注意观察里面的几个重要指标，例如从 <code>description</code> 中可以看到任务描述，从 <code>tasks</code> 中可以找到任务的 <code>id</code>【例如 <code>ietwyYpJRo-gz_C1tCbpgQ:420711</code>】，从 <code>cancellable</code> 可以判断任务是否支持取消操作。</p><p>这个 <code>API</code> 其实就是模糊匹配，同理也可以查询其它类型的任务信息，例如使用 <code>GET _tasks?detailed=true&amp;actions=*byquery</code> 查看查询请求的状态。</p><p>如果知道了 <code>task_id</code>，也可以使用 <code>GET /_tasks/task_id</code> 更加准确地查询指定的任务状态，避免集群的任务过多，不方便查看。</p><p>如果遇到操作失误的场景，想取消任务，有没有办法呢？有，泼出去的水还是可以收回的，通过 <code>_tasks API</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">POST _tasks/task_id/_cancel</span><br></pre></td></tr></table></figure><p>这里的 <code>task_id</code> 就是通过上面的查询任务接口获取的，另外还需要任务支持取消操作【<code>cancellable</code> 为 <code>true</code>】。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 参考官网：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/docs-reindex.html" target="_blank" rel="noopener">docs-reindex</a> 。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在业务中只要有使用到 &lt;code&gt;Elasticsearch&lt;/code&gt; 的场景，那么有时候会遇到需要重构索引的情况，例如 &lt;code&gt;mapping&lt;/code&gt; 被污染了、某个字段需要变更类型等。如果对 &lt;code&gt;reindex API&lt;/code&gt; 不熟悉，那么在遇到重构的时候，必然事倍功半，效率低下。但是如果熟悉了，就可以方便地进行索引重构，省时省力。&lt;/p&gt;&lt;p&gt;本文演示内容基于 &lt;code&gt;Elasticsearch v5.6.8&lt;/code&gt;，在以后会不断补充完善。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="HTTP" scheme="https://www.playpi.org/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>Spark 异常之 Failed to create local dir</title>
    <link href="https://www.playpi.org/2020010201.html"/>
    <id>https://www.playpi.org/2020010201.html</id>
    <published>2020-01-01T16:32:44.000Z</published>
    <updated>2020-02-10T16:32:44.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>今天的 <code>Spark Streaming</code> 程序莫名出现异常【对于一个 <code>task</code> 来说，<code>Spark Streaming</code> 会重试 4 次，全部重试都失败后整个 <code>Stage</code> 才会失败】，紧接着 <code>task</code> 中的 <code>batch</code> 就会卡住不动【后来查到卡住是其它原因造成的】，使得整个 <code>Spark Streaming</code> 任务进程进入到等待状态，所有的 <code>batch</code> 都处于 <code>queued</code> 状态，数据流无法继续执行。本文内容基于 <code>Spark v1.6.2</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 线上一个一直很稳定的 <code>Spark Streaming</code> 程序，突然出现异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Job aborted due to stage failure: Task 0 in stage 2174.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2174.0 (TID 32656, host): java.io.IOException: Failed to create local dir in /cloud/data2/spark/local/spark-4fccb5c2-29f5-45f9-926e-1c6e33636884/executor-30fdf8f9-6459-43c0-bba5-3a406db7e700/blockmgr-7edadea3-1fa3-4f32-bef2-1cf81230042a/07.</span><br><span class="line">at org.apache.spark.storage.DiskBlockManager.getFile (DiskBlockManager.scala:73)</span><br><span class="line">at org.apache.spark.storage.DiskStore.contains (DiskStore.scala:161)</span><br><span class="line">at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus (BlockManager.scala:398)</span><br><span class="line">at org.apache.spark.storage.BlockManager.doPut (BlockManager.scala:827)</span><br><span class="line">at org.apache.spark.storage.BlockManager.putBytes (BlockManager.scala:700)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$anonfun$$getRemote$1$1.apply (TorrentBroadcast.scala:130)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$anonfun$$getRemote$1$1.apply (TorrentBroadcast.scala:127)</span><br><span class="line">at scala.Option.map (Option.scala:145)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.org$apache$spark$broadcast$TorrentBroadcast$$anonfun$$getRemote$1 (TorrentBroadcast.scala:127)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply (TorrentBroadcast.scala:137)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply (TorrentBroadcast.scala:137)</span><br><span class="line">at scala.Option.orElse (Option.scala:257)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp (TorrentBroadcast.scala:137)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply (TorrentBroadcast.scala:120)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply (TorrentBroadcast.scala:120)</span><br><span class="line">at scala.collection.immutable.List.foreach (List.scala:318)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks (TorrentBroadcast.scala:120)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply (TorrentBroadcast.scala:175)</span><br><span class="line">at org.apache.spark.util.Utils$.tryOrIOException (Utils.scala:1205)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock (TorrentBroadcast.scala:165)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute (TorrentBroadcast.scala:64)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast._value (TorrentBroadcast.scala:64)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast.getValue (TorrentBroadcast.scala:88)</span><br><span class="line">at org.apache.spark.broadcast.Broadcast.value (Broadcast.scala:70)</span><br><span class="line">at org.apache.spark.scheduler.ResultTask.runTask (ResultTask.scala:62)</span><br><span class="line">at org.apache.spark.scheduler.Task.run (Task.scala:89)</span><br><span class="line">at org.apache.spark.executor.Executor$TaskRunner.run (Executor.scala:227)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">Driver stacktrace:</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200211004947.png" alt="异常信息" title="异常信息"></p><p>重点内容在于 <code>java.io.IOException: Failed to create local dir</code>。</p><p>为了不影响业务逻辑，首先尝试重启，重启后短暂恢复正常，大概运行 20-40 分钟后，继续出现上述异常，非常诡异【重启 5 次左右仍旧出现】。</p><p>同时，伴随着部分 <code>Stage</code> 失败，<code>Spark Streaming</code> 任务出现了 <code>batch</code> 卡住的现象，有 2 个 <code>btach</code> 一直处于 <code>processing</code> 状态，极不正常。导致后面所有的 <code>batch</code> 都处于 <code>queued</code> 状态，数据流无法继续执行，最终整个 <code>Spark Streaming</code> 任务会卡住不动，类似于假死。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200211010335.png" alt="batch 卡住" title="batch 卡住"></p><h1 id="问题分析解决"><a href="# 问题分析解决" class="headerlink" title="问题分析解决"></a>问题分析解决 </h1><p> 查了一下文档，发现有两种情况会造成这个异常，一是没有权限写入磁盘，二是磁盘空间不足。</p><p>找运维人员协助查看了一下，服务器的磁盘没有问题，根据进程的用户判断权限也没有问题，而且有好几个其它类似的 <code>Spark Straming</code> 任务可以正常运行，一点问题没有。</p><p>于是回顾一下最近的代码变动，发现有问题的 <code>Spark Streaming</code> 任务都变更过一个第三方接口的调用，于是联系对应的开发人员。</p><p>经过沟通测试，发现了问题，第三方接口有一个异常没有捕捉，导致上述异常产生。同时，由此会导致资源不释放的 <code>bug</code>，进而影响了 <code>Spark Streaming</code> 任务的 <code>batch</code> 卡住。</p><p>以上这些问题都与 <code>Spark</code> 无关，后面紧急升级第三方接口，任务得以正常运行，后续又观察了三天，都没有问题。</p><h1 id="深入探究"><a href="# 深入探究" class="headerlink" title="深入探究"></a>深入探究 </h1><p> 问题虽然解决了，但还是要关注一下这个异常的场景，通过查看源码，发现这个异常是在创建目录的时候产生的，如下图：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200211011126.png" alt="查看源码" title="查看源码"></p><p>那这个场景就很简单了，如果进程没有写入磁盘的权限或者磁盘空间不足，都会产生这个异常。</p><p>进一步思考，为什么会创建这个目录，作用是什么呢？</p><p>原来，<code>Spark</code> 在 <code>shuffle</code> 时需要通过 <code>diskBlockManage</code> 将 <code>map</code> 结果写入本地，优先写入 <code>memory store</code>，在 <code>memore store</code> 空间不足时会创建临时文件。这是一个二级目录，如异常中的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/cloud/data2/spark/local/spark-4fccb5c2-29f5-45f9-926e-1c6e33636884/executor-30fdf8f9-6459-43c0-bba5-3a406db7e700/blockmgr-7edadea3-1fa3-4f32-bef2-1cf81230042a/07</span><br></pre></td></tr></table></figure><p>使用完成后会立即删除。</p><p>那 <code>shuffle</code> 又是咋回事呢？<code>Spark</code> 作为并行计算框架，同一个作业会被划分为多个任务在多个节点上执行，<code>reduce</code> 的输入可能存在于多个节点，因此需要 <code>shuffle</code> 将所有 <code>reduce</code> 的输入汇总起来。这一步比较消耗内存或者说是磁盘，视选择的缓存方式而定。</p><p>那上面的 <code>memory store</code> 的大小是多少，什么情况下会超出上限从而选择使用 <code>disk store</code>？其实，<code>memory store</code> 的大小取决于 <code>spark.excutor.memory</code> 的大小，默认为 <code>spark.excutor.memory * 0.6</code>。此外，官方是不建议更改 0.6 这个系数的【参数：<code>spark.storage.memoryFraction</code>】，参考：<a href="https://spark.apache.org/docs/1.6.2/configuration.html" target="_blank" rel="noopener">configuration-1.6.2</a> 。</p><p>那临时文件的目录，可以更改吗，例如磁盘空间不足后，新挂载了一块磁盘。是可以的，在 <code>spark.env</code> 中添加 <code>SPARK_LOCAL_DIRS</code> 变量即可【通过 <code>spark-env.sh</code> 脚本可以添加】，或者直接在程序中配置【<code>spark conf</code>，参数名是 <code>spark.local.dir</code>】，可配置多个路径，使用英文逗号分隔，这样可以增强 <code>IO</code> 效率。这个参数的官方说明如下，默认目录是 <code>/tmp</code>。</p><blockquote><p>Directory to use for “scratch” space in Spark, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. NOTE: In Spark 1.0 and later this will be overriden by SPARK_LOCAL_DIRS (Standalone, Mesos) or LOCAL_DIRS (YARN) environment variables set by the cluster manager.</p></blockquote><p>综上所述，在生产环境中一定要确保磁盘空间充足和磁盘写权限，切记磁盘空间按需配置，不可乱用，运维侧也要加上磁盘相关的监控，有问题可以及时预警。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 异常参考：<a href="https://stackoverflow.com/questions/41238121/spark-java-ioexception-failed-to-create-local-dir-in-tmp-blockmgr" target="_blank" rel="noopener">stackoverflow</a> 。</p><p>官方文档参考：<a href="https://spark.apache.org/docs/1.6.2/configuration.html" target="_blank" rel="noopener">configuration-1.6.2</a> 。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;今天的 &lt;code&gt;Spark Streaming&lt;/code&gt; 程序莫名出现异常【对于一个 &lt;code&gt;task&lt;/code&gt; 来说，&lt;code&gt;Spark Streaming&lt;/code&gt; 会重试 4 次，全部重试都失败后整个 &lt;code&gt;Stage&lt;/code&gt; 才会失败】，紧接着 &lt;code&gt;task&lt;/code&gt; 中的 &lt;code&gt;batch&lt;/code&gt; 就会卡住不动【后来查到卡住是其它原因造成的】，使得整个 &lt;code&gt;Spark Streaming&lt;/code&gt; 任务进程进入到等待状态，所有的 &lt;code&gt;batch&lt;/code&gt; 都处于 &lt;code&gt;queued&lt;/code&gt; 状态，数据流无法继续执行。本文内容基于 &lt;code&gt;Spark v1.6.2&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Spark" scheme="https://www.playpi.org/tags/Spark/"/>
    
      <category term="Streaming" scheme="https://www.playpi.org/tags/Streaming/"/>
    
      <category term="IOException" scheme="https://www.playpi.org/tags/IOException/"/>
    
  </entry>
  
  <entry>
    <title>写入 Elasticsearch 异常：413 Request Entity Too Large</title>
    <link href="https://www.playpi.org/2019122901.html"/>
    <id>https://www.playpi.org/2019122901.html</id>
    <published>2019-12-29T15:53:41.000Z</published>
    <updated>2019-12-29T15:53:41.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>在一个非常简单的业务场景中，偶尔出现异常：<code>413 Request Entity Too Large</code>，业务场景是写入数据到 <code>Elasticsearch</code> 中，异常日志中还有 <code>Nginx</code> 字样。</p><p>本文记录排查过程，本文环境基于 <code>Elasticsearch v5.6.8</code>，使用的写入客户端是 <code>elasticsearch-rest-high-level-client-5.6.8.jar</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 在后台日志中，发现异常信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">19/12/29 23:06:41 ERROR ESBulkProcessor: bulk [2307 : 1577632001467] 1000 request - 0 response - Unable to parse response body</span><br><span class="line">ElasticsearchStatusException [Unable to parse response body]; nested: ResponseException [POST http://your_ip_address:9200/_bulk?timeout=1m: HTTP/1.1 413 Request Entity Too Large</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;&lt;title&gt;413 Request Entity Too Large&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;center&gt;&lt;h1&gt;413 Request Entity Too Large&lt;/h1&gt;&lt;/center&gt;</span><br><span class="line">&lt;hr&gt;&lt;center&gt;nginx/1.16.1&lt;/center&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br><span class="line">];</span><br><span class="line">at org.elasticsearch.client.RestHighLevelClient.parseResponseException (RestHighLevelClient.java:506)</span><br><span class="line">at org.elasticsearch.client.RestHighLevelClient$1.onFailure (RestHighLevelClient.java:477)</span><br><span class="line">at org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onDefinitiveFailure (RestClient.java:605)</span><br><span class="line">at org.elasticsearch.client.RestClient$1.completed (RestClient.java:362)</span><br><span class="line">at org.elasticsearch.client.RestClient$1.completed (RestClient.java:343)</span><br><span class="line">at org.apache.http.concurrent.BasicFuture.completed (BasicFuture.java:115)</span><br><span class="line">at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted (DefaultClientExchangeHandlerImpl.java:173)</span><br><span class="line">at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse (HttpAsyncRequestExecutor.java:355)</span><br><span class="line">at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady (HttpAsyncRequestExecutor.java:242)</span><br><span class="line">at org.apache.http.impl.nio.client.LoggingAsyncRequestExecutor.inputReady (LoggingAsyncRequestExecutor.java:87)</span><br><span class="line">at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput (DefaultNHttpClientConnection.java:264)</span><br><span class="line">at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady (InternalIODispatch.java:73)</span><br><span class="line">at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady (InternalIODispatch.java:37)</span><br><span class="line">at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady (AbstractIODispatch.java:113)</span><br><span class="line">at org.apache.http.impl.nio.reactor.BaseIOReactor.readable (BaseIOReactor.java:159)</span><br><span class="line">at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent (AbstractIOReactor.java:338)</span><br><span class="line">at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents (AbstractIOReactor.java:316)</span><br><span class="line">at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute (AbstractIOReactor.java:277)</span><br><span class="line">at org.apache.http.impl.nio.reactor.BaseIOReactor.execute (BaseIOReactor.java:105)</span><br><span class="line">at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run (AbstractMultiworkerIOReactor.java:584)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">Suppressed: java.lang.IllegalStateException: Unsupported Content-Type: text/html</span><br><span class="line">at org.elasticsearch.client.RestHighLevelClient.parseEntity (RestHighLevelClient.java:523)</span><br><span class="line">at org.elasticsearch.client.RestHighLevelClient.parseResponseException (RestHighLevelClient.java:502)</span><br><span class="line">... 20 more</span><br><span class="line">Caused by: org.elasticsearch.client.ResponseException: POST http://your_ip_address:9200/_bulk?timeout=1m: HTTP/1.1 413 Request Entity Too Large</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;&lt;title&gt;413 Request Entity Too Large&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;center&gt;&lt;h1&gt;413 Request Entity Too Large&lt;/h1&gt;&lt;/center&gt;</span><br><span class="line">&lt;hr&gt;&lt;center&gt;nginx/1.16.1&lt;/center&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br><span class="line"></span><br><span class="line">at org.elasticsearch.client.RestClient$1.completed (RestClient.java:354)</span><br><span class="line">... 17 more</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191230002704.jpg" alt="异常信息" title="异常信息"></p><p>留意重点内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;&lt;title&gt;413 Request Entity Too Large&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;center&gt;&lt;h1&gt;413 Request Entity Too Large&lt;/h1&gt;&lt;/center&gt;</span><br><span class="line">&lt;hr&gt;&lt;center&gt;nginx/1.16.1&lt;/center&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>看起来是发送的 <code>HTTP</code> 请求的请求体【<code>body</code>】过大，超过了服务端 <code>Nginx</code> 的配置，导致返回异常。</p><p>这个请求体过大，本质上还是将要写入 <code>Elasticsearch</code> 的文档过大，可见是某个字段的取值过大【这种情况一般都是异常数据导致的，例如采集系统把整个网页的内容全部抓回来作为正文，或者把网站反扒的干扰长文本全部抓回来作为正文】。</p><p>但是我又不禁想，这个配置参数名是什么呢？限制的最大字节数是多少呢？</p><h1 id="问题排查解决"><a href="# 问题排查解决" class="headerlink" title="问题排查解决"></a>问题排查解决 </h1><p> 在 <code>Elasticsearch</code> 官网查看相关配置项，发现有一个参数：<code>http.max_content_length</code>，表示一个 <code>HTTP</code> 请求的内容大小上限，默认为 <code>100MB</code>【对于 <code>v5.6</code> 来说】。</p><p>官网地址：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-http.html" target="_blank" rel="noopener">elasticsearch 关于 HTTP 的配置 </a> ，以下为参数说明：</p><blockquote><p>The max content of an HTTP request. Defaults to 100mb. If set to greater than Integer.MAX_VALUE, it will be reset to 100mb.</p></blockquote><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191230005435.jpg" alt="HTTP 相关配置" title="HTTP 相关配置"></p><p> 这个参数是配置在 <code>elasticsearch.yml</code> 配置文件中的，我查看了我使用的 <code>Elasticsearch</code> 集群中对应的配置，没有发现参数的设置，说明使用了默认配置。</p><p>其实，<code>100MB</code> 对于文本来说很大了，一般正常的文本也不过只有几 <code>KB</code> 大小，对于长一点的文本来说，例如几万个字符，也就是几百 <code>KB</code>。由于写入 <code>Elasticsearch</code> 是批量的，1000 条数据一批，如果一批里面包含的全部是长文本，还是有可能超过 <code>100MB</code> 的，可见调整 <code>HTTP</code> 请求大小的上限是有必要的，或者是降低批次的数据量【会影响写入性能】。</p><p>此外，关于 <code>HTTP</code> 的另外两个参数也值得关注：<code>http.max_initial_line_length</code>、<code>http.max_header_size</code>。</p><p>前者表示 <code>HTTP</code> 请求链接的长度，默认为 <code>4KB</code>：</p><blockquote><p>The max length of an HTTP URL. Defaults to 4kB</p></blockquote><p>后者表示 <code>HTTP</code> 请求头的大小上限，默认为 <code>8KB</code>：</p><blockquote><p>The max size of allowed headers. Defaults to 8kB</p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在一个非常简单的业务场景中，偶尔出现异常：&lt;code&gt;413 Request Entity Too Large&lt;/code&gt;，业务场景是写入数据到 &lt;code&gt;Elasticsearch&lt;/code&gt; 中，异常日志中还有 &lt;code&gt;Nginx&lt;/code&gt; 字样。&lt;/p&gt;&lt;p&gt;本文记录排查过程，本文环境基于 &lt;code&gt;Elasticsearch v5.6.8&lt;/code&gt;，使用的写入客户端是 &lt;code&gt;elasticsearch-rest-high-level-client-5.6.8.jar&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="HTTP" scheme="https://www.playpi.org/tags/HTTP/"/>
    
      <category term="RestHighLevelClient" scheme="https://www.playpi.org/tags/RestHighLevelClient/"/>
    
  </entry>
  
  <entry>
    <title>大数据平台框架常用参数优化</title>
    <link href="https://www.playpi.org/2019121901.html"/>
    <id>https://www.playpi.org/2019121901.html</id>
    <published>2019-12-19T14:54:12.000Z</published>
    <updated>2019-12-19T14:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --><p>本文记录大数据平台框架的一些常用参数，这些参数基本是我见过的或者实际使用过的，我会列出参数的含义以及使用效果，具有一定的参考意义。当然，根据实际的场景不同，参数值并不能随便设置为一样，必须要考虑到实际的情况，否则可能没有效果，或者具有反作用。</p><p>会保持更新。</p><a id="more"></a><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><p>选择 <code>HBase</code>、<code>Hadoop</code> 时注意版本适配的问题，<code>Hadoop</code> 选择 <code>v2.7.1</code> 还是很好的，能适配 <code>HBase v1.2.x</code> 以及以上的版本【<code>Hbase</code> 兼容的 <code>Hadoop</code> 版本参见：<a href="http://hbase.apache.org/book.html#configuration" target="_blank" rel="noopener">hbase-configuration</a> 】，也能适配 <code>Hive v0.10.0</code> 以及以上的版本【<code>Hive</code> 兼容的 <code>Hadoop</code> 版本参见：<a href="http://hive.apache.org/downloads.html" target="_blank" rel="noopener">hive-downloads</a> 】。</p><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><ul><li><code>fs.hdfs.impl.disable.cache</code>，如果设置为 <code>true</code>，表示关闭文件系统的缓存，这样多线程手动处理 <code>HDFS</code> 文件时，不会 <code>IOException: Filesystem closed</code></li></ul><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><ul><li><code>map</code> 并发大小：<code>mapreduce.job.running.map.limit</code>，可以设置大点，50、100 随便 </li><li><code>map</code> 内存大小：<code>mapreduce.map.memory.mb</code>，单位为 <code>MB</code>，一般 <code>4GB</code> 够用</li><li><code>reduce</code> 启动延迟：<code>mapred.reduce.slowstart.completed.maps</code>，表示 <code>reduce</code> 在 <code>map</code> 执行到什么程度可以启动，例如设置为 <code>1.0</code> 表示等待 <code>map</code> 全部完成后才能执行 <code>reduce</code></li><li><code>reduce</code> 内存大小：<code>mapreduce.reduce.memory.mb</code>，单位为 <code>MB</code>，要根据实际情况设置，一般 <code>4GB</code> 够用</li><li><code>reduce</code> 虚拟内存：<code>yarn.nodemanager.vmem-pmem-ratio</code>，一般 2-5 即可</li><li><code>reduce</code> 并发大小：<code>mapreduce.job.running.reduce.limit</code>，一般 5-10 个够用【根据业务场景、机器资源而定】</li></ul><h2 id="es-hadoop"><a href="#es-hadoop" class="headerlink" title="es-hadoop"></a>es-hadoop</h2><p> 使用 <code>es-hadoop</code> 框架处理 <code>Elasticsearch</code> 数据，可以专注于数据 <code>ETL</code> 处理逻辑，其它与集群交互的读写操作交给 <code>es-hadoop</code> 框架处理，这里面有一些常用的参数。</p><p>参考官方文档：<a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html" target="_blank" rel="noopener">configuration</a> 。</p><ul><li>读取，只读取指定的字段：<code>es.read.field.include</code>，默认为空，读取全部字段，注意，在 <code>query</code> 中设置 <code>_source</code> 是无效的 </li><li> 读取，排除指定的字段：<code>es.read.field.exclude</code>，默认为空，则不排除任何字段 </li><li> 读取，关闭日期的处理：<code>es.mapping.date.rich</code>，默认为 <code>true</code>，关闭后，读取 <code>Elasticsearch</code> 的 <code>date</code> 类型的字段，会自动转换为 <code>long</code> 类型，不再是 <code>date</code> 类型 </li><li> 读取，解析指定字段为数组类型：<code>es.read.field.as.array.include</code>，默认为空，则不解析任何字段【字段类型保持原样】</li><li>读取，排除解析指定字段为数组字段：<code>es.read.field.as.array.exclude</code>，默认为空，则不排除任何字段【字段该是数组的还是数组，不是数组的仍旧保持原样】</li></ul><h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><p>待整理 </p><h1 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h1><p> 总述，在设置 <code>Elasticsearch</code> 堆大小时需要通过 <code>$ES_HEAP_SIZE</code> 环境变量，遵循两个规则：</p><ul><li>不要超过可用 <code>RAM</code> 的 50%，<code>Lucene</code> 能很好利用文件系统的缓存，它是通过系统内核管理的，如果没有足够的文件系统缓存空间，性能会受到影响。 此外，专门用于堆的内存越多意味着其它可用的内存越少，例如 <code>fielddata</code></li><li>不要超过 <code>32GB</code>，如果堆大小小于 <code>32GB</code>，<code>JVM</code> 可以利用指针压缩，这可以大大降低内存的使用，每个指针是 4 字节而不是 8 字节 </li></ul><p> 分片的分配：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/shards-allocation.html" target="_blank" rel="noopener">shards-allocation</a> 。<br>脚本的使用：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html" target="_blank" rel="noopener">modules-scripting-using</a> 。<br>熔断器相关：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/circuit-breaker.html" target="_blank" rel="noopener">circuit-breaker</a> 。<br>节点选举、故障检测：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.8/modules-discovery-zen.html" target="_blank" rel="noopener">modules-discovery-zen</a> 。</p><p><code>fielddata</code>，对字段进行 <code>agg</code> 时，会把数据加载到内存中【索引数据时不会】，记录的是占用内存空间情况，超过指定的值，开始回收内存，防止 <code>OOM</code>。</p><p><code>allocate</code> 表示分片的复制分配；<code>relocate</code> 表示分片再次进行 <code>allocate</code>。<code>Recovery</code> 表示将一个索引的未分配 <code>shard</code> <code>allocate</code> 到一个结点的过程，在快照恢复、更改索引副本数量、结点故障、结点启动时发生。</p><p>如果设置索引副本数为 1，同一个索引的主分片、副本分片不会被分配在同一个节点上面，这才能保证数据高可用，挂了一个节点也没关系【如果一台物理节点开启了两个 <code>Elasticsearch</code> 节点，需要注意使用 <code>cluster.routing.allocation.same_shard.host</code> 参数】。</p><ul><li>磁盘空间使用占比上限：<code>cluster.routing.allocation.disk.watermark.high</code>，默认为 90%，表示如果当前节点的磁盘使用占比超过这个值，则分片【针对所有类型的分片：主分片、副本分片】会被自动 <code>relocate</code> 到其它节点，并且任何分片都不会 <code>allocate</code> 到当前节点【此外，对于新创建的 <code>primary</code> 分片也是如此，尽管不是 <code>allocate</code> 动作，除非整个 <code>Elasticsearch</code> 集群只有一个节点】</li><li>磁盘空间使用占比下限：<code>cluster.routing.allocation.disk.watermark.low</code>，默认为 85%，表示如果当前节点的磁盘使用占比超过这个值，则分片【新创建的 <code>primary</code> 分片、从来没有进行过 <code>allocate</code> 的分片除外】不会被 <code>allocate</code> 到当前节点 </li><li> 索引的分片副本数：<code>number_of_replicas</code>，一般设置为 1，表示总共有 2 份数据 </li><li> 每个节点分配的分片个数：<code>total_shards_per_node</code>，一般设置为 2，一个节点只分配 2 个分片，分别为主分片、副本分片 </li><li> 索引的分片个数：<code>number_of_shards</code>，当索引数据很大时，一般设置为节点个数【例如 <code>索引数据大小 / 50GB</code> 大于节点个数，例如 10 个节点，索引大小 <code>800GB</code>，此时按照官方建议应该设置 16 个分片，但是分片过多也不好，就可以设置 10 个分片，每个分片大小 <code>80GB</code>】，再配合 <strong>分片副本数为 1</strong>、<strong> 每个节点分配的分片个数为 2</strong>，就可以确保分片分配在所有的节点上面，并且每个节点上有 2 个分片，分别为主分片、副本分片 </li><li> 数据刷新时间：<code>refresh_interval</code>，表示数据写入后等待多久可以被搜索到，默认值 <code>1s</code>，每次索引的 <code>refresh</code> 会产生一个新的 <code>lucene</code> 段，这会导致频繁的合并行为，如果业务需求对实时性要求没那么高，可以将此参数调大，例如调整为 <code>60s</code>，会大大降低 <code>cpu</code> 的使用率 </li><li> 索引的分片大小，官方建议是每个分片大小在 <code>30GB</code> 到 <code>50GB</code> 不要超过 <code>50GB</code>，所以当索引的数据很大时，就要考虑增加分片的数量 </li><li> 设置 <code>terms</code> 最大个数：<code>index.max_terms_count</code>，默认最大个数为 65535，可以根据集群情况降低，例如设置为 10000，为了集群稳定，一般不需要设置那么大 </li><li> 设置 <code>Boolean Query</code> 的子语句数量：<code>indices.query.bool.max_clause_count</code>，默认为 1024，不建议增大这个值，也可以根据集群情况适当减小 </li><li> 查看热点线程：<code>http://your_ip:your_port/_nodes/your_node_name/hot_threads</code>，可以判断热点线程是 <code>search</code>，<code>bulk</code>，还是 <code>merge</code>，从而进一步分析是查询还是写入导致负载过高 </li><li> 数据目录：<code>path.data: /path/to/data</code>，多个目录使用逗号分隔，里面存放数据文件 </li><li> 日志目录：<code>path.logs: /path/to/logs</code>，里面存放的是节点的日志、慢查询日志、慢索引日志 </li><li> 家目录：<code>path.home: /path/to/home</code>，<code>elasticsearch</code> 的家目录，里面有插件、<code>lib</code>、配置文件等 </li><li> 插件目录：<code>path.plugins: /path/to/plugins</code>，插件目录，里面存放的是插件，例如：分词器 </li><li> 设置慢获取时间边界：<code>index.search.slowlog.threshold.fetch.warn: 30s</code>，超过这个时间的信息会被记录在日志文件中，<code>path.logs</code> 参数指定的目录中 <code>cluster-name_index_fetch_slowlog.log</code> 文件 </li><li> 设置慢查询时间边界：<code>index.search.slowlog.threshold.query.warn: 60s</code>，超过这个时间的信息会被记录在日志文件中，<code>path.logs</code> 参数指定的目录中 <code>cluster-name_index_search_slowlog.log</code> 文件 </li><li> 设置慢索引时间边界：<code>index.search.slowlog.threshold.index.warn: 60s</code>，超过这个时间的信息会被记录在日志文件中，<code>path.logs</code> 参数指定的目录中 <code>cluster-name_index_indexing_slowlog.log</code> 文件 </li><li> 禁止集群重分配：<code>cluster.routing.allocation.enable=none</code>，手动操作分片前需要关闭，否则会引起分片的移动，造成不必要的 <code>IO</code></li><li>开启集群重分配：<code>cluster.routing.allocation.enable=all</code>，集群的分片管理权限交由集群，保持数据均衡 </li><li> 推迟索引的分片分配时间，在分片节点出故障或者重启时，可以避免分片数据的移动，前提是及时把节点恢复：<code>index.unassigned.node_left.delayed_timeout=5m</code>，通俗点说，就是趁分片不注意，节点已经恢复了，此时数据分片保持不变，避免了不必要的 <code>IO</code></li><li><code>index.max_slices_per_scroll</code>，除了传统的 <code>scroll</code> 读取数据的方式，<code>v5.x</code> 之后 <code>Elasticsearch</code> 又增加了对每个分片读取数据的功能，称之为切片处理【<code>sliced scroll</code>】，这种读取方式可以对多个分片并行读取数据，大大提高了取数效率，<code>elasticsearch-hadoop</code> 就是采用这种方式读取数据的。但是，这里面有一个限制，<code>Elasticsearch</code> 默认一个 <code>scroll</code> 最大的切片数量为 1024【一般小于等于分片数，也可以通过指定切片字段来创建大于分片数的切片】，可以通过 <code>index.max_slices_per_scroll</code> 参数来变更【不建议更改】</li><li><code>cluster.routing.allocation.same_shard.host</code>，在单台物理节点配置多个 <code>Elasticsearch</code> 实例时，这个参数才生效，用来检查同一个分片的多个实例【主分片、副本分片】是否能分配在同一台主机上面，默认值为 <code>false</code>。如果设置为 <code>true</code>，表示开启检查机制，一台物理机上面启动 2 个 <code>Elasticsearch</code> 节点，则分配相同编号的分片时，不会都在这台机器上面，尽管可以满足主分片、副本分片不在同一个 <code>Elasticsearch</code> 节点上 </li><li><code>script.groovy.sandbox.enabled: false</code>，禁用 <code>Grovvy</code> 脚本，默认是关闭的</li><li><code>script.inline: false</code>，允许使用内置 <code>painless</code> 脚本</li><li><code>script.stored: false</code>，允许使用保存在 <code>config/scripts</code> 中的脚本，调用时使用 <code>id</code> 即可，类似方法名</li><li><code>script.file: false</code>，允许使用外部脚本文件</li><li><code>http.port: 9200</code>，集群的 <code>HTTP</code> 端口号</li><li><code>transport.tcp.port: 9300</code>，集群的 <code>TCP</code> 端口号</li><li><code>thread_pool.bulk.queue_size: 1500</code>，<code>bulk</code> 队列的大小</li><li><code>indices.breaker.total.use_real_memory: true</code>，熔断器回收内存，防止 <code>OOM</code>，决定父熔断器是考虑实际内存使用情况，还是仅考虑子熔断器内存使用情况</li><li><code>indices.breaker.total.limit: 70%</code>，熔断器回收内存，防止 <code>OOM</code>，当 <code>use_real_memory</code> 为 <code>true</code> 时，默认为 95%，否则默认为 70%</li><li><code>indices.breaker.fielddata.limit: 40%</code>，熔断器回收内存，防止 <code>OOM</code>，默认 40%</li><li><code>indices.breaker.request.limit: 60%</code>，熔断器回收内存，防止 <code>OOM</code>，默认 60%</li><li><code>discovery.zen.fd.ping_timeout: 60s</code>，集群故障检测</li><li><code>discovery.zen.fd.ping_interval: 10s</code>，集群故障检测</li><li><code>discovery.zen.fd.ping_retries: 10</code>，集群故障检测</li><li><ul><li><code>discovery.zen.master_election.ignore_non_master_pings: true</code>，选举主节点，设置为 <code>true</code> 时非 <code>node.master</code> 节点不能参与选举，投票也无效</li></ul></li><li><code>discovery.zen.minimum_master_nodes: 2</code>，选举主节点，最少有多少个备选主节点参加选举，防止脑裂现象</li><li><code>discovery.zen.ping_timeout: 10s</code>，选举主节点</li><li><code>discovery.zen.ping.unicast.hosts: [&quot;ip1:port&quot;,&quot;ip2:port&quot;]</code>，选举主节点，主机列表</li><li><code>node.data: true</code>，数据节点</li><li><code>node.master: true</code>，有资格被选举为主节点</li></ul><h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><ul><li> 序列化方式：<code>spark.serializer</code>，可以选择：<code>org.apache.spark.serializer.KryoSerializer</code></li><li><code>executor</code> 附加参数：<code>spark.executor.extraJavaOptions</code>，例如可以添加：<code>-Dxx=yy</code>【如果仅仅在 <code>driver</code> 端设置，<code>executor</code> 是不会有的】</li><li><code>driver</code> 附加参数：<code>spark.driver.extraJavaOptions</code>，例如可以添加：<code>-Dxx=yy</code></li><li>日志配置文件设置，<code>Spark</code> 使用的是 <code>log4j</code>，默认在 <code>Spark</code> 安装目录的 <code>conf</code> 下面，如果想要增加 <code>log4j</code> 相关配置，更改 <code>driver</code> 机器上面的 <code>log4j.properties</code> 配置文件是无效的，必须把所有的 <code>executor</code> 节点上的配置文件全部更新。如果没有权限，也可以自己上传配置文件，然后需要在 <code>executor</code> 附加参数中指定：<code>-Dlog4j.configuration=file:/path/to/file</code>，启动 <code>Spark</code> 任务时还需要使用 <code>--files</code> 指定配置文件名称，多个用逗号分隔，用来上传配置文件到 <code>Spark</code> 节点 </li><li> 开启允许多 <code>SparkContext</code> 存在：<code>spark.driver.allowMultipleContexts</code>，设置为 <code>true</code> 即可，在使用多个 <code>SparkContext</code> 时，需要先停用当前活跃的，使用 <code>stop</code> 方法【在 <code>Spark v2.0</code> 以及以上版本，已经取消了这个限制】</li><li><code>spark.executor.cores</code>，每个执行器上面的占用核数，会消耗 <code>CPU</code>，一般设置为 2-3</li><li><code>spark.executor.memory</code>，执行器上面的堆内存大小，一般设置为 <code>2048M</code>、<code>4096M</code></li><li><code>spark.port.maxRetries</code>，提交任务的 <code>Spark UI</code> 重试次数 </li><li><code>spark.default.parallelism</code>，默认并行度</li><li><code>spark.cores.max</code>，最大核心数</li><li><code>spark.executor.logs.rolling.strategy</code></li><li><code>spark.executor.logs.rolling.maxRetainedFiles</code></li><li><code>spark.executor.logs.rolling.size.maxBytes</code></li><li><code>spark.ui.showConsoleProgress</code></li></ul><h2 id="Kafka- 输入数据源"><a href="#Kafka- 输入数据源" class="headerlink" title="Kafka 输入数据源"></a>Kafka 输入数据源</h2><p><code>Spark Streaming</code> 配置：</p><ul><li><code>spark.streaming.backpressure.enabled</code>，开启反压机制</li><li><code>spark.streaming.backpressure.pid.minRate</code>，</li><li><code>spark.streaming.kafka.maxRatePerPartition</code>，每个 <code>partition</code> 的最大读取速度，单位秒，一般设置 500-100 即可</li><li><code>spark.streaming.receiver.maxRate</code>，<code>receiver</code> 最大处理数据量，单位秒，与 <code>maxRatePerPartition</code>、<code>Durations</code> 有关，实际运行时由于反压机制，数据处理速度会低于这个值</li><li><code>spark.streaming.receiver.writeAheadLog.enable</code>，</li><li><code>spark.streaming.stopGracefullyOnShutdown</code>，优雅地退出</li><li><code>spark.streaming.gracefulStopTimeout</code>，</li><li><code>xx</code>，</li></ul><p><code>Kakfa</code> 配置：</p><ul><li><code>metadata.broker.list</code>，</li><li><code>offsets.storage</code>，设置为 <code>kafka</code></li><li><code>zookeeper.connect</code>，</li><li><code>zookeeper.connection.timeout.ms</code></li><li><code>group.id</code></li><li><code>fetch.message.max.bytes</code></li><li><code>auto.offset.reset</code>，消费的起始位置，这个参数高低版本之间的名称、值都会不同，需要注意</li><li><code>consumer.timeout.ms</code></li><li><code>rebalance.max.retries</code></li><li><code>rebalance.backoff.ms</code></li></ul><h2 id="集群参数"><a href="# 集群参数" class="headerlink" title="集群参数"></a> 集群参数 </h2><p><code>yarn</code> 集群：</p><ul><li> 待定 </li></ul><p><code>standalone</code> 集群</p><ul><li> 临时目录：<code>SPARK_LOCAL_DIRS</code>，用来存放 <code>Spark</code> 任务运行过程中的临时数据，例如内存不足时把数据缓存到磁盘，就会有数据写入这个目录，当然，在启动 <code>Spark</code> 任务时也可以单独指定，但是最好还是设置在集群上面，可以在 <code>spark-env.sh</code> 脚本中设置，键值对的形式，例如：<code>SPARK_LOCAL_DIRS=/your_path/spark/local</code>。需要注意的是，启动 <code>Excutor</code> 的用户必须有这个目录的写权限，并且保证这个目录的磁盘空间足够使用，否则在 <code>Spark</code> 任务中会出现异常：<code>java.io.IOException: Failed to create local dir in xx</code>，进而导致 <code>Task</code> 失败 </li><li><code>Work</code> 目录：<code>SPARK_WORKER_DIR</code>，用来存放 <code>Work</code> 的信息，设置方式同上面的 <code>SPARK_LOCAL_DIRS</code>，如果 <code>Spark</code> 任务里面有 <code>System.out ()</code>，输出的内容在此目录下</li><li><code>SPARK_LOG_DIR</code>，<code>Spark</code> 集群自身的日志文件，例如 <code>Work</code> 接收 <code>Spark</code> 任务后通信的内容</li></ul><h1 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h1><ul><li><code>StormUI nimbus</code> 内容传输大小限制：<code>nimbus.thrift.max_buffer_size: 1048576</code>，取值的单位是字节，默认为 <code>1048576</code>，如果 <code>nimbus</code> 汇报的内容过多，超过这个值，则在 <code>StormUI</code> 上面无法查看 <code>Topology Summary</code> 信息，会报错：<code>Internal Server Error org.apache.thrift7.transport.TTransportException: Frame size (3052134) larger than max length (1048576)</code></li><li> 执行实例 <code>worker</code> 对应的端口号：<code>supervisor.slots.ports:</code>，可以设置多个，和 <code>CPU</code> 的核数一致，或者稍小，提高机器资源的使用率 </li><li><code>worker</code> 的 <code>JVM</code> 参数：<code>WORKER_GC_OPTS</code>，取值参考：<code>-Xms1G -Xmx5G -XX:+UseG1GC</code>，根据集群机器的资源多少而定，<code>G1</code> 是一种垃圾回收器</li><li><code>supervisor</code> 的 <code>JVM</code> 参数：<code>SUPERVISOR_GC_OPTS</code>，取值参考：<code>-Xms1G -Xmx5G -XX:+UseG1GC</code>，根据集群机器的资源多少而定，<code>G1</code> 是一种垃圾回收器</li><li><code>Storm UI</code> 的服务端口：<code>ui.port</code>，可以使用浏览器打开网页查看 <code>Topology</code> 详细信息</li><li><code>ZooKeeper</code> 服务器列表：<code>storm.zookeeper.servers</code></li><li><code>ZooKeeper</code> 连接端口：<code>storm.zookeeper.port</code></li><li><code>ZooKeeper</code> 中 <code>Storm</code> 的根目录位置：<code>storm.zookeeper.root</code>，用来存放 <code>Storm</code> 集群元信息</li><li> 客户端连接 <code>ZooKeeper</code> 超时时间：<code>storm.zookeeper.session.timeout</code></li><li><code>Storm</code> 使用的本地文件系统目录：<code>storm.local.dir</code>，注意此目录必须存在并且 <code>Storm</code> 进程有权限可读写 </li><li><code>Storm</code> 集群运行模式：<code>storm.cluster.mode</code>，取值可选：<code>distributed</code>、<code>local</code></li></ul><h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><p> 注意，<code>Kafka</code> 的不同版本参数名、参数值会有变化，特别是 <code>v0.9.x</code> 之后，与之前的低版本差异很大，例如数据游标的参数可以参考我的另外一篇博文：<a href="https://www.playpi.org/2017060101.html">记录一个 Kafka 错误：OffsetOutOfRangeException</a> ，<code>Kafka</code> 官网参见：<a href="https://kafka.apache.org/090/documentation.html#consumerconfigs" target="_blank" rel="noopener">Kafka-v0.9.0.x-configuration</a> 。</p><p>配置优化都是修改 <code>server.properties</code> 文件中参数值。</p><ul><li><code>JVM</code> 参数：<code>KAFKA_HEAP_OPTS</code>，取值参考：<code>-Xmx2G</code></li><li>文件存放位置：<code>log.dirs</code>，多个使用逗号分隔，注意所有的 <code>log</code> 级别需要设置为 <code>INFO</code></li><li>单个 <code>Topic</code> 的文件保留策略：<code>log.retention.hours=72</code>【数据保留 72 小时，超过时旧数据被删除】，<code>log.retention.bytes=1073741824</code>【数据保留 1GB，超过时旧数据被删除】</li><li>数据文件刷盘策略：<code>log.flush.interval.messages=10000</code>【每当 <code>producer</code> 写入 10000 条消息时，刷数据到磁盘】，<code>log.flush.interval.ms=1000</code>【每间隔 1 秒钟时间，刷数据到磁盘】</li><li><code>Topic</code> 的分区数量：<code>num.partitions=8</code></li><li>启动 <code>Fetch</code> 线程给副本同步数据传输大小限制：<code>replica.fetch.max.bytes=10485760</code>，要比 <code>message.max.bytes</code> 大 </li><li><code>message.max.bytes=10485700</code>，这个参数决定了 <code>broker</code> 能够接收到的最大消息的大小，要比 <code>max.request.size</code> 大</li><li><code>max.request.size=10480000</code>，这个参数决定了 <code>producer</code> 生产消息的大小</li><li><code>fetch.max.bytes=10485760</code>，这个参数决定了 <code>consumer</code> 消费消息的大小，要比 <code>message.max.bytes</code> 大</li><li><code>broker</code> 处理消息的最大线程数：<code>num.network.threads=17</code>，一般 <code>num.network.threads</code> 主要处理网络 <code>IO</code>，读写缓冲区数据，基本没有 <code>IO</code> 等待，配置线程数量为 <code>CPU</code> 核数加 1</li><li><code>broker</code> 处理磁盘 <code>IO</code> 的线程数：<code>num.io.threads=32</code>，<code>num.io.threads</code> 主要进行磁盘 <code>IO</code> 操作，高峰期可能有些 <code>IO</code> 等待，因此配置需要大些，配置线程数量为 <code>CPU</code> 核数 2 倍，最大不超过 3 倍</li><li> 强制新建一个 <code>segment</code> 的时间：<code>log.roll.hour=72</code></li><li>是否允许自动创建 <code>Topic</code>：<code>auto.create.topics.enable=true</code>，如果设置为 <code>false</code>，则代码无法创建，需要通过 <code>kafka</code> 的命令创建 <code>Topic</code></li><li><code>auto.offset.reset</code>，关于数据游标的配置【<code>earliest</code> 与 <code>latest</code>、<code>smallest</code>、<code>largest</code>】，由于不同版本之间的差异，可以参考：<a href="https://www.playpi.org/2017060101.html">记录一个 Kafka 错误：OffsetOutOfRangeException</a></li><li><code>advertised.host.name</code>、<code>advertised.port</code>，关于外网集群可以访问的配置，跨网络生产、消费数据，<code>v082</code> 以及之前的版本【之后的版本有保留这两个参数，但是不建议使用】</li><li><code>advertised.listeners</code>、<code>listeners</code>，关于外网集群可以访问的配置，跨网络生产、消费数据，<code>v090</code> 以及之后的版本 </li></ul><p> 留意参数取值大小的限制：<code>fetch.max.bytes</code> 大于 <code>message.max.bytes</code> 大于 <code>max.request.size</code>，<code>replica.fetch.max.bytes</code> 大于 <code>message.max.bytes</code> 大于 <code>max.request.size</code>。</p><h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><p>配置 <code>zoo.cfg</code> 文件：</p><ul><li><code>dataDir</code>，表示快照日志目录</li><li><code>dataLogDir</code>，表示事务日志目录，不配置的时候事务日志目录同 <code>dataDir</code></li><li><code>clientPort=2181</code>，服务的监听端口</li><li><code>tickTime=2000</code>，<code>Zookeeper</code> 的时间单元，<code>Zookeeper</code> 中所有时间都是以这个时间单元的整数倍去配置的，例如，<code>session</code> 的最小超时时间是 <code>2*tickTime</code>【单位：毫秒】</li><li><code>syncLimit=5</code>，表示 <code>Follower</code> 和 <code>Observer</code> 与 <code>Leader</code> 交互时的最大等待时间，只不过是在与 <code>leader</code> 同步完毕之后，进入正常请求转发或 <code>ping</code> 等消息交互时的超时时间</li><li><code>initLimit=10</code>，<code>Observer</code> 和 <code>Follower</code> 启动时，从 <code>Leader</code> 同步最新数据时，<code>Leader</code> 允许 <code>initLimit * tickTime</code> 的时间内完成，如果同步的数据量很大，可以相应地把这个值设置大一些</li><li><code>maxClientCnxns=384</code>，最大并发客户端数，用于防止 <code>Ddos</code> 的，默认值是 10，设置为 0 是不加限制</li><li><code>maxSessionTimeout=120000</code>，<code>Session</code> 超时时间限制，如果客户端设置的超时时间不在这个范围，那么会被强制设置一个最大时间，默认的 <code>Session</code> 超时时间是在 <code>2 * tickTime ~ 20 * tickTime</code> 这个范围</li><li><code>minSessionTimeout=4000</code>，同 <code>maxSessionTimeout</code></li><li><code>server.x=hostname:2888:3888</code>，<code>x</code> 是一个数字，与每个服务器的 <code>myid</code> 文件中的 <code>id</code> 是一样的，<code>hostname</code> 是服务器的 <code>hostname</code>，右边配置两个端口，第一个端口用于 <code>Follower</code> 和 <code>Leader</code> 之间的数据同步和其它通信，第二个端口用于 <code>Leader</code> 选举过程中投票通信</li><li><code>autopurge.purgeInterval=24</code>，在 <code>v3.4.0</code> 及之后的版本，<code>Zookeeper</code> 提供了自动清理事务日志文件和快照日志文件的功能，这个参数指定了清理频率，单位是小时，需要配置一个 1 或更大的整数。默认是 0，表示不开启自动清理功能</li><li><code>autopurge.snapRetainCount=30</code>，参数指定了需要保留的事务日志文件和快照日志文件的数目，默认是保留 3 个，和 <code>autopurge.purgeInterval</code> 搭配使用</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;本文记录大数据平台框架的一些常用参数，这些参数基本是我见过的或者实际使用过的，我会列出参数的含义以及使用效果，具有一定的参考意义。当然，根据实际的场景不同，参数值并不能随便设置为一样，必须要考虑到实际的情况，否则可能没有效果，或者具有反作用。&lt;/p&gt;&lt;p&gt;会保持更新。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="Hadoop" scheme="https://www.playpi.org/tags/Hadoop/"/>
    
      <category term="Spark" scheme="https://www.playpi.org/tags/Spark/"/>
    
      <category term="HDFS" scheme="https://www.playpi.org/tags/HDFS/"/>
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="Zookeeper" scheme="https://www.playpi.org/tags/Zookeeper/"/>
    
      <category term="MapReduce" scheme="https://www.playpi.org/tags/MapReduce/"/>
    
      <category term="Kafka" scheme="https://www.playpi.org/tags/Kafka/"/>
    
      <category term="Storm" scheme="https://www.playpi.org/tags/Storm/"/>
    
  </entry>
  
  <entry>
    <title>解决 jar 包冲突的神器：maven-shade-plugin</title>
    <link href="https://www.playpi.org/2019120101.html"/>
    <id>https://www.playpi.org/2019120101.html</id>
    <published>2019-11-30T16:54:21.000Z</published>
    <updated>2019-11-30T16:54:21.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --><p>最近因为协助升级相关业务的 <code>sdk</code>，遇到过多次 <code>jar</code> 包冲突的问题，此外自己在业务中升级算法接口的 <code>sdk</code> 时，也遇到过 <code>jar</code> 冲突问题。而且，这种冲突是灾难性的，不要指望通过排除特定包、升级版本、降级版本解决，根本无济于事，还会越来越混乱。</p><p>那么，最高效的方法是使用 <code>maven-shade-plugin</code> 插件，只要加上冲突相关的 <code>relocation</code> 配置，变更包名，即可迅速化解冲突的问题。</p><a id="more"></a><p>在此提前说明，下文中涉及的代码已经被我上传至 <code>GtiHub</code>：<a href="https://github.com/iplaypi/iplaypistudy-shade" target="_blank" rel="noopener">iplaypistudy-shade</a> ，特别独立创建了一个 <code>Maven</code> 小项目，专供演示使用，读者可以提前下载使用。</p><h1 id="前提场景"><a href="# 前提场景" class="headerlink" title="前提场景"></a>前提场景 </h1><p> 在 <code>Maven</code> 项目中，当功能越来越丰富，需要的第三方依赖也就越来越多，此时很容易发生 <code>jar</code> 包冲突。而通常是因为，<code>Mavne</code> 项目中依赖了同一个 <code>jar</code> 包的多个版本，即坐标版本号不同。</p><p>一般的思路是只保留一个版本，删除掉不需要的版本，但是在复杂情况下，版本之间不兼容，不可能就这么删掉某一个【因为多个 <code>jar</code> 分别被引用了不同的方法】，所以这种思路行不通。</p><p>例如我最近遇到了一个下图这样的例子【本文开头指定的 <code>GitHub</code> 源代码可以直接下载】：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208195230.png" alt="依赖冲突的项目结构" title="依赖冲突的项目结构"></p><p>其中，<code>module-a</code>、<code>module-b</code>、<code>module-c</code> 是我项目中的三个模块，<code>module-a</code> 同时依赖了子模块 <code>module-b</code> 和 <code>module-c</code>，这个很容易理解。</p><p>但是，在 <code>module-b</code>、<code>module-c</code> 中分别依赖了不同版本的 <code>guava</code>，并且在代码中有实际调用不兼容的方法，高版本的方法在低版本中不存在，低版本的方法在高版本中不存在【这属于 <code>guava</code> 没有做到向前兼容的问题】。</p><p>代码具体内容在后面的演示中会详细描述，这里先探讨一下这种情况该怎么办。</p><p>如果排除掉 <code>guava v19.0</code> 的话【使用 <code>exclude</code> 特性】，<code>module-b</code> 会报错，如果排除掉 <code>guava v26.0-jre</code> 的话，<code>module-c</code> 会报错，但是我又希望在项目中可以同时使用 <code>guava v19.0</code> 和 <code>guava v26.0-jre</code>，为了功能考虑也必须同时使用，不能排除任何一个。</p><p>好像陷入了僵局，反正我一开始是没有什么好办法的，直到有一位同事，在我旁边偶尔提了一句，你可以使用 <code>maven-shade-plugin</code> 插件，能完美解决你这个需求场景，方便快捷，毫无痛苦。</p><p>我自己先去了解了一下，后来又听他解释了一遍，才恍然大悟，感觉技术观念再一次被刷新了，居然还有这种操作。</p><p>下面就简单描述一下具体怎么使用 <code>maven-shade-plugin</code> 插件解决这个问题。</p><h1 id="解决方案演示"><a href="# 解决方案演示" class="headerlink" title="解决方案演示"></a>解决方案演示 </h1><h2 id="案例说明"><a href="# 案例说明" class="headerlink" title="案例说明"></a> 案例说明 </h2><p> 由于是演示 <code>maven-shade-plugin</code> 插件的使用，所以仅仅只有几行核心代码、几个核心依赖，但是完全可以表达出解决冲突的思路，源代码请读者从本文开头指定的 <code>GitHub</code> 链接下载。</p><p>如上图所示，<code>module-a</code>、<code>module-b</code>、<code>module-c</code> 是我项目中的三个模块，<code>module-a</code> 同时依赖了子模块 <code>module-b</code> 和 <code>module-c</code>。在子模块 <code>module-b</code> 中，依赖了 <code>guava v19.0</code>，在 子模块 <code>module-c</code> 中，依赖了 <code>guava v26.0-jre</code>。</p><p>好，接下来重点来了，在 <code>guava</code> 的两个版本中有下面两个不兼容的方法，特意挑选出来，用来测试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 这个方法在 v19.0 中有，在 v26.0-jre 中没有 </span><br><span class="line">@CheckReturnValue</span><br><span class="line">  @Deprecated</span><br><span class="line">  public static ToStringHelper toStringHelper (Object self) &#123;</span><br><span class="line">return new ToStringHelper (self.getClass ().getSimpleName ());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 这个方法在 v26.0-jre 中有，在 v19.0 中没有 </span><br><span class="line">public static String lenientFormat (@Nullable String template, @Nullable Object... args) &#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当然，如果在 <code>module-b</code>、<code>module-c</code> 的依赖 <code>jar</code> 源码中有调用到，也是可以的，但是不直观，而且依赖 <code>jar</code> 的方法也不一定会执行，不好控制，所以我选择手动显式写代码调用的方式来演示。</p><h2 id="代码清单"><a href="# 代码清单" class="headerlink" title="代码清单"></a>代码清单 </h2><p> 演示代码主要内容如下。</p><p>在 <code>module-b</code> 中有一个类 <code>ModuleBRun</code>，调用了 <code>toStringHelper ()</code> 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">@Slf4j</span><br><span class="line">public class ModuleBRun &#123;</span><br><span class="line">public static void main (String [] args) &#123;</span><br><span class="line">log.info (&quot;====Hello World!&quot;);</span><br><span class="line">run ();</span><br><span class="line">&#125;</span><br><span class="line">public static void run () &#123;</span><br><span class="line">// 这个方法在 v19.0 中有，在 v26.0-jre 中没有 </span><br><span class="line">log.info (&quot;==== 开始执行 module-b 的代码 & quot;);</span><br><span class="line">Objects.ToStringHelper toStringHelper = Objects.toStringHelper (new Object ());</span><br><span class="line">toStringHelper.add (&quot;in&quot;, &quot;in&quot;);</span><br><span class="line">toStringHelper.add (&quot;out&quot;, &quot;out&quot;);</span><br><span class="line">log.info (&quot;====[&#123;&#125;]&quot;, toStringHelper.toString ());</span><br><span class="line">log.info (&quot;====module-b 的代码执行完成 & quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208015421.png" alt="ModuleBRun" title="ModuleBRun"></p><p>在 <code>module-c</code> 中有一个类 <code>ModuleCRun</code>，调用了 <code>lenientFormat ()</code> 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Slf4j</span><br><span class="line">public class ModuleCRun &#123;</span><br><span class="line">public static void main (String [] args) &#123;</span><br><span class="line">log.info (&quot;====Hello World!&quot;);</span><br><span class="line">run ();</span><br><span class="line">&#125;</span><br><span class="line">public static void run () &#123;</span><br><span class="line">log.info (&quot;==== 开始执行 module-c 的代码 & quot;);</span><br><span class="line">// 这个方法在 v26.0-jre 中有，在 v19.0 中没有 </span><br><span class="line">log.info (&quot;====[&#123;&#125;]&quot;, Strings.lenientFormat (&quot;&quot;, &quot;in&quot;, &quot;out&quot;));</span><br><span class="line">log.info (&quot;====module-c 的代码执行完成 & quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208015600.png" alt="ModuleCRun" title="ModuleCRun"></p><p>在 <code>module-a</code> 中有一个类 <code>ModuleARun</code>，有一个 <code>run ()</code> 方法，分别调用了上面的 <code>ModuleBRun.run ()</code>、<code>ModuleCRun.run ()</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * 依赖 b/c 时，无法成功运行 </span><br><span class="line">     * &lt;p&gt;</span><br><span class="line">     * 依赖 b/c-shade 时，可以成功运行 </span><br><span class="line">     */</span><br><span class="line">public static void run () &#123;</span><br><span class="line">log.info (&quot;==== 开始执行 module-a 的代码 & quot;);</span><br><span class="line">ModuleBRun.run ();</span><br><span class="line">ModuleCRun.run ();</span><br><span class="line">log.info (&quot;====module-a 的代码执行完成 & quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208015629.png" alt="ModuleARun" title="ModuleARun"></p><h2 id="运行效果"><a href="# 运行效果" class="headerlink" title="运行效果"></a>运行效果 </h2><p> 此时，尝试本地调试运行 <code>ModuleARun.run ()</code>，或者使用 <code>Maven</code> 打 <code>jar</code> 包后运行：<code>java -jar iplaypistudy-shade-module-a-1.0-SNAPSHOT-jar-with-dependencies.jar</code>，需要提前使用 <code>maven-shade-plugin</code> 配置 <code>mainClass</code> 后打包。</p><p>可以发现如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleARun:13: ====Hello World!</span><br><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleARun:24: ==== 开始执行 module-a 的代码 </span><br><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleBRun:19: ==== 开始执行 module-b 的代码 </span><br><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleBRun:23: ====[Object&#123;in=in, out=out&#125;]</span><br><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleBRun:24: ====module-b 的代码执行完成 </span><br><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleCRun:18: ==== 开始执行 module-c 的代码 </span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.google.common.base.Strings.lenientFormat (Ljava/lang/String;[Ljava/lang/Object;) Ljava/lang/String;</span><br><span class="line">at org.playpi.study.ModuleCRun.run (ModuleCRun.java:20)</span><br><span class="line">at org.playpi.study.ModuleARun.run (ModuleARun.java:26)</span><br><span class="line">at org.playpi.study.ModuleARun.main (ModuleARun.java:14)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208014735.png" alt="调试运行结果" title="调试运行结果"></p><p>看到 <code>NoSuchMethodError</code> 就知道出现了严重的问题，如果试图使用搜索功能搜索 <code>Strings</code> 这个类，可以发现有 2 个一模一样的类，但是他们对应的 <code>guava jar</code> 的版本号不一致。这时候有经验的工程师就可以立马判断，编译运行 <code>JVM</code> 加载的 <code>jar</code> 对于 <code>ModuleCRun.run ()</code> 方法来说是有问题的，只加载了特定版本的 <code>guava jar</code>，确保了 <code>ModuleBRun.run ()</code> 方法可以顺利执行【和手动排除 <code>module-c</code> 中的 <code>guava v26.0-jre</code> 一个效果】。</p><p>如果是编译打包后使用 <code>java</code> 命令再运行，可以发现同样的错误，如果此时尝试解压 <code>jar</code> 包，反编译源码，查看具体的类，可以看到编译打包后有些类是不存在的【多版本的 <code>jar</code> 只会保留一个，就会导致另一个 <code>jar</code> 中的类全部丢失，如果此时恰好有不兼容的类，那就出问题】。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208013756.png" alt="搜索 Strings 类" title="搜索 Strings 类"></p><p>那有人会想到，能不能手动排除 <code>module-a</code> 中的 <code>guava v19.0</code> 呢，我来试试，在 <code>module-a</code> 的 <code>pom.xml</code> 中对 <code>module-b</code> 添加 <code>exclude</code> 属性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.playpi.study&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;iplaypistudy-shade-module-b&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;parent.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;!-- 这里排除会导致调用 ModuleBRun.run () 时出现 NoSuchMethodError --&gt;</span><br><span class="line">    &lt;exclusions&gt;</span><br><span class="line">        &lt;exclusion&gt;</span><br><span class="line">            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;guava&lt;/artifactId&gt;</span><br><span class="line">        &lt;/exclusion&gt;</span><br><span class="line">    &lt;/exclusions&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>调试运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2020-02-08_01:40:33 [main] INFO study.ModuleARun:13: ====Hello World!</span><br><span class="line">2020-02-08_01:40:33 [main] INFO study.ModuleARun:24: ==== 开始执行 module-a 的代码 </span><br><span class="line">2020-02-08_01:40:33 [main] INFO study.ModuleBRun:19: ==== 开始执行 module-b 的代码 </span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.google.common.base.Objects.toStringHelper (Ljava/lang/Object;) Lcom/google/common/base/Objects$ToStringHelper;</span><br><span class="line">at org.playpi.study.ModuleBRun.run (ModuleBRun.java:20)</span><br><span class="line">at org.playpi.study.ModuleARun.run (ModuleARun.java:25)</span><br><span class="line">at org.playpi.study.ModuleARun.main (ModuleARun.java:14)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208014710.png" alt="调试运行结果" title="调试运行结果"></p><p>可见还是有同样的问题，运行到 <code>ModuleBRun ()</code> 方法时已经出错了，根源就在于多版本的 <code>guava</code> 之间无法兼容。</p><p>这里需要注意的是，在 <code>module-a</code> 中并不能随意调用 <code>module-c</code> 中 <code>guava v26.0-jre</code> 的方法，如果方法不存在的话编译不会通过【<code>maven</code> 先加载了低版本的 <code>guava v19.0</code>】。而单独看 <code>module-c</code> 的话，它是一个独立的子模块，所以 <code>module-c</code> 中的方法不受编译的限制，只有在把 <code>module-a</code> 打包后，真正运行时才会抛出异常。</p><p>具体可以参考 <code>ModuleARun</code> 中的 <code>runGuava ()</code> 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * 依赖 b/c 时或者依赖 b/c-shade 时:</span><br><span class="line">     * 在这里无法像 module-c 那样直接调用 26.0-jre 里面的方法，编译无法通过 </span><br><span class="line">     * 但是 module-c 里面的代码是单独处于模块里面，编译时无法检测，所以 ModuleCRun.run () 可以通过编译 (编译阶段不会检测 run 里面的代码)</span><br><span class="line">     * &lt;p&gt;</span><br><span class="line">     * 所以:</span><br><span class="line">     * 制作 shade 只是可以保证 ModuleCRun.run () 正常执行，并不能保证 Strings.lenientFormat 可用 (连编译都无法通过)</span><br><span class="line">     */</span><br><span class="line">public static void runGuava () &#123;</span><br><span class="line">log.info (&quot;==== 开始执行 module-a 的 guava v19.0 代码 & quot;);</span><br><span class="line">Objects.ToStringHelper toStringHelper = Objects.toStringHelper (new Object ());</span><br><span class="line">toStringHelper.add (&quot;in&quot;, &quot;in&quot;);</span><br><span class="line">toStringHelper.add (&quot;out&quot;, &quot;out&quot;);</span><br><span class="line">log.info (&quot;====[&#123;&#125;]&quot;, toStringHelper.toString ());</span><br><span class="line">log.info (&quot;====module-a 的 guava v19.0 代码执行完成 & quot;);</span><br><span class="line">log.info (&quot;&quot;);</span><br><span class="line">log.info (&quot;==== 开始执行 module-a 的 guava v26.0-jre 代码 & quot;);</span><br><span class="line">//        log.info (&quot;====[&#123;&#125;]&quot;, Strings.lenientFormat (&quot;&quot;, &quot;in&quot;, &quot;out&quot;));</span><br><span class="line">log.info (&quot;====module-a 的 guava v26.0-jre 代码执行完成 & quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="插件登场"><a href="# 插件登场" class="headerlink" title="插件登场"></a>插件登场 </h2><p> 看似疑无路，其实还有柳暗花明，使用 <code>maven-shade-plugin</code> 插件可以完美解决上述的问题。</p><p>在 <code>module-c</code> 的 <code>pom.xml</code> 配置文件中，给插件 <code>maven-shade-plugin</code> 添加 <code>relocation</code> 配置，把 <code>com.google.common</code> 包路径变为 <code>iplaypi.com.google.common</code>，要确保独一无二，总体内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 非常好用的 shade 插件 --&gt;</span><br><span class="line">&lt;plugin&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;maven-shade-plugin.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;executions&gt;</span><br><span class="line">        &lt;execution&gt;</span><br><span class="line">            &lt;!-- Maven 的生命周期 --&gt;</span><br><span class="line">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">                &lt;!-- 插件目标 --&gt;</span><br><span class="line">                &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;!-- 配置多版本 jar 包中类路径的重命名 --&gt;</span><br><span class="line">                &lt;relocations&gt;</span><br><span class="line">                    &lt;relocation&gt;</span><br><span class="line">                        &lt;pattern&gt;com.google.common&lt;/pattern&gt;</span><br><span class="line">                        &lt;shadedPattern&gt;iplaypi.com.google.common&lt;/shadedPattern&gt;</span><br><span class="line">                    &lt;/relocation&gt;</span><br><span class="line">                &lt;/relocations&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/execution&gt;</span><br><span class="line">    &lt;/executions&gt;</span><br><span class="line">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208185321.png" alt="给 C 模块添加 relocation" title="给 C 模块添加 relocation"></p><p>此外，在 <code>module-a</code> 中也需要配置常规的打包参数，使用 <code>mainClass</code> 指定主类，使用 <code>shadedClassifierName</code> 指定 <code>jar</code> 包后缀【不会用到 <code>relocation</code> 的功能】，内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;maven-shade-plugin.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;executions&gt;</span><br><span class="line">        &lt;execution&gt;</span><br><span class="line">            &lt;!-- Maven 的生命周期 --&gt;</span><br><span class="line">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">                &lt;!-- 插件目标 --&gt;</span><br><span class="line">                &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;transformers&gt;</span><br><span class="line">                    &lt;!-- 使用资源转换器 ManifestResourceTransformer, 可执行的 jar 包 --&gt;</span><br><span class="line">                    &lt;transformer</span><br><span class="line">                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;</span><br><span class="line">                        </span><br><span class="line">                     &lt;!-- 指定主类入口 --&gt;                       &lt;mainClass&gt;org.playpi.study.ModuleARun&lt;/mainClass&gt;</span><br><span class="line">                    &lt;/transformer&gt;</span><br><span class="line">                &lt;/transformers&gt;</span><br><span class="line">                &lt;!-- 指定 jar 包后缀 --&gt;</span><br><span class="line">                &lt;shadedClassifierName&gt;jar-with-dependencies&lt;/shadedClassifierName&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/execution&gt;</span><br><span class="line">    &lt;/executions&gt;</span><br><span class="line">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208185900.png" alt="给 A 模块添加打包参数" title="给 A 模块添加打包参数"></p><p>接着就可以编译打包了：<code>mvn clean package</code>，打包完成后，在 <code>target</code> 目录下找到最终的 <code>jar</code> 包，使用 <code>java</code> 命令执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -jar iplaypistudy-shade-module-a-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleARun:12: ====Hello World!</span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleARun:23: ==== 开始执行 module-a 的代码 </span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleBRun:19: ==== 开始执行 module-b 的代码 </span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleBRun:23: ====[Object&#123;in=in, out=out</span><br><span class="line">]</span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleBRun:24: ====module-b 的代码执行完成 </span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleCRun:18: ==== 开始执行 module-c 的代码 </span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleCRun:20: ====[[in, out]]</span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleCRun:21: ====module-c 的代码执行完成 </span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleARun:26: ====module-a 的代码执行完成 </span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208190909.png" alt="运行成功" title="运行成功"></p><p>可以看到运行结果，所有的方法都调用成功，说明不存在多版本的 <code>jar</code> 包冲突的问题了。</p><p>注意，此时不能使用 <strong>调试运行的方法 </strong>，读者会发现使用 <code>IDEA</code> 等工具直接调试运行，仍旧会出错，这是因为 <code>IDEA</code> 调试运行只是经过了 <code>compile</code> 阶段，而 <code>maven-shade-plugin</code> 插件中的 <code>shade relocation</code> 根本没有执行。由于我们配置的 <code>phase</code> 是 <code>package</code>【绑定到 <code>Maven</code> 的 <code>package</code> 生命周期】，因此，必须经过打包后，直接指定 <code>main</code> 主类运行 <code>jar</code> 包，才会看到效果。</p><p>为了知其然也知其所以然，我们肯定要看看 <code>jar</code> 包到底发生了什么变化，找到 <code>jar</code> 包，使用 <code>Java Decompiler</code> 工具反编译字节码文件，查看 <code>.java</code> 文件有什么变化，我们首先能想到的就是类路径变化了。</p><p>找到 <code>Strings</code> 类文件，可以看到它的类路径变化了，已经变为了 <code>iplaypi.com.google.common.base</code>，同时它所 <code>import</code> 的类路径也添加了 <code>iplaypi</code> 前缀。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208192045.png" alt="反编译查看源码" title="反编译查看源码"></p><p>也就是说，打包完成之后，在 <code>jar</code> 包里面可以看到原本 <code>com.google.common</code> 下面的类全部被保留，<code>guava v19.0</code> 的类路径没有变化，而 <code>guava v26.0-jre</code> 的所有类路径都被添加了前缀 <code>iplaypi.</code>，而这正是 <code>shade</code> 的功劳。如此一来，高、低版本的所有类都分离开了，调用方可以任意使用，不会再有冲突或者缺失的情况。</p><p>那我们再看看调用方的 <code>import</code> 是怎样的，分别找到 <code>ModuleBRun</code>、<code>ModuleCRun</code> 类。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208192409.png" alt="反编译后的 ModuleBRun" title="反编译后的 ModuleBRun"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208192525.png" alt="反编译后的 ModuleCRun" title="反编译后的 ModuleCRun"></p><p>从 <code>ModuleCRun</code> 中可以看到，调用方的代码类的 <code>import</code> 类路径也被同步替换。当然，由于 <code>ModuleBRun</code> 并没有参与 <code>shade relocation</code> 流程，所以 <code>import</code> 还是原来的样子。</p><p><strong>总结来说 </strong>，其实 <code>maven-shade-plugin</code> 插件并没有什么难以理解的地方，它只是帮助我们在构建 <code>jar</code> 包时，把特定的类路径转换为了我们指定的新路径，同时把所有调用方的 <code>import</code> 语句也改变了，这样就能确保这些类在加载到 <code>JVM</code> 中是独一无二的，也就不会冲突了【当然会造成最后的 <code>uber jar</code> 变大了，加载到 <code>JVM</code> 中的类也变多了】。</p><p>它的效果概念图如下：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208195142.png" alt="效果概念图" title="效果概念图"></p><p>当然，只看到现象还不够，下面我们来探讨一下它的实现方法，读者请看下一小节：<strong> 实现分析 </strong>。</p><h2 id="实现分析"><a href="# 实现分析" class="headerlink" title="实现分析"></a>实现分析 </h2><p> 想要分析 <code>maven-shade-plugin</code> 插件是如何实现这个功能的，源代码少不了，下面简单分析一下，可以直接打断点调试一下源代码，跟着源代码跑一遍打包的流程即可。</p><p>首先，需要下载源代码，在 <code>GitHub</code> 上面下载：<a href="https://github.com/apache/maven-shade-plugin/tree/maven-shade-plugin-3.2.1" target="_blank" rel="noopener">maven-shade-plugin</a> ，注意下载后切换到指定版本的，例如我使用的版本是 <code>v3.2.1</code>，则 <code>git clone</code> 后需要 <code>git checkout</code> 到指定的 <code>tag</code>【例如：<code>maven-shade-plugin-3.2.1</code>】。</p><p>源码下载成功后，它其实也是一个 <code>Maven</code> 项目【如果导入时 <code>IDEA</code> 识别不了，可以先 <code>Open</code> 看一下，需要一些初始化动作】，可以直接以 <code>Module</code> 的形式导入 <code>IDEA</code> 中，然后就可以直接被我们自己的项目依赖。</p><p>在 <code>IDEA</code> 中依次选择 <code>File</code>、<code>New</code>、<code>Module from existing Sources</code>【也可以在 <code>Project Structure</code> 中直接添加】，最终选择已经下载的项目源码，导入过程中还需要选择一些配置，例如项目为 <code>Maven</code> 类型、项目名称，直接使用默认值即可。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205501.png" alt="添加模块" title="添加模块"></p><p>由于有部分 <code>jar</code> 包需要从远程仓库拉取，如果网络不好的话【或者没配置国内的仓库、镜像】，速度有点慢，需要耐心等待。</p><p>添加成功后，需要确保 <code>maven-shade-plugin</code> 模块正常，通过 <code>File</code>、<code>Project Structure</code>、<code>Module</code> 查看。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205616.png" alt="检查模块" title="检查模块"></p><p>此时，我们 <code>module-a</code> 的 <code>pom.xml</code> 文件中配置的 <code>maven-shade-plugin</code> 插件，实际使用的就不是本地仓库的了，而是我们导入的 <code>Module</code>，这样就可以调试代码了。</p><p>找到 <code>maven-shade-plugin</code> 插件的入口，<code>Maven</code> 规定一般是 <code>@Mojo</code> 注解类的 <code>execute ()</code> 方法，我在这里找到类：<code>org.apache.maven.plugins.shade.mojo.ShadeMojo</code>，<code>execute ()</code> 方法在代码 381 行，在这个方法入口处 385 行：<code>setupHintedShader ();</code>，打上断点。</p><p>具体的生成 <code>jar</code> 包以及 <code>shade relocation</code> 功能的实现逻辑在 <code>org.apache.maven.plugins.shade.DefaultShader</code> 中，我们在 160 行的 <code>shadeJars ()</code> 方法中打上断点。</p><p>接着准备调试的步骤，可以增加一个 <code>Run/Debug Configuration</code>，把 <code>mvn clean package</code> 配置成为一个 <code>Application</code>，最后点击 <code>debug</code> 按钮就可以调试了。也可以直接选中项目右键，依次选择 <code>Debug Maven</code>、<code>debug: package</code>，直接进行调试，我使用的就是这种方式，如下图：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205712.png" alt="开始调试" title="开始调试"></p><p>首先进入到第一个断点：<code>execute ()</code> 方法，说明调试程序执行正常，直接进入到下一个断点：<code>shadeJars ()</code> 方法【注意，我这里截图执行的是 <code>module-c</code> 打包的流程，列出的 <code>jar</code> 包仅和 <code>module-c</code> 有关】：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205731.png" alt="execute 方法" title="execute 方法"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205746.png" alt="shadeJars 方法" title="shadeJars 方法"></p><p>可以从 <code>shadeRequest</code> 对象中看到 <code>jar</code> 包列表，以及 <code>relocators</code> 列表，<code>shade relocation</code> 的代码逻辑在 <code>org.apache.maven.plugins.shade.relocation.SimpleRelocator</code> 里面，里面有替换类路径、文件路径的操作实现。</p><p>接着进入到 <code>shadeSingleJar ()</code> 方法，可以看到对每一个文件进行处理，替换、合并等操作。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205801.png" alt="shadeSingleJar 方法" title="shadeSingleJar 方法"></p><p>最后也可以测试一下，如果不对 <code>module-c</code> 做 <code>shade relocation</code>，最终项目打包收集的所有 <code>jar</code> 包中，是没有 <code>guava v26.0-jre</code> 的，只有 <code>guava v19.0</code>，这也可以解释为什么运行时会缺失。</p><h2 id="另一种情况"><a href="# 另一种情况" class="headerlink" title="另一种情况"></a>另一种情况 </h2><p> 假设 <code>module-c</code> 不是我们自己维护的模块，我们无权限变更，更不可能直接去更改它的 <code>pom.xml</code> 文件，此时应该怎么办。可以把 <code>module-c</code> 类比成一个独立的 <code>jar</code> 包，拥有自己的坐标，由开源组织发布【例如 <code>hive-client</code>、<code>hbase-client</code>】，被 <code>module-a</code> 依赖引用，此时我们不可能去更改它的配置文件或者代码。</p><p>也有办法，那就是为这类 <code>jar</code> 包单独创建一个独立的 <code>module</code>，在这个 <code>module</code> 中完成 <code>shade</code> 操作，然后才把这个 <code>module</code> 给我们的项目引用。</p><p>在本例中，就以 <code>module-c</code> 为例，假如我们没有权限更改 <code>module-c</code> 中的代码、配置文件，只能新创建一个 <code>module-c-shade</code>，它里面什么代码都没有，只是简单地依赖 <code>module-c</code>，然后在配置文件 <code>pom.xml</code> 中做一个 <code>shade relocation</code>，把可能冲突的类解决掉。</p><p>项目结构如下图：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208200838.png" alt="复杂情况的传递依赖" title="复杂情况的传递依赖"></p><p>和上面的效果一致，编译打包后，依旧可以成功运行。</p><p>可以多思考一下，根据上面的情况，还有在什么场景下需要单独创建一个 <code>module</code>，里面没有任何代码，只是为了做影子依赖呢？</p><p>最先想到的肯定是类似上面那种，传递依赖导致的冲突，例如项目中依赖了 <code>es-hadoop</code>，而由此带来的 <code>guava</code>、<code>http</code> 等 <code>jar</code> 包冲突，我们不可能想着去改 <code>es-hadoop</code> 的 <code>pom.xml</code> 文件，因为我们不应当变更源码【太麻烦而且不利于管理】，当然也不一定能拿到源码。那么，只能单独创建一个 <code>module</code>，使用 <code>maven-shade-plugin</code> 插件做 <code>shade relocation</code>。</p><p>另外还有一种情况，如果传递依赖过多，例如 <code>es-hadoop</code> 中的 <code>guava</code>，<code>hbase</code> 中的 <code>commons-lang</code>，也没有必要为每一个 <code>jar</code> 包都单独创建一个 <code>module</code>，显得繁琐而且没必要。此时可以只创建一个 <code>module</code>，用来解决所有的依赖冲突，但是如果这些 <code>jar</code> 包之间的传递依赖本来就冲突，那还是得为每一个 <code>jar</code> 包都创建一个 <code>module</code>【此时这种 <code>Maven</code> 项目冲突过多，是不健康的，还是升级适配为好】。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p>1、新建 <code>module</code> 时如果卡住，可以设置参数 <code>archetypeCatalog=internal</code> 解决。</p><p>2、还要注意一点，低版本的 <code>maven-shade-plugin</code> 插件并不支持 <code>relocation</code> 参数来制作影子，编译时会报错，例如 <code>v2.4.3</code> 就不行，需要 <code>v3.0</code> 以上，例如：<code>v3.1.0</code>、<code>v3.2.1</code>。</p><p>3、引入新依赖后，要确保传递依赖不能污染了当前项目的依赖，而制作 <code>shade</code> 的目的在于这个新依赖不会有异常。</p><p> 当前项目中或者当前项目的依赖中，会有一些调用，如果被传递依赖污染，会导致异常。如果是当前项目的代码显式调用，编译不会通过，但是如果是在依赖 <code>jar</code> 中调用，编译阶段是检测不出来的，只会在运行调用时抛出异常。</p><p>使用上面的例子来说，如果在 <code>module-a</code> 中与 <code>module-b</code> 中的依赖有相同的，则在 <code>module-a</code> 中代码引用使用时【不是 <code>module-a</code> 中我们写的代码，而是 <code>module-a</code> 中 <code>jar</code> 的源代码】，确保使用的是 <code>module-a</code> 中的版本对应的类或者方法【即把 <code>module-b</code> 中的依赖给排除掉】，否则编译会通过，但是打包后还是会缺失。</p><p>因为 <code>jar</code> 包中的源代码在编译阶段不会被检测调用的是哪个依赖里面的类或者方法【编译时只会检测我们写的代码】，必须是打包运行后才明确【其实运行前就会把所有 <code>jar</code> 包的类加载到 <code>JVM</code> 中，由于冲突会丢弃一些】，但是运行前的加载 <code>JVM</code> 过程对于多版本的依赖无法确定具体是哪个依赖生效，编译完成后到运行的时候【执行到 <code>jar</code> 中相应的代码】，就会出问题。注意这里虽然在 <code>module-b</code> 中对部分依赖做了 <code>shade</code>，但是只是对 <code>module-b</code> 生效，而对 <code>module-a</code> 是无效的，所以可能会导致 <code>module-a</code> 中的 <code>jar</code> 中源代码引用时找不到类或者方法，于是编译打包正常，运行时就会出现 <code>NoClassDefFoundError</code> 异常。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;最近因为协助升级相关业务的 &lt;code&gt;sdk&lt;/code&gt;，遇到过多次 &lt;code&gt;jar&lt;/code&gt; 包冲突的问题，此外自己在业务中升级算法接口的 &lt;code&gt;sdk&lt;/code&gt; 时，也遇到过 &lt;code&gt;jar&lt;/code&gt; 冲突问题。而且，这种冲突是灾难性的，不要指望通过排除特定包、升级版本、降级版本解决，根本无济于事，还会越来越混乱。&lt;/p&gt;&lt;p&gt;那么，最高效的方法是使用 &lt;code&gt;maven-shade-plugin&lt;/code&gt; 插件，只要加上冲突相关的 &lt;code&gt;relocation&lt;/code&gt; 配置，变更包名，即可迅速化解冲突的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="踩坑系列" scheme="https://www.playpi.org/categories/series-of-fixbug/"/>
    
    
      <category term="Maven" scheme="https://www.playpi.org/tags/Maven/"/>
    
      <category term="Java" scheme="https://www.playpi.org/tags/Java/"/>
    
      <category term="shade" scheme="https://www.playpi.org/tags/shade/"/>
    
  </entry>
  
  <entry>
    <title>Spark 项目依赖冲突问题总结</title>
    <link href="https://www.playpi.org/2019112901.html"/>
    <id>https://www.playpi.org/2019112901.html</id>
    <published>2019-11-29T12:05:46.000Z</published>
    <updated>2019-11-29T12:05:46.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --><p>今天遇到一个常见的依赖冲突问题，在一个 <code>Spark</code> 项目中，引用了多个其它项目的公共包【例如公共 <code>elt</code> 模块、算法模块】，在提交运行 <code>Spark</code> 任务时，由于依赖冲突而失败，高低版本无法兼容。</p><p>本文记录问题解决过程以及经验总结，重要开发环境说明：<code>Spark v1.6</code>、<code>es-hadoop v5.6.8</code>、<code>kafka v0.9.x</code> 。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 在一个 <code>SparkStreaming</code> 项目中，由于业务需要而新增加了算法模块的依赖【公司开放的公共 <code>jar</code> 包】，结果无法正常运行，根本原因在于依赖包冲突，版本无法完全匹配。</p><p>下面简单描述一下各种现象，这当然是为了给读者参考才这么做的，在实际开发过程中如果也这么尝试是很浪费时间的【当然对于初学者还是很有必要的，实际踩坑才知道痛苦】。</p><p>在一开始，添加算法模块的依赖后，使用本地 <code>local</code> 模式试运行程序正常，相关算法接口可用，但是当提交任务到 <code>Spark</code> 集群后【<code>standalone</code> 模式】，提交任务失败，出现 <code>kryo</code> 序列化异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">2019-11-26_18:00:32 [task-result-getter-0] WARN scheduler.TaskSetManager:70: Lost task 0.0 in stage 0.0 (TID 0, dev4): java.io.EOFException</span><br><span class="line">at org.apache.spark.serializer.KryoDeserializationStream.readObject (KryoSerializer.scala:232)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject (TorrentBroadcast.scala:217)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply (TorrentBroadcast.scala:178)</span><br><span class="line">at org.apache.spark.util.Utils$.tryOrIOException (Utils.scala:1205)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock (TorrentBroadcast.scala:165)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute (TorrentBroadcast.scala:64)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast._value (TorrentBroadcast.scala:64)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast.getValue (TorrentBroadcast.scala:88)</span><br><span class="line">at org.apache.spark.broadcast.Broadcast.value (Broadcast.scala:70)</span><br><span class="line">at org.apache.spark.scheduler.ResultTask.runTask (ResultTask.scala:62)</span><br><span class="line">at org.apache.spark.scheduler.Task.run (Task.scala:89)</span><br><span class="line">at org.apache.spark.executor.Executor$TaskRunner.run (Executor.scala:227)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2018/20191201000221.png" alt="启动 Spark 任务失败" title="启动 Spark 任务失败"></p><p>经过简单排查，上述错误的原因在于 <code>Spark</code> 需要依赖 <code>kryo v2.21</code>，而算法模块里面依赖了 <code>kryo v4.0.1</code>，在多版本同时存在的情况下，<code>Java</code> 类加载器加载到了高版本的 <code>kryo</code>【当然先加载到哪个类不确定，但是由前面的现象可以判定先加载了高版本的 <code>jar</code> 包】，导致 <code>Spark</code> 不兼容。</p><p>进一步想到可以将算法模块中的高版本 <code>kryo</code> 排除【当然此时没有考虑这样做对算法接口的影响】，我还就这么做了，又试了一次，结果出现以下异常【敏感包名使用 <code>xxx.yyy</code> 替换】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">java.lang.reflect.InvocationTargetException</span><br><span class="line">at org.apache.dubbo.common.bytecode.Wrapper0.invokeMethod (Wrapper0.java)</span><br><span class="line">at com.xxx.yyy.consumer.proxy.JavassistProxyFactory$1.doInvoke (JavassistProxyFactory.java:28)</span><br><span class="line">at com.xxx.yyy.consumer.proxy.AbstractProxyInvoker.doInvoke (AbstractProxyInvoker.java:57)</span><br><span class="line">at com.xxx.yyy.consumer.metric.ThanosConsumerMetric.makeMetric (ThanosConsumerMetric.java:90)</span><br><span class="line">at com.xxx.yyy.consumer.proxy.AbstractProxyInvoker.invoke (AbstractProxyInvoker.java:53)</span><br><span class="line">at com.xxx.yyy.consumer.proxy.InvokerInvocationHandler.invoke (InvokerInvocationHandler.java:36)</span><br><span class="line">at org.apache.dubbo.common.bytecode.proxy1.classify (proxy1.java)</span><br><span class="line">at com.xxx.zzz.analyz.rpc.ThanosRpcAlgorithmAnalyzer.classify (ThanosRpcAlgorithmAnalyzer.java:50)</span><br><span class="line">at com.xxx.zzz.analyz.rpc.ThanosRpcAlgorithmAnalyzer.classify (ThanosRpcAlgorithmAnalyzer.java:55)</span><br><span class="line">at com.xxx.zzz.analyz.rpc.ThanosRpcAlgorithmAnalyzer.main (ThanosRpcAlgorithmAnalyzer.java:70)</span><br><span class="line">Caused by: org.apache.dubbo.rpc.RpcException: Failed to invoke the method classify in the service com.xxx.yyy.service.Classifier. Tried 3 times of the providers [172.18.5.66:31142, 172.18.5.145:31142] (2/2) from the registry dev3:2181 on the consumer 172.18.7.203 using the dubbo version 2.7.3. Last error is: Failed to invoke remote method: classify, provider: dubbo://172.18.5.145:31142/com.xxx.yyy.service.Classifier?application=xxx-rpc-consumer&amp;check=false&amp;cluster=backpressure&amp;deprecated=false&amp;dubbo=2.0.2&amp;interface=com.xxx.yyy.service.Classifier&amp;lazy=false&amp;loadbalance=leastactive&amp;pid=10388&amp;qos.enable=false&amp;reference.filter=requestid,activelimit&amp;register.ip=172.18.7.203&amp;release=2.7.3&amp;remote.application=xxx-rpc-provider&amp;retries=2&amp;revision=0.1-20191122.093730-4&amp;serialization=kryo&amp;side=consumer&amp;sticky=false&amp;timeout=2147483647&amp;timestamp=1574327320883&amp;weight=16, cause: org.apache.dubbo.remoting.RemotingException: io.netty.handler.codec.EncoderException: java.lang.NoClassDefFoundError: com/esotericsoftware/kryo/pool/KryoFactory</span><br><span class="line">io.netty.handler.codec.EncoderException: java.lang.NoClassDefFoundError: com/esotericsoftware/kryo/pool/KryoFactory</span><br><span class="line">at io.netty.handler.codec.MessageToByteEncoder.write (MessageToByteEncoder.java:125)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:658)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:716)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:651)</span><br><span class="line">at io.netty.handler.timeout.IdleStateHandler.write (IdleStateHandler.java:266)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:658)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:716)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:651)</span><br><span class="line">at io.netty.channel.ChannelDuplexHandler.write (ChannelDuplexHandler.java:106)</span><br><span class="line">at org.apache.dubbo.remoting.transport.netty4.NettyClientHandler.write (NettyClientHandler.java:87)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:658)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.access$2000 (AbstractChannelHandlerContext.java:32)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write (AbstractChannelHandlerContext.java:939)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write (AbstractChannelHandlerContext.java:991)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run (AbstractChannelHandlerContext.java:924)</span><br><span class="line">at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks (SingleThreadEventExecutor.java:380)</span><br><span class="line">at io.netty.channel.nio.NioEventLoop.run (NioEventLoop.java:357)</span><br><span class="line">at io.netty.util.concurrent.SingleThreadEventExecutor$2.run (SingleThreadEventExecutor.java:116)</span><br><span class="line">at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run (DefaultThreadFactory.java:137)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">Caused by: java.lang.NoClassDefFoundError: com/esotericsoftware/kryo/pool/KryoFactory</span><br><span class="line">at java.lang.ClassLoader.defineClass1 (Native Method)</span><br><span class="line">at java.lang.ClassLoader.defineClass (ClassLoader.java:763)</span><br><span class="line">at java.security.SecureClassLoader.defineClass (SecureClassLoader.java:142)</span><br><span class="line">at java.net.URLClassLoader.defineClass (URLClassLoader.java:467)</span><br><span class="line">at java.net.URLClassLoader.access$100 (URLClassLoader.java:73)</span><br><span class="line">at java.net.URLClassLoader$1.run (URLClassLoader.java:368)</span><br><span class="line">at java.net.URLClassLoader$1.run (URLClassLoader.java:362)</span><br><span class="line">at java.security.AccessController.doPrivileged (Native Method)</span><br><span class="line">at java.net.URLClassLoader.findClass (URLClassLoader.java:361)</span><br><span class="line">at java.lang.ClassLoader.loadClass (ClassLoader.java:424)</span><br><span class="line">at sun.misc.Launcher$AppClassLoader.loadClass (Launcher.java:349)</span><br><span class="line">at java.lang.ClassLoader.loadClass (ClassLoader.java:357)</span><br><span class="line">at org.apache.dubbo.common.serialize.kryo.KryoObjectOutput.&lt;init&gt;(KryoObjectOutput.java:39)</span><br><span class="line">at org.apache.dubbo.common.serialize.kryo.KryoSerialization.serialize (KryoSerialization.java:51)</span><br><span class="line">at org.apache.dubbo.remoting.exchange.codec.ExchangeCodec.encodeRequest (ExchangeCodec.java:234)</span><br><span class="line">at org.apache.dubbo.remoting.exchange.codec.ExchangeCodec.encode (ExchangeCodec.java:69)</span><br><span class="line">at org.apache.dubbo.rpc.protocol.dubbo.DubboCountCodec.encode (DubboCountCodec.java:40)</span><br><span class="line">at org.apache.dubbo.remoting.transport.netty4.NettyCodecAdapter$InternalEncoder.encode (NettyCodecAdapter.java:70)</span><br><span class="line">at io.netty.handler.codec.MessageToByteEncoder.write (MessageToByteEncoder.java:107)</span><br><span class="line">... 19 more</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2018/20191201001332.png" alt="算法接口抛异常" title="算法接口抛异常"></p><p>这里可以明确得出的是，由于擅自排除了算法模块需要的高版本 <code>kryo</code>，现在算法接口无法提供服务了，缺失 <code>KryoFactory</code> 类。</p><p>没办法，只好对算法模块中的 <code>kryo</code> 做了影子复制，把包名 <code>com.esotericsoftware.kryo</code> 变更了一下，这样既不会影响到算法接口的使用，也不会影响到 <code>Spark</code> 任务提交。</p><p>好，<code>kryo</code> 的冲突问题解决了，但是紧接着又出现了 <code>netty</code> 冲突问题，现象类似，异常信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">2019-11-29_18:23:32 [appclient-register-master-threadpool-0] INFO client.AppClient$ClientEndpoint:58: Connecting to master spark://dev4:7077...</span><br><span class="line">2019-11-29_18:23:32 [shuffle-client-0] ERROR client.TransportClient:235: Failed to send RPC 8750922883607188033 to dev4/172.18.5.204:7077: java.lang.AbstractMethodError: org.apache.spark.network.protocol.MessageWithHeader.touch (Ljava/l</span><br><span class="line">ang/Object;) Lio/netty/util/ReferenceCounted;</span><br><span class="line">java.lang.AbstractMethodError: org.apache.spark.network.protocol.MessageWithHeader.touch (Ljava/lang/Object;) Lio/netty/util/ReferenceCounted;</span><br><span class="line">at io.netty.util.ReferenceCountUtil.touch (ReferenceCountUtil.java:77)</span><br><span class="line">at io.netty.channel.DefaultChannelPipeline.touch (DefaultChannelPipeline.java:116)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:785)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:701)</span><br><span class="line">at io.netty.handler.codec.MessageToMessageEncoder.write (MessageToMessageEncoder.java:112)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0 (AbstractChannelHandlerContext.java:716)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:708)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:791)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:701)</span><br><span class="line">at io.netty.handler.timeout.IdleStateHandler.write (IdleStateHandler.java:303)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0 (AbstractChannelHandlerContext.java:716)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:708)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.access$1700 (AbstractChannelHandlerContext.java:56)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write (AbstractChannelHandlerContext.java:1102)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write (AbstractChannelHandlerContext.java:1149)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run (AbstractChannelHandlerContext.java:1073)</span><br><span class="line">at io.netty.util.concurrent.AbstractEventExecutor.safeExecute (AbstractEventExecutor.java:163)</span><br><span class="line">at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks (SingleThreadEventExecutor.java:510)</span><br><span class="line">at io.netty.channel.nio.NioEventLoop.run (NioEventLoop.java:518)</span><br><span class="line">at io.netty.util.concurrent.SingleThreadEventExecutor$6.run (SingleThreadEventExecutor.java:1044)</span><br><span class="line">at io.netty.util.internal.ThreadExecutorMap$2.run (ThreadExecutorMap.java:74)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">2019-11-29_18:23:32 [appclient-register-master-threadpool-0] WARN client.AppClient$ClientEndpoint:91: Failed to connect to master dev4:7077</span><br><span class="line">java.io.IOException: Failed to send RPC 8750922883607188033 to dev4/172.18.5.204:7077: java.lang.AbstractMethodError: org.apache.spark.network.protocol.MessageWithHeader.touch (Ljava/lang/Object;) Lio/netty/util/ReferenceCounted;</span><br><span class="line">at org.apache.spark.network.client.TransportClient$3.operationComplete (TransportClient.java:239)</span><br><span class="line">at org.apache.spark.network.client.TransportClient$3.operationComplete (TransportClient.java:226)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.notifyListener0 (DefaultPromise.java:577)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.notifyListenersNow (DefaultPromise.java:551)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.notifyListeners (DefaultPromise.java:490)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.setValue0 (DefaultPromise.java:615)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.setFailure0 (DefaultPromise.java:608)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.tryFailure (DefaultPromise.java:117)</span><br><span class="line">at io.netty.util.internal.PromiseNotificationUtil.tryFailure (PromiseNotificationUtil.java:64)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.notifyOutboundHandlerException (AbstractChannelHandlerContext.java:818)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0 (AbstractChannelHandlerContext.java:718)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:708)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:791)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:701)</span><br><span class="line">at io.netty.handler.timeout.IdleStateHandler.write (IdleStateHandler.java:303)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0 (AbstractChannelHandlerContext.java:716)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:708)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.access$1700 (AbstractChannelHandlerContext.java:56)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write (AbstractChannelHandlerContext.java:1102)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2018/20191201001713.png" alt="netty 异常" title="netty 异常"></p><p>这个问题我见过很多次，通过简单排查发现 <code>Spark</code> 需要的是 <code>netty-all v4.0.29</code>，而算法模块需要的是 <code>v4.1.25</code>，我在本地看到实际加载的是 <code>v4.0.29</code>，这里 <code>Spark</code> 任务为什么提交失败我有疑惑【我只能怀疑服务器加载类的顺序和我本机的不一致，导致服务器上面实际加载的并不是 <code>Spark</code> 需要的版本】。</p><p>接着按照我的怀疑把高版本 <code>netty-all</code> 排除了，恢复正常【这里不需要复制影子，因为版本差别不大，算法模块可以兼容低版本 <code>netty-all</code> 依赖】。</p><p>但是，接着又出现 <code>org.apache.curator:curator-recipes</code> 依赖的问题，这是 <code>Spark</code> 任务读取 <code>kafka</code> 需要的依赖，而在算法模块中也需要。</p><p>异常信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: org.apache.curator.framework.api.CreateBuilder.creatingParentsIfNeeded () Lorg/apache/curator/framework/api/ProtectACLCreateModePathAndBytesable;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2018/20191201001855.png" alt="curator 异常" title="curator 异常"></p><p>其中，在 <code>Spark</code> 中需要的版本是 <code>v2.4.0</code>，而在算法模块中需要的是 <code>v4.0.1</code>，我看到实际加载的是 <code>v4.0.1</code>，所以 <code>Spark</code> 任务又失败了。</p><p>再按照这个节奏进行下去，读者是不是要疯掉了！好，我们到此为止，准备使用万能优雅的 <code>maven-shade-plugin</code> 插件解决这类让人抓狂的问题【只需要找到冲突的 <code>jar</code> 包替换包名，不需要排除】。</p><h1 id="问题分析解决"><a href="# 问题分析解决" class="headerlink" title="问题分析解决"></a>问题分析解决 </h1><h2 id="简单分析解决"><a href="# 简单分析解决" class="headerlink" title="简单分析解决"></a> 简单分析解决 </h2><p> 在上面的流程中，我会想到变更 <code>jar</code> 包依赖版本，或者移除多余的依赖，尝试让合适的版本出现，从而兼容代码中所有的调用。但是，遇到稍微复杂的情况这种做法显然是徒劳的。</p><p>诚然，这种方式针对单线程或者本地 <code>local</code> 模式运行的程序是可以生效的，但是对于集群模式的【<code>standalone</code>、<code>yarn</code> 等】<code>Spark</code> 任务，就无能为力了，很难恰好找到匹配的版本，毕竟公共包本身使用的依赖不是你能控制的，也不会为了你而做兼容【公共包面向大众发布，一般都会使用最新版本的依赖】。</p><p>接着详细来解释一下我这个典型场景，<code>Spark</code> 使用了一个低版本的 <code>kryo</code>，而算法模块使用了另外高版本的 <code>kyro</code>，但是诡异的是它们的依赖坐标不一致【算法模块是 <code>com.esotericsoftware:kryo</code>、<code>Spark</code> 是 <code>com.esotericsoftware.kryo:kryo</code>】，而实际类的包名却是一致的【都是 <code>com.esotericsoftware.kryo</code>】，这就导致类冲突无法兼容【在人们的经验中，<code>jar</code> 包坐标不同，类的包名也应该不同才对】。当然，<code>kryo</code> 高低版本之间的类不同也是无法兼容的原因之一。</p><p>如果选择移除算法模块的 <code>kryo</code>，调用算法接口时会报找不到类异常，如果移除 <code>Spark</code> 的 <code>kryo</code>，提交 <code>Spark</code> 任务时会报无法反序列化异常。</p><p>而且，比较让人崩溃的是，真的无法找到兼容两者的版本，那就只能利用 <code>maven-shade-plugin</code> 插件了。</p><p>我这里的项目本身使用的 <code>maven-shade-plugin</code> 插件是为了把所有的依赖都打包在一起，形成 <code>uber jar</code>，需要启动 <code>Spark</code> 任务时一起提交到集群。这样做主要是因为 <code>Spark</code> 集群的 <code>libs</code> 中没有存放任何公共依赖包，比较纯净，所以需要提交任务的客户端自己打包携带，这样也可以避免很多业务方共同使用同一个 <code>Spark</code> 集群产生依赖冲突问题。</p><p>无奈，最终只好决定使用 <code>maven-shade-plugin</code> 插件的高级功能：影子别名，直接变更类名，就不怕再冲突了。</p><p>使用 <code>maven-shade-plugin</code> 插件制作影子的相关类配置【把类的包名替换掉，避免冲突，根据项目的实际冲突情况而配置，这里仅供参考】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;relocations&gt;</span><br><span class="line">        &lt;relocation&gt;</span><br><span class="line">            &lt;pattern&gt;com.google&lt;/pattern&gt;</span><br><span class="line">            &lt;shadedPattern&gt;iplaypi.com.google&lt;/shadedPattern&gt;</span><br><span class="line">        &lt;/relocation&gt;</span><br><span class="line">        &lt;relocation&gt;</span><br><span class="line">            &lt;pattern&gt;io.netty&lt;/pattern&gt;</span><br><span class="line">            &lt;shadedPattern&gt;iplaypi.io.netty&lt;/shadedPattern&gt;</span><br><span class="line">        &lt;/relocation&gt;</span><br><span class="line">        &lt;relocation&gt;</span><br><span class="line">            &lt;pattern&gt;org.apache.curator&lt;/pattern&gt;</span><br><span class="line">            &lt;shadedPattern&gt;iplaypi.org.apache.curator&lt;/shadedPattern&gt;</span><br><span class="line">        &lt;/relocation&gt;</span><br><span class="line">        &lt;relocation&gt;</span><br><span class="line">            &lt;pattern&gt;com.esotericsoftware&lt;/pattern&gt;</span><br><span class="line">            &lt;shadedPattern&gt;iplaypi.com.esotericsoftware&lt;/shadedPattern&gt;</span><br><span class="line">        &lt;/relocation&gt;</span><br><span class="line">        &lt;relocation&gt;</span><br><span class="line">            &lt;pattern&gt;de.javakaffee&lt;/pattern&gt;</span><br><span class="line">            &lt;shadedPattern&gt;iplaypi.de.javakaffee&lt;/shadedPattern&gt;</span><br><span class="line">        &lt;/relocation&gt;</span><br><span class="line">    &lt;/relocations&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>我这里把 <code>guava</code>、<code>netty</code>、<code>curator</code>、<code>kryo</code> 全部制作影子了，仅供参考。</p><h2 id="抽象简化问题"><a href="# 抽象简化问题" class="headerlink" title="抽象简化问题"></a>抽象简化问题 </h2><p> 下面就用模型简化一下我遇到的这类场景，使用 <code>guava</code> 包冲突做示例。</p><p><code>Maven</code> 项目中有 <code>a</code>、<code>b</code>、<code>c</code> 三个模块【分散为三个模块读者更容易理解，解决问题思路也更清晰】，<code>a</code> 同时依赖了 <code>b</code>、<code>c</code>。其中，<code>b</code> 依赖了低版本 <code>guava</code> 并调用了一个低版本独有的方法，<code>c</code> 依赖了高版本 <code>guava</code> 并调用了高版本独有的方法【当然引用特有的类也行】。</p><p>它们之间的关系如下图：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209000340.png" alt="项目依赖关系" title="项目依赖关系"></p><p>在这个 <code>Maven</code> 项目中，发生 <code>jar</code> 包冲突很明显是因为，项目中依赖了同一个 <code>jar</code> 包的多个版本，而且分别调用了高低版本特有的方法，或者引用了高低版本特有的类。面对此类问题，一般的解决思路是只保留一个版本，排除掉不需要的版本，但是上面这种情况太特殊了，排除 <code>jar</code> 包不能解决问题。</p><p>可以试想一下，排除掉低版本 <code>guava</code> 的话 <code>b</code> 会报错，排掉高版本 <code>guava</code> 的话 <code>c</code> 会报错，所以希望在项目中同时使用低版本 <code>guava</code> 和高版本 <code>guava</code>。</p><p>那就只能使用 <code>maven-shade-plugin</code> 插件来构建影子 <code>jar</code> 包，替换类路径，制作影子的效果如下图【思路就是构建 <code>c</code> 时替换掉 <code>guava</code> 的包名】：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209000438.png" alt="shade 插件替换类路径" title="shade 插件替换类路径"></p><p>这样就可以非常优雅地解决问题，但是会导致打包的 <code>jar</code> 比以前大一点。如果 <code>Maven</code> 项目本身没有那么多模块，只有一个大模块，建议拆分，至少把有冲突的部分单独拆出来构建影子模块。</p><p>这个方案的详细说明以及代码演示读者可以参考我的另外一篇博文：<a href="https://www.playpi.org/2019120101.html">解决 jar 包冲突的神器：maven-shade-plugin</a> 。</p><h1 id="问题总结"><a href="# 问题总结" class="headerlink" title="问题总结"></a>问题总结 </h1><p> 在制作影子时，<code>Maven</code> 的子模块是必不可少的帮手，否则还需要下载源码自己重新打包，麻烦而且做法不合适。</p><p><code>Java</code> 项目拆分为子模块的好处之一，遇到依赖冲突时，可以很方便地使用 <code>maven-shade-plugin</code> 插件，分分钟就可以制作影子。例如上面的抽象简化例子，如果 <code>a</code>、<code>b</code>、<code>c</code> 没有拆分，一直是一个模块，遇到这种依赖冲突就没办法解决，怎么排除都是不行的，只能单独构建一个子模块用来制作影子。</p><p>当然，如果上面的 <code>c</code> 本身就依赖了很多 <code>jar</code> 包，它们之间在 <code>c</code> 模块中就有冲突，也不好制作影子，还是单独新建一个纯净的子模块比较好【例如把类似 <code>guava</code> 冲突的 <code>jar</code> 包以及代码抽出来，单独创建 <code>c-sub-shade</code> 模块，在里面制作影子，这个模块给 <code>c</code> 引用】。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209000650.png" alt="shade 子模块" title="shade 子模块"></p><p>注意，上图和我本文中遇到的例子有一点不同，我的 <code>jar</code> 包在 <code>c</code> 模块中并没有冲突，所以可以直接利用 <code>c</code> 制作影子模块 <code>c-shade</code>，当然不怕麻烦也可以制作 <code>c-sub-shade</code>。</p><p>此外，我在一年前也遇到过一种简单的场景：<a href="https://www.playpi.org/2018100801.html">Spark Kryo 异常</a>，当时直接通过排除依赖就解决问题了，但是这次的场景太复杂，只能启用 <code>maven-shade-plugin</code> 插件了。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;今天遇到一个常见的依赖冲突问题，在一个 &lt;code&gt;Spark&lt;/code&gt; 项目中，引用了多个其它项目的公共包【例如公共 &lt;code&gt;elt&lt;/code&gt; 模块、算法模块】，在提交运行 &lt;code&gt;Spark&lt;/code&gt; 任务时，由于依赖冲突而失败，高低版本无法兼容。&lt;/p&gt;&lt;p&gt;本文记录问题解决过程以及经验总结，重要开发环境说明：&lt;code&gt;Spark v1.6&lt;/code&gt;、&lt;code&gt;es-hadoop v5.6.8&lt;/code&gt;、&lt;code&gt;kafka v0.9.x&lt;/code&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="踩坑系列" scheme="https://www.playpi.org/categories/series-of-fixbug/"/>
    
    
      <category term="Spark" scheme="https://www.playpi.org/tags/Spark/"/>
    
      <category term="Maven" scheme="https://www.playpi.org/tags/Maven/"/>
    
      <category term="shade" scheme="https://www.playpi.org/tags/shade/"/>
    
  </entry>
  
  <entry>
    <title>使用海龟绘图绘制一些植物</title>
    <link href="https://www.playpi.org/2019110201.html"/>
    <id>https://www.playpi.org/2019110201.html</id>
    <published>2019-11-02T13:58:18.000Z</published>
    <updated>2019-11-02T13:58:18.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>在 2019 年 10 月 1 日的时候，我尝试使用海龟绘图绘制了一面五星红旗，参考我的另外一篇博文：<a href="https://www.playpi.org/2019100101.html">使用海龟绘图绘制一面五星红旗 </a> ，我觉得挺好玩的，还想进一步了解一下相关知识。后来，我又探索了一些绘图内容，发现可以绘制一些植物，例如树木、花草，核心就是要定义好绘制曲线。本文记录几个常见的植物：樱花树、火树银花、玫瑰花。</p><a id="more"></a><p> 提前声明，下文中涉及的 <code>Python</code> 脚本已经被我上传至 <code>GitHub</code>，读者可以提前下载查看：<a href="https://github.com/iplaypi/iplaypipython/tree/master/iplaypipython/20191102" target="_blank" rel="noopener">绘制植物脚本 </a> ，脚本命名使用英文单词作为前缀。</p><h1 id="樱花树"><a href="# 樱花树" class="headerlink" title="樱花树"></a> 樱花树 </h1><p> 画樱花树的整体思路就是先绘制樱花树，再绘制地上的落叶。</p><p>其中，绘制樱花树使用了递归的方式，从主干开始绘制，绘制主干完成后分为左右两侧的枝干，不停递归绘制，对于长度比较长的枝干，仍旧按照主干的方式绘制，直到长度比较短的枝干，作为树枝末端存在，会有不同的颜色、粗细。</p><p>代码示例如下，里面包含了注释，很容易就能看懂：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding=utf-8</span><br><span class="line"># 画一棵樱花树（模拟）</span><br><span class="line"># 导入 turtle 模块 </span><br><span class="line">import turtle</span><br><span class="line"># 导入 random 模块，每次绘制的樱花树形状随机 </span><br><span class="line">import random</span><br><span class="line">from turtle import *</span><br><span class="line">from time import sleep</span><br><span class="line"></span><br><span class="line"># 画樱花的躯干，传入躯干长度、画布 </span><br><span class="line"># 这里面会有递归调用 </span><br><span class="line"># 先画主干，然后递归画树枝，树枝越来越短，颜色会随机生成 </span><br><span class="line">def draw_tree (branchLen, t):</span><br><span class="line">    sleep (0.0005)</span><br><span class="line">    if branchLen &gt; 3:</span><br><span class="line">        # 末端的树枝 </span><br><span class="line">        if 8 &lt;= branchLen &lt;= 12:</span><br><span class="line">            # 随机生成画笔的颜色，用来画末端的树枝 </span><br><span class="line">            if random.randint (0,2) == 0:</span><br><span class="line">                # 白色 </span><br><span class="line">                t.color (&apos;snow&apos;)</span><br><span class="line">            else:</span><br><span class="line">                # 淡珊瑚色 </span><br><span class="line">                t.color (&apos;lightcoral&apos;)</span><br><span class="line">            # 画笔的线条粗细 </span><br><span class="line">            t.pensize (branchLen / 3)</span><br><span class="line">        elif branchLen &lt; 8:</span><br><span class="line">            if random.randint (0,1) == 0:</span><br><span class="line">                t.color (&apos;snow&apos;)</span><br><span class="line">            else:</span><br><span class="line">                t.color (&apos;lightcoral&apos;) # 淡珊瑚色 </span><br><span class="line">            t.pensize (branchLen / 2)</span><br><span class="line">        else:</span><br><span class="line">            # 树干的颜色赭 (zhě) 色、粗细 6</span><br><span class="line">            t.color (&apos;sienna&apos;)</span><br><span class="line">            t.pensize (branchLen / 10)</span><br><span class="line">        # 向前移动 branchLen 个像素 </span><br><span class="line">        t.forward (branchLen)</span><br><span class="line">        # 随机生成右转的角度 </span><br><span class="line">        a = 1.5 * random.random ()</span><br><span class="line">        t.right (20 * a)</span><br><span class="line">        # 递归画樱花树 </span><br><span class="line">        b = 1.5 * random.random ()</span><br><span class="line">        draw_tree (branchLen - 10 * b, t)</span><br><span class="line">        # 左转，递归画樱花树 </span><br><span class="line">        t.left (40 * a)</span><br><span class="line">        draw_tree (branchLen - 10 * b, t)</span><br><span class="line">        # 画笔回正方向，向前移动 </span><br><span class="line">        t.right (20 * a)</span><br><span class="line">        t.up ()</span><br><span class="line">        t.backward (branchLen)</span><br><span class="line">        t.down ()</span><br><span class="line"> </span><br><span class="line"># 掉落的花瓣，传入个数、画布 </span><br><span class="line">def draw_petal (m, t):</span><br><span class="line">    # 循环绘制 m 个花瓣 </span><br><span class="line">    for i in range (m):</span><br><span class="line">        # 生成随机的移动像素个数，a 用来控制左右的移动，b 用来控制上下的移动 </span><br><span class="line">        # a 大一点，b 小一点，总体可以让花瓣看起来有透视立体感 </span><br><span class="line">        a = 200 - 400 * random.random ()</span><br><span class="line">        b = 10 - 20 * random.random ()</span><br><span class="line">        # 以下就是到达花瓣位置 </span><br><span class="line">        # 提起画笔 </span><br><span class="line">        t.up ()</span><br><span class="line">        # 向前移动 b 个像素 </span><br><span class="line">        t.forward (b)</span><br><span class="line">        # 左转 90 度角度 </span><br><span class="line">        t.left (90)</span><br><span class="line">        # 向前移动 a 个像素 </span><br><span class="line">        t.forward (a)</span><br><span class="line">        # 放下画笔 </span><br><span class="line">        t.down ()</span><br><span class="line">        # 淡珊瑚色，花瓣的颜色 </span><br><span class="line">        t.color (&apos;lightcoral&apos;)</span><br><span class="line">        # 以下是绘制一个花瓣 </span><br><span class="line">        # 绘制一个圆 </span><br><span class="line">        t.circle (1)</span><br><span class="line">        # 以下就是回到中心点 </span><br><span class="line">        # 提起画笔 </span><br><span class="line">        t.up ()</span><br><span class="line">        # 向后移动 a 个像素 </span><br><span class="line">        t.backward (a)</span><br><span class="line">        # 右转 90 度角度 </span><br><span class="line">        t.right (90)</span><br><span class="line">        # 向后移动 b 个像素 </span><br><span class="line">        t.backward (b)</span><br><span class="line"></span><br><span class="line">def draw_cherry ():</span><br><span class="line">    # 海龟绘图区域 </span><br><span class="line">    t = turtle.Turtle ()</span><br><span class="line">    # 画布 </span><br><span class="line">    w = turtle.Screen ()</span><br><span class="line">    # 设置大小，4 个参数：宽度、高度、起始值 x 轴、起始值 y 轴 </span><br><span class="line">    w.setup (1000, 600, 200, 100)</span><br><span class="line">    # 设置背景为小麦颜色 </span><br><span class="line">    w.bgcolor (&apos;wheat&apos;)</span><br><span class="line">    # 隐藏画笔 </span><br><span class="line">    t.hideturtle ()</span><br><span class="line">    # 获取屏幕，并追踪 </span><br><span class="line">    t.getscreen ().tracer (5, 0)</span><br><span class="line">    t.left (90)</span><br><span class="line">    t.up ()</span><br><span class="line">    t.backward (200)</span><br><span class="line">    t.down ()</span><br><span class="line">    # 1、画樱花的躯干 </span><br><span class="line">    draw_tree (60, t)</span><br><span class="line">    # 2、画掉落的花瓣 </span><br><span class="line">    draw_petal (200, t)</span><br><span class="line">    # 3、点击退出 </span><br><span class="line">    w.exitonclick ()</span><br><span class="line"></span><br><span class="line"># 程序入口 </span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    print (&apos; 开始绘制樱花树 & apos;)</span><br><span class="line">    draw_cherry ()</span><br><span class="line">    print (&apos; 结束绘制樱花树 & apos;)</span><br><span class="line">    # input (&apos; 暂停，等待输入（输入任意内容按回车键可退出）：&apos;)</span><br></pre></td></tr></table></figure><p>运行结果如下，由于角度是随机生成的，所以每次运行结果都会不一样：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191105000726.png" alt="运行结果 1" title="运行结果 1"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191105000732.png" alt="运行结果 2" title="运行结果 2"></p><h1 id="火树银花"><a href="# 火树银花" class="headerlink" title="火树银花"></a>火树银花 </h1><p> 绘制火树银花的思路和上面的樱花树一致，只不过火树银花这个名字比较酷，树枝没有区分粗细，只区分长度、颜色，整个画面采用黑色背景，看起来非常闪耀。</p><p>需要注意的是，运行一次耗时比较长，大概需要 4-5 分钟。</p><p>代码内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"># !/usr/bin/python3</span><br><span class="line"># -*-coding:UTF-8-*-</span><br><span class="line"># 火树银花 </span><br><span class="line"></span><br><span class="line"># 导入海龟作图模块 </span><br><span class="line">import turtle</span><br><span class="line"># 导入随机数模块 </span><br><span class="line">import random as rm</span><br><span class="line"></span><br><span class="line"># 角度 </span><br><span class="line">angle = [15, 5, 10, 20, 25, 30]</span><br><span class="line"># 颜色数组，多种颜色供绘制时随机选择，青、红、粉、蓝、绿、黄 </span><br><span class="line">color = [&apos;yellow&apos;, &apos;green&apos;, &apos;blue&apos;, &apos;red&apos;, &apos;pink&apos;, &apos;cyan&apos;]</span><br><span class="line"></span><br><span class="line"># 绘制树干，传入长度、画布对象 </span><br><span class="line"># 绘制思路：根据长度的不同，生成的角度不同，树干会分为 2 个树枝，然后树枝再递归分叉 </span><br><span class="line"># 直到树枝的长度过小，变为树枝末梢，不再分叉 </span><br><span class="line">def draw_tree (branch_len, t, cr):</span><br><span class="line">    # 树干颜色 </span><br><span class="line">    t.color (cr)</span><br><span class="line">    # 设置画笔的粗细 </span><br><span class="line">    t.pensize (1)</span><br><span class="line">    # 随机选择颜色，不等于树干颜色，用于分叉树枝 </span><br><span class="line">    new_color = color [:]</span><br><span class="line">    new_color.remove (cr)</span><br><span class="line">    new_cr = rm.choice (new_color)</span><br><span class="line">    # 随机转动角度，用于分叉树枝 </span><br><span class="line">    ag1 = rm.choice (angle)</span><br><span class="line">    ag2 = rm.choice (angle)</span><br><span class="line">    # 分叉树枝的长度，默认等于树干的长度 </span><br><span class="line">    new_branch_len = branch_len</span><br><span class="line">    # 分叉树枝的长度重新计算，越来越短 </span><br><span class="line">    if branch_len &gt; 120:</span><br><span class="line">        new_branch_len = branch_len - 20</span><br><span class="line">    elif branch_len &gt;= 60:</span><br><span class="line">        new_branch_len = branch_len - 15</span><br><span class="line">    elif branch_len &gt;= 20:</span><br><span class="line">        new_branch_len = branch_len - 10</span><br><span class="line">    else:</span><br><span class="line">        new_branch_len = branch_len - 5</span><br><span class="line">    # 开始绘制 </span><br><span class="line">    if 10 &gt;= branch_len:</span><br><span class="line">        # 树枝太短，无需绘制，递归结束 </span><br><span class="line">        pass</span><br><span class="line">    else:</span><br><span class="line">        # 向前移动，绘制树干 </span><br><span class="line">        t.forward (branch_len)</span><br><span class="line">        # 右转指定角度 1，分叉 </span><br><span class="line">        t.right (ag1)</span><br><span class="line">        # 递归画树干，可以理解成子树 </span><br><span class="line">        draw_tree (new_branch_len, t, new_cr)</span><br><span class="line">        # 左转指定角度 2，分叉 </span><br><span class="line">        t.left (ag1 + ag2)</span><br><span class="line">        draw_tree (new_branch_len, t, new_cr)</span><br><span class="line">        # 角度回正，右转指定角度 2</span><br><span class="line">        t.right (ag2)</span><br><span class="line">        # 恢复颜色并后退 </span><br><span class="line">        t.color (cr)</span><br><span class="line">        t.backward (branch_len)</span><br><span class="line"></span><br><span class="line"># 开始绘制整棵树 </span><br><span class="line">def draw_fire_cilver ():</span><br><span class="line">    t = turtle.Turtle ()</span><br><span class="line">    w = turtle.Screen ()</span><br><span class="line">    # 设置背景为黑色 </span><br><span class="line">    w.bgcolor (&apos;black&apos;)</span><br><span class="line">    # 设置弹框大小，4 个参数：宽度、高度、起始值 x 轴、起始值 y 轴 </span><br><span class="line">    w.setup (1200, 800, 200, 50)</span><br><span class="line">    # 加快速度 </span><br><span class="line">    t.speed (10)</span><br><span class="line">    # 调整画笔的位置，开始的位置在中间偏下方 </span><br><span class="line">    t.left (90)</span><br><span class="line">    t.up ()</span><br><span class="line">    t.backward (400)</span><br><span class="line">    t.down ()</span><br><span class="line">    # 跟踪画笔，可以看到整个绘制轨迹 </span><br><span class="line">    turtle.tracer (5)</span><br><span class="line">    # 垂直位置绘制 1 棵，左右边再各绘制 3 棵，共 7 棵 </span><br><span class="line">    t.left (15)</span><br><span class="line">    for i in range (0,7):</span><br><span class="line">        # 绘制 1 棵，右转 5 度 </span><br><span class="line">        print (&apos;==== 绘制第 [&apos; + str (i + 1) + &apos;] 棵树 & apos;)</span><br><span class="line">        draw_tree (150, t, &apos;cyan&apos;)</span><br><span class="line">        t.right (5)</span><br><span class="line">    # 单机退出 </span><br><span class="line">    w.exitonclick ()</span><br><span class="line"></span><br><span class="line"># 主程序入口 </span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    print (&apos; 开始绘制火树银花 & apos;)</span><br><span class="line">    draw_fire_cilver ()</span><br><span class="line">    print (&apos; 结束绘制火树银花 & apos;)</span><br><span class="line">    # input (&apos; 暂停，等待输入（输入任意内容按回车键可退出）：&apos;)</span><br></pre></td></tr></table></figure><p>运行结果如下图，由于角度、颜色也是随机生成的，所以每次运行结果是不一致的。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191105001116.png" alt="运行结果" title="运行结果"></p><p>在网络上找到的示例，看起来更好看一些。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191105001233.png" alt="网络上找到的示例" title="网络上找到的示例"></p><h1 id="玫瑰花"><a href="# 玫瑰花" class="headerlink" title="玫瑰花"></a>玫瑰花 </h1><p> 玫瑰花比较有意思，会涉及到非规则图形，花瓣的形状怎么绘制、绿叶的形状怎么绘制等。</p><p>简单思路：</p><ul><li>先绘制花瓣的边框，包括填充颜色 </li><li> 再绘制花瓣中的线条，凸显出花瓣的层次 </li><li> 绘制花枝主干 </li><li> 绘制两片绿叶，包括绿叶的枝条 </li></ul><p> 代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line"># 导入海龟绘图模块 </span><br><span class="line">import turtle as t</span><br><span class="line"></span><br><span class="line"># 定义一个曲线绘制函数 </span><br><span class="line"># 思路就是画多个小圆弧，构成曲线 </span><br><span class="line"># n 表示画多少次圆弧，n 越大画的曲线越长 </span><br><span class="line"># r 表示圆弧半径，r 越大则曲线越平滑 </span><br><span class="line"># d=1 则是左弯的圆弧，d=-1 则是右弯的圆弧（由于屏幕的分辨率不同，有时候看不出来明显的弯度）</span><br><span class="line">def degree_curve (n, r, d=1):</span><br><span class="line">    for i in range (n):</span><br><span class="line">        t.left (d)</span><br><span class="line">        # r 是半径，abs (d) 是夹角 </span><br><span class="line">        t.circle (r, abs (d))</span><br><span class="line"></span><br><span class="line"># 绘制玫瑰花 </span><br><span class="line">def draw_rose (s):</span><br><span class="line">    # 设置画笔速度 </span><br><span class="line">    t.speed (100)</span><br><span class="line">    # 提起画笔，移动到指定位置 </span><br><span class="line">    t.penup ()</span><br><span class="line">    t.goto (0, 900 * s)</span><br><span class="line">    # 放下画笔 </span><br><span class="line">    t.pendown ()</span><br><span class="line"></span><br><span class="line">    # 开始填充，并绘制花朵形状 </span><br><span class="line">    t.begin_fill ()</span><br><span class="line">    # 起步花蕊的曲线，30 度圆弧，一层椭圆下侧 </span><br><span class="line">    t.circle (200 * s, 30)</span><br><span class="line">    # 左弯曲线，60 次半径为 10 的圆弧，一层椭圆右侧 </span><br><span class="line">    degree_curve (60, 50 * s)</span><br><span class="line">    # 和起步花蕊的曲线对称，30 度圆弧 </span><br><span class="line">    t.circle (200 * s, 30)</span><br><span class="line">    # 左弯曲线，4 次半径为 20 的圆弧，为了调整角度 </span><br><span class="line">    degree_curve (4, 100 * s)</span><br><span class="line">    # 50 度圆弧，一层椭圆上侧 </span><br><span class="line">    t.circle (200 * s, 50)</span><br><span class="line">    # 左弯曲线，50 次半径为 10 的圆弧，一层椭圆左侧下侧 </span><br><span class="line">    degree_curve (50, 50 * s)</span><br><span class="line">    # 65 度圆弧，一层椭圆下侧 </span><br><span class="line">    t.circle (350 * s, 65)</span><br><span class="line">    # 左弯曲线，40 次半径为 14 的圆弧，二层椭圆右侧 </span><br><span class="line">    degree_curve (40, 70 * s)</span><br><span class="line">    # 50 度圆弧，二层椭圆右侧上侧 </span><br><span class="line">    t.circle (150 * s, 50)</span><br><span class="line">    # 右弯曲线，20 次半径为 10 的圆弧，二层椭圆上侧 </span><br><span class="line">    degree_curve (20, 50 * s, -1)</span><br><span class="line">    # 60 度圆弧，二层椭圆上侧 </span><br><span class="line">    t.circle (400 * s, 60)</span><br><span class="line">    # 右弯曲线，18 次半径为 10 的圆弧，二层椭圆左侧 </span><br><span class="line">    degree_curve (18, 50 * s)</span><br><span class="line">    # 前进 125，直线，二层椭圆左侧连接处 </span><br><span class="line">    t.fd (250 * s)</span><br><span class="line">    # 右转 150 度 </span><br><span class="line">    t.right (150)</span><br><span class="line">    # 12 度圆弧，顺时针画圆，右弯曲线 </span><br><span class="line">    t.circle (-500 * s, 12)</span><br><span class="line">    # 左转 140 度 </span><br><span class="line">    t.left (140)</span><br><span class="line">    # 110 度圆弧，左侧花瓣边缘 </span><br><span class="line">    t.circle (550 * s, 110)</span><br><span class="line">    # 左转 27 度 </span><br><span class="line">    t.left (27)</span><br><span class="line">    # 100 度圆弧，右侧花瓣边缘 </span><br><span class="line">    t.circle (650 * s, 100)</span><br><span class="line">    # 左转 130 度 </span><br><span class="line">    t.left (130)</span><br><span class="line">    # 20 度圆弧，顺时针画圆 </span><br><span class="line">    t.circle (-300 * s, 20)</span><br><span class="line">    # 右转 123 度 </span><br><span class="line">    t.right (123)</span><br><span class="line">    # 57 度圆弧，连接到二层椭圆右侧 </span><br><span class="line">    t.circle (220 * s, 57)</span><br><span class="line">    # 至此图形封闭，颜色填充完成 </span><br><span class="line">    t.end_fill ()</span><br><span class="line"></span><br><span class="line">    # 绘制花枝形状，包括勾勒花瓣中间的线条 </span><br><span class="line">    # 左转 120 度 </span><br><span class="line">    t.left (120)</span><br><span class="line">    # 前进 140</span><br><span class="line">    t.fd (280 * s)</span><br><span class="line">    # 左转 115 度 </span><br><span class="line">    t.left (115)</span><br><span class="line">    # 33 度圆弧，连接到右侧花瓣边缘 </span><br><span class="line">    t.circle (300 * s, 33)</span><br><span class="line">    # 左转 180 度 </span><br><span class="line">    t.left (180)</span><br><span class="line">    # 33 度圆弧，顺时针，为了回到上一步画圆弧之前的位置 </span><br><span class="line">    t.circle (-300 * s, 33)</span><br><span class="line">    # 右弯曲线，70 次半径为 113 的圆弧，右侧花瓣线条 </span><br><span class="line">    degree_curve (70, 225 * s, -1)</span><br><span class="line">    # 104 度圆弧，右侧花瓣线条 </span><br><span class="line">    t.circle (350 * s, 104)</span><br><span class="line">    # 左转 90 度 </span><br><span class="line">    t.left (90)</span><br><span class="line">    # 105 度圆弧，左侧花瓣线条 </span><br><span class="line">    t.circle (200 * s, 105)</span><br><span class="line">    # 63 度弧度，顺时针，左侧花瓣线条，至此花瓣线条完成 </span><br><span class="line">    t.circle (-500 * s, 63)</span><br><span class="line">    # 提起画笔，移动到指定位置，花瓣与花枝连接处 </span><br><span class="line">    t.penup ()</span><br><span class="line">    t.goto (170 * s, -30 * s)</span><br><span class="line">    # 放下画笔 </span><br><span class="line">    t.pendown ()</span><br><span class="line">    # 左转 160 度，朝向调整为朝下 </span><br><span class="line">    t.left (160)</span><br><span class="line">    # 左弯曲线，20 次半径为 1250 的圆弧，花枝 </span><br><span class="line">    degree_curve (20, 2500 * s)</span><br><span class="line">    # 右弯曲线，220 次半径为 125 的圆弧，花枝 </span><br><span class="line">    degree_curve (220, 250 * s, -1)</span><br><span class="line"></span><br><span class="line">    # 下面开始绘制 2 片绿叶 </span><br><span class="line">    # 绘制一个绿色叶子，上方的 </span><br><span class="line">    t.fillcolor (&apos;green&apos;)</span><br><span class="line">    # 提起画笔移动到指定位置，叶尖 </span><br><span class="line">    t.penup ()</span><br><span class="line">    t.goto (670 * s, -180 * s)</span><br><span class="line">    t.pendown ()</span><br><span class="line">    # 右转 140 度，调整角度 </span><br><span class="line">    t.right (140)</span><br><span class="line">    # 开始填充 </span><br><span class="line">    t.begin_fill ()</span><br><span class="line">    # 120 度弧度，绿叶上侧 </span><br><span class="line">    t.circle (300 * s, 120)</span><br><span class="line">    # 左转 60 度 </span><br><span class="line">    t.left (60)</span><br><span class="line">    # 120 度弧度，绿叶下侧 </span><br><span class="line">    t.circle (300 * s, 120)</span><br><span class="line">    # 完成填充 </span><br><span class="line">    t.end_fill ()</span><br><span class="line">    t.penup ()</span><br><span class="line">    # 移动到绿叶枝条起始处 </span><br><span class="line">    t.goto (180 * s, -550 * s)</span><br><span class="line">    t.pendown ()</span><br><span class="line">    # 右转 85 度 </span><br><span class="line">    t.right (85)</span><br><span class="line">    # 40 度圆弧，绿叶枝条 </span><br><span class="line">    t.circle (600 * s, 40)</span><br><span class="line">    </span><br><span class="line">    # 绘制另一个绿色叶子，下方的 </span><br><span class="line">    # 提笔，移动到叶尖 </span><br><span class="line">    t.penup ()</span><br><span class="line">    t.goto (-150 * s, -1000 * s)</span><br><span class="line">    t.pendown ()</span><br><span class="line">    t.begin_fill ()</span><br><span class="line">    # 右转 120 度，调整角度 </span><br><span class="line">    t.rt (120)</span><br><span class="line">    # 115 度圆弧，叶子下侧 </span><br><span class="line">    t.circle (300 * s, 115)</span><br><span class="line">    # 左转 75 度 </span><br><span class="line">    t.left (75)</span><br><span class="line">    # 100 度弧度，叶子上侧 </span><br><span class="line">    t.circle (300 * s, 100)</span><br><span class="line">    t.end_fill ()</span><br><span class="line">    t.penup ()</span><br><span class="line">    # 移动到绿叶枝条起始处 </span><br><span class="line">    t.goto (430 * s, -1070 * s)</span><br><span class="line">    t.pendown ()</span><br><span class="line">    # 右转 30 度，调整角度 </span><br><span class="line">    t.right (30)</span><br><span class="line">    # 35 度圆弧，右弯，叶子枝条 </span><br><span class="line">    t.circle (-600 * s, 35)</span><br><span class="line">    # 等待退出 </span><br><span class="line">    t.exitonclick ()</span><br><span class="line"></span><br><span class="line"># 程序入口 </span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    print (&apos; 开始绘制玫瑰花 & apos;)</span><br><span class="line">    # 比例设定 </span><br><span class="line">    s = 0.2</span><br><span class="line">    # 设置弹窗大小 </span><br><span class="line">    t.setup (500 * 5 * s, 750 * 5 * s)</span><br><span class="line">    # 背景颜色，小麦色 </span><br><span class="line">    t.bgcolor (&apos;wheat&apos;)</span><br><span class="line">    # 设置画笔颜色，黑色 </span><br><span class="line">    t.pencolor (&quot;black&quot;)</span><br><span class="line">    # 设置填充颜色为红色，绘制花朵 </span><br><span class="line">    t.fillcolor (&quot;red&quot;)</span><br><span class="line">    draw_rose (s)</span><br><span class="line">    print (&apos; 结束绘制玫瑰花 & apos;)</span><br><span class="line">    # input (&apos; 暂停，等待输入（输入任意内容按回车键可退出）：&apos;)</span><br></pre></td></tr></table></figure><p>这里面的重点就是 <code>degree_curve (n, r, d=1)</code> 方法，它是为了绘制不规则图形而定义的。此外用的次数比较多的就是海龟绘图内置的 <code>circle</code> 方法，用来绘制标准的圆弧。</p><p>运行结果如下图，包含花瓣、绿叶。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191117003518.png" alt="玫瑰花运行结果" title="玫瑰花运行结果"></p><h1 id="参考"><a href="# 参考" class="headerlink" title="参考"></a>参考</h1><p><code>Python</code> 官方文档：<a href="https://docs.python.org/zh-cn/3/library/turtle.html" target="_blank" rel="noopener">Python3 文档说明</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在 2019 年 10 月 1 日的时候，我尝试使用海龟绘图绘制了一面五星红旗，参考我的另外一篇博文：&lt;a href=&quot;https://www.playpi.org/2019100101.html&quot;&gt;使用海龟绘图绘制一面五星红旗&lt;/a&gt; ，我觉得挺好玩的，还想进一步了解一下相关知识。后来，我又探索了一些绘图内容，发现可以绘制一些植物，例如树木、花草，核心就是要定义好绘制曲线。本文记录几个常见的植物：樱花树、火树银花、玫瑰花。&lt;/p&gt;
    
    </summary>
    
      <category term="知识改变生活" scheme="https://www.playpi.org/categories/knowledge-for-life/"/>
    
    
      <category term="Python" scheme="https://www.playpi.org/tags/Python/"/>
    
      <category term="Turtle" scheme="https://www.playpi.org/tags/Turtle/"/>
    
      <category term="cherry" scheme="https://www.playpi.org/tags/cherry/"/>
    
      <category term="tree" scheme="https://www.playpi.org/tags/tree/"/>
    
      <category term="rose" scheme="https://www.playpi.org/tags/rose/"/>
    
  </entry>
  
  <entry>
    <title>重装系统后软件环境清单</title>
    <link href="https://www.playpi.org/2019102401.html"/>
    <id>https://www.playpi.org/2019102401.html</id>
    <published>2019-10-23T16:37:51.000Z</published>
    <updated>2019-10-23T16:37:51.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>最近又重装电脑系统了，<code>Windows 10</code>，需要搞很多开发工具的初始化，整个流程操作下来感觉挺麻烦的，而且显得很混乱。因此，我整理这一份文档，把基础环境的安装过程记录下来，需要安装哪些工具、配置哪些参数，都一一列举，为以后的重装系统做指导，同时也给部分读者参考。</p><p>以后如果有新增内容会持续补充。</p><a id="more"></a><h1 id="基础环境"><a href="# 基础环境" class="headerlink" title="基础环境"></a>基础环境 </h1><ol><li><p><code>JDK</code>，<code>Java</code> 开发基础工具，包含虚拟机、依赖包等。</p></li><li><p><code>Maven</code>，项目管理工具，非常流行。</p></li><li><p><code>Git</code>，版本控制系统，非常流行。</p></li><li><p><code>Nodejs</code>，一个流行的 <code>js</code> 库。</p></li><li><p><code>Python</code>，<code>Python</code> 开发基础工具。</p></li><li><p><code>Shadiwsocks</code>，一款代理客户端。</p></li><li><p><code>Chrome</code> 浏览器，一款流行全球的浏览器。</p></li><li><p><code>Gradle</code>，项目管理工具。</p></li><li><p><code>Scala</code>，<code>Scala</code> 开发基础工具。</p></li><li><p><code>Memory Analyzer</code>，一款内存诊断分析工具。</p></li></ol><h1 id="开发工具"><a href="# 开发工具" class="headerlink" title="开发工具"></a> 开发工具 </h1><ol><li><p><code>IDEA</code>，<code>IntelliJ</code> 系列的 <code>Java</code> 开发工具，非常流行。</p></li><li><p><code>PyCharm</code>，<code>IntelliJ</code> 系列的 <code>Python</code> 开发工具，非常流行。</p></li><li><p><code>Notepad++</code>，文本编辑器，小巧好用【但是作者是 <code>td</code>，可以废弃】。</p></li><li><p><code>Sublime</code>，文本编辑器，好用。</p></li><li><p><code>Navicat</code>，一款 <code>MySQL</code> 可视化管理工具，好用。</p></li></ol><p> 曾经在某个论坛发现有人贡献了一枚注册码：<code>NAVN-LNXG-XHHX-5NOO</code>，用户名可以任意指定。</p><ol start="6"><li><p><code>Aria2</code>，一款下载工具，支持多种协议，线程数可以自定义。</p></li><li><p><code>Typora</code>，一款 <code>Markdown</code> 编辑器，简洁好用。</p></li><li><p><code>Xshell</code>，一款 <code>ssh</code> 工具。</p></li><li><p><code>Xftp</code>，一款 <code>ftp</code> 工具，用来传输文件。</p></li><li><p><code>Everything</code>，文件快速搜索工具，索引创建成功后，基本秒出，比 <code>Windows</code> 自带的文件浏览器快得多。</p></li><li><p><code>Android Studio</code>，一款安卓开发工具，非常流行。</p></li><li><p><code>AndroidKiller</code>，一款安卓逆向工具。</p></li><li><p><code>IDAPro</code>，一款逆向开发工具。</p></li><li><p><code>Wireshark</code>，一款网络抓包工具。</p></li><li><p><code>PostMan</code>，一款 <code>HTTP</code> 调试工具，非常好用。</p></li><li><p><code>xMind</code>，一款脑图工具。</p></li><li><p><code>RedisDesktop</code>，一款 <code>Redis</code> 可视化管理工具。</p></li><li><p><code>picGo</code>，图床上传工具。</p></li><li><p><code>ProcessExplorer</code>，<code>Windows</code> 进程查看工具。</p></li><li><p><code>Imagine</code>，一款图片压缩工具，压缩大小而不失真。</p></li><li><p><code>EditPlus</code>，文本编辑器。</p></li></ol><h1 id="附加工具"><a href="# 附加工具" class="headerlink" title="附加工具"></a>附加工具 </h1><ol><li><p> 迅雷下载，一款下载工具。</p></li><li><p>迅雷看看，一款视频播放工具。</p></li><li><p><code>PhotoShop</code>，即大家所说的 <code>PS</code>，用来修图。</p></li><li><p><code>Premiere</code>，即大家所说的 <code>PR</code>，用来剪辑视频。</p></li><li><p><code>ffmpeg</code>，视频剪辑命令行工具，小巧好用，例如截取、拼接、格式转换。</p></li><li><p><code>EmEditor</code>，专为 <code>csv</code> 文件开发，可以打开超大文件，例如几 <code>GB</code>、几十万行的文件可以被轻松打开【当然，操作系统的内存要大一点，例如 <code>8GB</code>、<code>16GB</code>】。</p></li></ol><p>曾经逛相关论坛，发现有人贡献了几个注册码，有效期到 2021 年 6 月份，可以试用一下【注册码可以开启一些高级功能】：</p><ul><li>DEAZV-27TFM-BL52D-PVN9L-ADULD，2021-06-11 到期 </li><li>DEAZW-38TGM-HH52D-XG5WR-FX4QW，2021-06-11 到期</li><li>DMAZW-48TGM-LQ52C-G82V6-2JJUC，2021-06-10 到期</li><li>DMAZW-4ATGM-QL52D-M6XEM-TCFCS，2021-06-11 到期</li></ul><ol start="7"><li><p><code>IDM</code>，即 <code>Internet Download Manager</code>，网络下载工具，支持多种协议，例如可以下载 <code>Youtube</code> 的视频</p></li><li><p><code>Tomcat</code>，一款服务器。</p></li><li><p><code>Nginx</code>，一款服务器。</p></li><li><p><code>Ditto</code>，好用的粘贴板工具，高效便捷。</p></li><li><p><code>HandShaker</code>，锤子科技的手机文件管理器，可以方便把手机连接至电脑。</p></li><li><p><code>Office</code>，微软系列的办公软件，例如 <code>Word</code>、<code>Excel</code>、<code>PPT</code> 等。</p></li><li><p><code>openVPN</code>，<code>VPN</code> 连接工具。</p></li><li><p><code>UItraCompare</code>，一款文件比较器，<code>UItra</code> 系列，另外还有一个 <code>UItraEdit</code>，文本编辑器。</p></li><li><p><code>WinRAR</code>，一款解压、压缩工具。</p></li><li><p> 易我数据恢复，一款数据恢复工具。</p></li><li><p><code>WPS</code>，金山系列的办公软件，例如 <code>Word</code>、<code>Excel</code>、<code>PPT</code> 等。</p></li><li><p><code>youtubu-dl</code>，一款下载工具，使用命令行操作，需要搭配 <code>Aria2</code> 使用</p></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;最近又重装电脑系统了，&lt;code&gt;Windows 10&lt;/code&gt;，需要搞很多开发工具的初始化，整个流程操作下来感觉挺麻烦的，而且显得很混乱。因此，我整理这一份文档，把基础环境的安装过程记录下来，需要安装哪些工具、配置哪些参数，都一一列举，为以后的重装系统做指导，同时也给部分读者参考。&lt;/p&gt;&lt;p&gt;以后如果有新增内容会持续补充。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="https://www.playpi.org/categories/essay/"/>
    
    
      <category term="Windows" scheme="https://www.playpi.org/tags/Windows/"/>
    
      <category term="init" scheme="https://www.playpi.org/tags/init/"/>
    
  </entry>
  
  <entry>
    <title>HBase 错误：The node hbase is not in ZooKeeper</title>
    <link href="https://www.playpi.org/2019101901.html"/>
    <id>https://www.playpi.org/2019101901.html</id>
    <published>2019-10-19T12:15:11.000Z</published>
    <updated>2019-10-19T12:15:11.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>使用 <code>phoenix</code> 向 <code>HBase</code> 中导入数据，使用的是 <code>phoenix</code> 自带的脚本 <code>psql.py</code>，结果报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">19/10/18 11:47:29 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br></pre></td></tr></table></figure><p>看起来是 <code>ZooKeeper</code> 环境有问题，本文记录解决过程。</p><p>本文开发环境基于 <code>HBase v1.1.2</code>、<code>phoenix v4.2.0</code> 。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 使用 <code>phoenix</code> 自带的导数脚本 <code>psql.py</code>，执行导入操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psql.py -t YOUR_TABLE dev4:2181 ./content.csv</span><br></pre></td></tr></table></figure><p>其中，<code>dev4:2191</code> 是 <code>Zookeeper</code> 集群节点，<code>./content.csv</code> 是数据文件，结果出现异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">19/10/18 11:47:29 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br><span class="line">19/10/18 11:47:29 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br><span class="line">19/10/18 11:47:29 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br><span class="line">19/10/18 11:47:30 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br><span class="line">19/10/18 11:47:31 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191019204203.png" alt="导入数据异常" title="导入数据异常"></p><p>看起来是 <code>Zookeeper</code> 中缺失 <code>/hbase</code> 节点目录。</p><h1 id="问题解决"><a href="# 问题解决" class="headerlink" title="问题解决"></a>问题解决 </h1><p> 从 <code>stackoverflow</code> 上面查到一条类似的问题，见备注链接。</p><p>表面原因的确是 <code>Zookeeper</code> 中缺失 <code>/hbase</code> 节点目录，因为 <code>phoenix</code> 需要从这个节点获取 <code>HBase</code> 集群的信息，例如表结构，节点目录缺失则无法获取。</p><p>查看 <code>conf/hbase-site.xml</code> 文件，找到配置项：<code>zookeeper.znode.parent</code>，它就是表示 <code>HBase</code> 在 <code>ZooKeeper</code> 中的管理目录，里面存储着关于 <code>HBase</code> 集群的各项重要信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zookeeper.znode.parent&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/hbase-unsecure&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>再去查看 <code>conf/hbase-env.sh</code> 里面的配置信息：<code>HBASE_MANAGES_ZK</code>，这个参数是告诉 <code>HBase</code> 是否使用自带的 <code>ZooKeeper</code> 管理 <code>HBase</code> 集群。如果为 <code>true</code>，则使用自带的 <code>ZooKeeper</code>；如果为 <code>false</code>，则使用外部的 <code>ZooKeeper</code>。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191019204433.png" alt="查看 hbase-env.sh 文件" title="查看 hbase-env.sh 文件"></p><p>可以看到我这里的参数设置的是 <code>false</code>，也就是使用外部的 <code>ZooKeeper</code> 集群。</p><p>在这里多说一下这个参数的不同值的使用场景：</p><ul><li>默认值为 <code>true</code>，但是，自带的 <code>ZooKeeper</code> 只能为单机或伪分布模式下的 <code>HBase</code> 提供服务，一般用于学习场景或者测试环境，比较方便管理 </li><li> 如果设置为 <code>false</code>，则使用外部的 <code>ZooKeeper</code> 管理 <code>HBase</code>，此时 <code>HBase</code> 既可以是单机模式、伪分布式模式，也可以是分布式模式，重点只有一个，需要自己搭建一套 <code>ZooKeeper</code> 集群 </li><li> 如果设置为 <code>true</code>，并且 <code>HBase</code> 使用伪分布式模式，则在启动 <code>HBase</code> 时，<code>HBase</code> 将 <code>Zookeeper</code> 作为自身的一部分运行，进程变为 <code>HQuorumPeer</code></li><li>一般建议使用 <code>false</code>，然后自己再单独搭建一套 <code>ZooKeeper</code>，这才是真生的分布式环境；当然，如果觉得复杂，只是自己学习、测试的时候使用，可以设置为 <code>true</code></li></ul><p>言归正传，既然使用的是外部的 <code>ZooKeeper</code>，也就是我这里指定的 <code>dev4:2181</code>，可见 <code>HBase</code> 集群已经设置了自己在 <code>Zookeeper</code> 中的元信息管理目录，而 <code>phoenix</code> 为什么要去另外一个目录 <code>/hbase</code> 获取呢。这里可能是 <code>phoenix</code> 的配置有问题。</p><p>不妨先去里面看一下是否存在 <code>/hbase</code> 节点即可，经过查看，没有这个节点。如果没有的话，也不妨先重新创建一个，使用：<code>create /hbase&quot;&quot;</code> 创建一个空内容节点，确保节点存在。</p><p>注意，这里只是创建了一个空节点，里面并没有任何信息，所以 <code>phoenix</code> 从里面是无法获取关于 <code>HBase</code> 集群的信息的。</p><p>测试了一下，果然，还是无法导入数据，抛出超时异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">19/10/19 20:47:12 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-phoenix.properties,hadoop-metrics2.properties</span><br><span class="line">org.apache.phoenix.exception.PhoenixIOException: callTimeout=600000, callDuration=1024368: </span><br><span class="line">at org.apache.phoenix.util.ServerUtil.parseServerException (ServerUtil.java:108)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated (ConnectionQueryServicesImpl.java:840)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable (ConnectionQueryServicesImpl.java:1134)</span><br><span class="line">at org.apache.phoenix.query.DelegateConnectionQueryServices.createTable (DelegateConnectionQueryServices.java:110)</span><br><span class="line">at org.apache.phoenix.schema.MetaDataClient.createTableInternal (MetaDataClient.java:1591)</span><br><span class="line">at org.apache.phoenix.schema.MetaDataClient.createTable (MetaDataClient.java:569)</span><br><span class="line">at org.apache.phoenix.compile.CreateTableCompiler$2.execute (CreateTableCompiler.java:175)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixStatement$2.call (PhoenixStatement.java:271)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixStatement$2.call (PhoenixStatement.java:263)</span><br><span class="line">at org.apache.phoenix.call.CallRunner.run (CallRunner.java:53)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation (PhoenixStatement.java:261)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixStatement.executeUpdate (PhoenixStatement.java:1043)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl$9.call (ConnectionQueryServicesImpl.java:1561)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl$9.call (ConnectionQueryServicesImpl.java:1530)</span><br><span class="line">at org.apache.phoenix.util.PhoenixContextExecutor.call (PhoenixContextExecutor.java:77)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl.init (ConnectionQueryServicesImpl.java:1530)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices (PhoenixDriver.java:162)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.connect (PhoenixEmbeddedDriver.java:126)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixDriver.connect (PhoenixDriver.java:133)</span><br><span class="line">at java.sql.DriverManager.getConnection (DriverManager.java:664)</span><br><span class="line">at java.sql.DriverManager.getConnection (DriverManager.java:208)</span><br><span class="line">at org.apache.phoenix.util.PhoenixRuntime.main (PhoenixRuntime.java:182)</span><br><span class="line">Caused by: java.net.SocketTimeoutException: callTimeout=600000, callDuration=1024368: </span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries (RpcRetryingCaller.java:156)</span><br><span class="line">at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable (HBaseAdmin.java:3390)</span><br><span class="line">at org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor (HBaseAdmin.java:408)</span><br><span class="line">at org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor (HBaseAdmin.java:429)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated (ConnectionQueryServicesImpl.java:772)</span><br><span class="line">... 20 more</span><br><span class="line">Caused by: org.apache.hadoop.hbase.MasterNotRunningException: java.io.IOException: Can&apos;t get master address from ZooKeeper; znode data == null</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$StubMaker.makeStub (ConnectionManager.java:1671)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$MasterServiceStubMaker.makeStub (ConnectionManager.java:1697)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getKeepAliveMasterService (ConnectionManager.java:1914)</span><br><span class="line">at org.apache.hadoop.hbase.client.HBaseAdmin$MasterCallable.prepare (HBaseAdmin.java:3363)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries (RpcRetryingCaller.java:125)</span><br><span class="line">... 24 more</span><br><span class="line">Caused by: java.io.IOException: Can&apos;t get master address from ZooKeeper; znode data == null</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.getMasterAddress (MasterAddressTracker.java:114)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$StubMaker.makeStubNoRetries (ConnectionManager.java:1597)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$StubMaker.makeStub (ConnectionManager.java:1643)</span><br><span class="line">... 28 more</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191019211617.png" alt="导入数据再次出现异常" title="导入数据再次出现异常"></p><p>可以看到，里面有 <code>Can&#39;t get master address from ZooKeeper</code> 字样，也就是无法从 <code>Zookeeper</code> 指定的目录中获取关于 <code>HBase</code> 的主节点信息，可见，单纯在 <code>Zookeeper</code> 中创建一个 <code>/hbase</code> 目录是没用的。因此，源头应该在于 <code>phoenix</code> 为什么不去 <code>/hbase-unsecure</code> 目录中获取 <code>HBase</code> 集群信息【这才是 <code>HBase</code> 集群的信息所在地】，是哪里的配置出了问题。</p><p>经过排查，<code>phoenix</code> 脚本在加载 <code>hbase_conf_dir</code> 参数的时候，目录错误，因此没有获取到 <code>HBase</code> 相的配置文件，最终导致没有去 <code>Zookeeper</code> 的 <code>/hbase-unsecure</code> 目录读取数据。这里排查的是 <code>psql.py</code>、<code>phoenix_utils.py</code> 这两个文件，里面有关于加载 <code>HBase</code>、<code>Hadoop</code> 集群的配置目录的参数，如果赋值错误就会导致上述现象。</p><p>把 <code>hbase_conf_dir</code> 参数的加载过程梳理清楚，确保可以加载到 <code>HBASE_HOME/conf</code> 目录，接着就可以顺利导入数据了。</p><p>同时当然也需要 <code>HADOOP_HOME/conf</code>，但是我这里已经是正确的了，如果读者没有配置好，可能会遇到找不到 <code>hdfs</code> 的相关类，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.hdfs.DistributedFileSystem not found</span><br></pre></td></tr></table></figure><p>最后一点需要注意，上传的 <code>csv</code> 文件内容列数要确保和 <code>HBase</code> 表的列数一致，并且不需要表头，否则无法成功导入【表头也会被当做内容】，日志也会报错提醒的。当然，字段也是有顺序的，<code>csv</code> 文件中字段的顺序要和 <code>HBase</code> 表中定义的一致。</p><p>顺利导入数据，导入成功，耗时 12 秒，导入 12000 条数据，从输出日志中可以看到详情。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191021215944.png" alt="数据导入成功" title="数据导入成功"></p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注</h1><p>1、参考：<a href="https://stackoverflow.com/questions/28605301/the-node-hbase-is-not-in-zookeeper" target="_blank" rel="noopener">HBase</a> ，这是个相似的问题。</p><p>2、如果数据量比较大的话，就不建议使用这种脚本导入的方式，反而可以使用 <code>xxx-client.jar</code> 包里面自带的处理类来执行，并提前把数据文件上传至 <code>hdfs</code>，然后后台会提交 <code>MapReduce</code> 任务来大批量导入数据。</p><p>3、数据导入、数据导出还可以使用 <code>pig</code> 这个工具。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;使用 &lt;code&gt;phoenix&lt;/code&gt; 向 &lt;code&gt;HBase&lt;/code&gt; 中导入数据，使用的是 &lt;code&gt;phoenix&lt;/code&gt; 自带的脚本 &lt;code&gt;psql.py&lt;/code&gt;，结果报错：&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;19/10/18 11:47:29 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &amp;apos;zookeeper.znode.parent&amp;apos;. There could be a mismatch with the one configured in the master.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;看起来是 &lt;code&gt;ZooKeeper&lt;/code&gt; 环境有问题，本文记录解决过程。&lt;/p&gt;&lt;p&gt;本文开发环境基于 &lt;code&gt;HBase v1.1.2&lt;/code&gt;、&lt;code&gt;phoenix v4.2.0&lt;/code&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="踩坑系列" scheme="https://www.playpi.org/categories/series-of-fixbug/"/>
    
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="Phoenix" scheme="https://www.playpi.org/tags/Phoenix/"/>
    
      <category term="Zookeeper" scheme="https://www.playpi.org/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>HBase 错误：NotServingRegionException</title>
    <link href="https://www.playpi.org/2019101201.html"/>
    <id>https://www.playpi.org/2019101201.html</id>
    <published>2019-10-12T12:19:31.000Z</published>
    <updated>2019-10-12T12:19:31.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>在使用 <code>SparkStreaming</code> 程序处理数据，结果写入 <code>HBase</code> 时，遇到异常 <code>NotServingRegionException</code>，只是突然出现一次，平时正常，怀疑是和开发环境有关，本文记录查找问题的过程。本文中涉及的开发环境为 <code>HBase v1.1.2</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p><code>SparkStreaming</code> 程序处理数据，结果写入 <code>HBase</code>，出现异常，并且一直持续：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">2019-10-13_16:40:31 [JobGenerator] INFO consumer.SimpleConsumer:68: Reconnect due to socket error: java.nio.channels.ClosedChannelException</span><br><span class="line">2019-10-13_16:40:32 [JobGenerator] INFO scheduler.JobScheduler:58: Added jobs for time 1570869630000 ms</span><br><span class="line">2019-10-13_16:40:33 [Executor task launch worker-0] INFO client.AsyncProcess:1656: #3, waiting for some tasks to finish. Expected max=0, tasksInProgress=29</span><br><span class="line">2019-10-13_16:40:34 [htable-pool3-t1] INFO client.AsyncProcess:1174: #3, table=YOUR_TABLE, attempt=29/35 failed=64ops, last exception: org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region YOUR_TABLE,f,1565318245911.a70001dfe6d9320600286510318bfeb6. is not online on dev6,16020,1570795214262</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName (HRegionServer.java:2898)</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion (RSRpcServices.java:947)</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.RSRpcServices.multi (RSRpcServices.java:1994)</span><br><span class="line">at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod (ClientProtos.java:32213)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcServer.call (RpcServer.java:2114)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.CallRunner.run (CallRunner.java:101)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop (RpcExecutor.java:130)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run (RpcExecutor.java:107)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line"> on dev6,16020,1569724523487, tracking started null, retrying after=20058ms, replay=64ops</span><br><span class="line">2019-10-13_16:40:46 [scheduled-rate-update] INFO streaming.ScheduledRateController:136: MinRateCondition, rateLimit = -1, minRate = 400</span><br><span class="line">2019-10-13_16:40:46 [stream-rate-update] INFO streaming.ScheduledRateController:155: MinRateCondition&apos;s execute, numOfBatches = 38 vs 80</span><br><span class="line">2019-10-13_16:40:54 [Executor task launch worker-0] INFO client.AsyncProcess:1656: #3, waiting for some tasks to finish. Expected max=0, tasksInProgress=30</span><br><span class="line">2019-10-13_16:40:54 [htable-pool3-t1] INFO client.AsyncProcess:1174: #3, table=YOUR_TABLE, attempt=30/35 failed=64ops, last exception: org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region YOUR_TABLE,f,1565318245911.a70001dfe6d9320600286510318bfeb6. is not online on dev6,16020,1570795214262</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName (HRegionServer.java:2898)</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion (RSRpcServices.java:947)</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.RSRpcServices.multi (RSRpcServices.java:1994)</span><br><span class="line">at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod (ClientProtos.java:32213)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcServer.call (RpcServer.java:2114)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.CallRunner.run (CallRunner.java:101)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop (RpcExecutor.java:130)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run (RpcExecutor.java:107)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line"> on dev6,16020,1569724523487, tracking started null, retrying after=20050ms, replay=64ops</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204453.png" alt="HBase 错误日志" title="HBase 错误日志"></p><p> 留意重点信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NotServingRegionException</span><br><span class="line">is not online on dev6,16020,1570795214262</span><br></pre></td></tr></table></figure><p>通过初步排查，发现只有一个数据表有此问题，更换其它表数据就可以正常写入，看来是和环境有关。</p><p>通过 <code>phoenix</code> 进行查询，发现也无法查询出数据，报错超时：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: org.apache.phoenix.exception.PhoenixIOException: org.apache.phoenix.exception.PhoenixIOException: Failed after attempts=36, exceptions:</span><br><span class="line">Sat Oct 12 16:30:48 CST 2019, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=70197: row &apos;0&apos; on table &apos;YOUR_TABLE&apos; at region=YOUR_TABLE,0,1565318245911.157723c2d47bbae2226f6286a56f0256., hostname=dev6,15020,1569724523487, seqNum=1627</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204535.png" alt="phoenix 查询超时" title="phoenix 查询超时"></p><p>但是从这个超时异常中看不到有效的线索。</p><p>接着通过 <code>RegionServer</code> 查看 <code>Region</code> 的分布，尝试搜索日志中出现的 <code>Region YOUR_TABLE,f,1565318245911.a70001dfe6d9320600286510318bfeb6</code>，发现不存在，看来这个表的 <code>Region</code> 信息有异常。</p><p>通过搜索问题关键词，在 <code>stackoverflow</code> 上面找到一个例子，出现这种现象是因为这个表的 <code>Region</code> 损坏了，导致无法找到指定的 <code>Region</code>，但是可以手动修复。</p><h1 id="问题解决"><a href="# 问题解决" class="headerlink" title="问题解决"></a>问题解决 </h1><p> 找问题原因，并且进一步得到了建议的解决方案，准备实施。</p><p>首先使用 <code>hbase hbck&quot;YOUR_TABLE&quot;</code> 检测数据表的状态，等待几十秒，会陆续打印出集群的状态以及表的状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">2019-10-13 17:44:20,160 INFO  [main-SendThread (dev5:2181)] zookeeper.ClientCnxn: Opening socket connection to server dev5/172.18.5.205:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">2019-10-13 17:44:20,247 INFO  [main-SendThread (dev5:2181)] zookeeper.ClientCnxn: Socket connection established to dev5/172.18.5.205:2181, initiating session</span><br><span class="line">2019-10-13 17:44:20,359 INFO  [main-SendThread (dev5:2181)] zookeeper.ClientCnxn: Session establishment complete on server dev5/172.18.5.205:2181, sessionid = 0x26d539786987a13, negotiated timeout = 40000</span><br><span class="line">Version: 1.1.2.2.4.2.0-258</span><br><span class="line">Number of live region servers: 3</span><br><span class="line">Number of dead region servers: 0</span><br><span class="line">Master: dev6,16000,1563773138374</span><br><span class="line">Number of backup masters: 1</span><br><span class="line">Average load: 382.6666666666667</span><br><span class="line">Number of requests: 0</span><br><span class="line">Number of regions: 1148</span><br><span class="line">Number of regions in transition: 30</span><br><span class="line">2019-10-13 17:44:24,693 INFO  [main] util.HBaseFsck: Loading regionsinfo from the hbase:meta table</span><br><span class="line"></span><br><span class="line">Number of empty REGIONINFO_QUALIFIER rows in hbase:meta: 0</span><br><span class="line">2019-10-13 17:44:24,954 INFO  [main] util.HBaseFsck: getHTableDescriptors == tableNames =&gt; [YOUR_TABLE]</span><br><span class="line">...</span><br><span class="line">2019-10-13 17:44:26,621 INFO  [main] util.HBaseFsck: Checking and fixing region consistency</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,6,1565318245911.60686e402d3b0e25edc210190f8290c6., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/60686e402d3b0e25edc210190f8290c6, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,9,1565318245911.3dab1e5fc8211112c46041544c8cf6a1., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/3dab1e5fc8211112c46041544c8cf6a1, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,0,1565318245911.157723c2d47bbae2226f6286a56f0256., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/157723c2d47bbae2226f6286a56f0256, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,3,1565318245911.8e6507c0aa0ba2f7864fb6adbab58cd4., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/8e6507c0aa0ba2f7864fb6adbab58cd4, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,f,1565318245911.a70001dfe6d9320600286510318bfeb6., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/a70001dfe6d9320600286510318bfeb6, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,c,1565318245911.e247e3f852573308fd554e07452fbe93., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/e247e3f852573308fd554e07452fbe93, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">2019-10-13 17:44:26,732 INFO  [main] util.HBaseFsck: Handling overlap merges in parallel. set hbasefsck.overlap.merge.parallel to false to run serially.</span><br><span class="line">ERROR: There is a hole in the region chain between 0 and 1.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.</span><br><span class="line">ERROR: There is a hole in the region chain between 3 and 4.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.</span><br><span class="line">ERROR: There is a hole in the region chain between 6 and 7.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.</span><br><span class="line">ERROR: There is a hole in the region chain between 9 and a.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.</span><br><span class="line">ERROR: There is a hole in the region chain between c and d.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.</span><br><span class="line">ERROR: Last region should end with an empty key. You need to create a new region and regioninfo in HDFS to plug the hole.</span><br><span class="line">ERROR: Found inconsistency in table YOUR_TABLE</span><br><span class="line">2019-10-13 17:44:26,747 INFO  [main] util.HBaseFsck: Computing mapping of all store files</span><br><span class="line">...</span><br><span class="line">2019-10-13 17:44:30,038 INFO  [main] util.HBaseFsck: Finishing hbck</span><br><span class="line">Summary:</span><br><span class="line">Table YOUR_TABLE is inconsistent.</span><br><span class="line">    Number of regions: 11</span><br><span class="line">    Deployed on:  dev4,16020,1570795191487 dev5,16020,1570795198827</span><br><span class="line">Table hbase:meta is okay.</span><br><span class="line">    Number of regions: 1</span><br><span class="line">    Deployed on:  dev5,16020,1570795198827</span><br><span class="line">12 inconsistencies detected.</span><br><span class="line">Status: INCONSISTENT</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204717.png" alt="hbase hbck 查看输出日志 - 1" title="hbase hbck 查看输出日志 - 1"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204734.png" alt="hbase hbck 查看输出日志 - 2" title="hbase hbck 查看输出日志 - 2"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204747.png" alt="hbase hbck 查看输出日志 - 3" title="hbase hbck 查看输出日志 - 3"></p><p>首先注意到前面那个不存在的 <code>Region</code> <code>a70001dfe6d9320600286510318bfeb6</code> 处于未部署状态，<code>RegionServer</code> 当然无法找到了。</p><p>可以看到最终的结论：<code>INCONSISTENT</code>，就是数据不一致。并且在输出日志里面还有说明出现了 <code>Region</code> 空洞【<code>Region hole</code>】。</p><p>那怎么解决呢，可以先尝试使用 <code>hbase hbck -fix&quot;YOUR_TABLE&quot;</code> 解决。</p><p>这里如果遇到操作 <code>HDFS</code> 无权限，记得切换用户 <code>export HADOOP_USER_NAME=hbase</code>，当然最好还是直接使用管理员权限操作：<br><code>sudo -u hbase hbase hbck -fix&quot;YOUR_TABLE&quot;</code>。</p><p>在修复过程中，仍旧会不断输出日志，如果看到：<br><code>util.HBaseFsck: Sleeping 10000ms before re-checking after fix...</code><br>则说明修复完成，为了验证修复结果，<code>HBase</code> 还会自动检测一次。</p><p>再次检测后，如果看到如下信息，说明修复成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Summary:</span><br><span class="line">Table YOUR_TABLE is okay.</span><br><span class="line">    Number of regions: 17</span><br><span class="line">2019-10-13 18:20:02,145 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down</span><br><span class="line">    Deployed on:  dev4,16020,1570795191487 dev5,16020,1570795198827</span><br><span class="line">Table hbase:meta is okay.</span><br><span class="line">    Number of regions: 1</span><br><span class="line">    Deployed on:  dev5,16020,1570795198827</span><br><span class="line">0 inconsistencies detected.</span><br><span class="line">Status: OK</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204942.png" alt="hbase hbck fix 修复完成" title="hbase hbck fix 修复完成"></p><p>接着就可以继续正常写入数据了。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 参考 <code>stackoverflow</code> 上面的例子：<a href="https://stackoverflow.com/questions/37507878/hbase-fails-with-org-apache-hadoop-hbase-notservingregionexception-region-is-not" target="_blank" rel="noopener">notservingregionexception</a> 。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在使用 &lt;code&gt;SparkStreaming&lt;/code&gt; 程序处理数据，结果写入 &lt;code&gt;HBase&lt;/code&gt; 时，遇到异常 &lt;code&gt;NotServingRegionException&lt;/code&gt;，只是突然出现一次，平时正常，怀疑是和开发环境有关，本文记录查找问题的过程。本文中涉及的开发环境为 &lt;code&gt;HBase v1.1.2&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="SparkStreaming" scheme="https://www.playpi.org/tags/SparkStreaming/"/>
    
  </entry>
  
  <entry>
    <title>Git 异常之 Unlink of file</title>
    <link href="https://www.playpi.org/2019100801.html"/>
    <id>https://www.playpi.org/2019100801.html</id>
    <published>2019-10-08T12:48:39.000Z</published>
    <updated>2019-10-08T12:48:39.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>在使用 <code>Git</code> 的时候，出现错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unlink of file &apos;.git/objects/pack/pack-xx.idx&apos; failed. Should I try again? (y/n)</span><br></pre></td></tr></table></figure><p>连续出现几十次，看起来像是 <code>Git</code> 在操作索引文件时被拒绝了，可能是文件权限问题，或者文件被占用。</p><p>本文内容中涉及的 <code>Git</code> 版本为：<code>2.18.0.windows.1</code>，操作系统为：<code>Windows 7x64</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 在对一个普通的 <code>Git</code> 项目进行 <code>git pull</code> 操作的时候，出现错误，显示如下的交互询问内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) </span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-670222495fa872c140e7e231e36cb2701d76c86b.idx&apos; failed. Should I try again? (y/n) </span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6acdf7d3bbb7394f39b68e0e40b47ca0116fbfa2.idx&apos; failed. Should I try again? (y/n) </span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6ffde68d8af2eafb0803063b895291418ed5f465.idx&apos; failed. Should I try again? (y/n)</span><br></pre></td></tr></table></figure><p>尝试手动输入 <code>y</code> 或者 <code>n</code>，并没有什么效果，输入 <code>y</code> 后同样的错误会继续出现，输入 <code>n</code> 会接着提示下一个类似的文件错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; iled. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) n</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-670222495fa872c140e7e231e36cb2701d76c86b.idx&apos; failed. Should I try again? (y/n) n</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6acdf7d3bbb7394f39b68e0e40b47ca0116fbfa2.idx&apos; failed. Should I try again? (y/n) n</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6ffde68d8af2eafb0803063b895291418ed5f465.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6ffde68d8af2eafb0803063b895291418ed5f465.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6ffde68d8af2eafb0803063b895291418ed5f465.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6ffde68d8af2eafb0803063b895291418ed5f465.idx&apos; failed. Should I try again? (y/n) y</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191008210804.png" alt="Git 文件被占用" title="Git 文件被占用"></p><p>可见是要把所有同类型的文件全部询问一次，看起来问题没那么简单。</p><p>如果有耐心的话，连续输入几十次 <code>n</code>，可能会把所有的文件都忽略掉，提示也就结束了，或者直接使用 <code>ctrl + c</code> 结束操作，强制退出，但是这样操作并没有从根本上解决这个问题。</p><h1 id="分析解决"><a href="# 分析解决" class="headerlink" title="分析解决"></a>分析解决 </h1><p> 经过查询分析，这个问题的根本原因是 <code>Git</code> 项目的文件被其它程序占用，导致 <code>Git</code> 没有权限变更这些文件。这些文件是 <code>Git</code> 产生的临时文件，需要从 <code>Git</code> 的工作区移除。</p><p>上面提及的其它程序极有可能是 <code>IDEA</code>、<code>Eclipse</code>、<code>Visual Studio</code> 等常用的开发工具。</p><p>参考：<a href="https://stackoverflow.com/questions/4389833/unlink-of-file-failed-should-i-try-again" target="_blank" rel="noopener">stackoverflow.com</a> 。</p><p>解决方案也很简单，把占用文件的程序关闭就行。但是有时候找不到是哪个程序占用了文件，怎么办，可以利用微软的 <code>Process Explorer</code> 工具，具体介绍参考备注内容。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p>1、<code>Process Explorer</code> 是一个任务管理器，目前由微软开发，仅用于 <code>Windows</code> 操作系统平台，可以查看系统的进程信息、资源占用信息、文件占用信息，官网地址：<a href="https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer" target="_blank" rel="noopener">Process Explorer</a> 。</p><p> 同时，这个工具目前在 <code>GitHub</code> 上已经开源，并重新命名为：<code>sysinternals</code>，<code>GitHub</code> 的地址：<a href="https://github.com/MicrosoftDocs/sysinternals/tree/live" target="_blank" rel="noopener">sysinternals</a> 。</p><p>使用时无需安装，解压后直接可以运行，在主界面依次选择 <code>Find</code> -&gt; <code>Find Handle or DLL</code>，在搜索框中输入程序的名字、文件的名字，点击搜索，就可以看到搜索结果了，例如正在运行的进程、文件的使用情况等。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191008210836.png" alt="使用 Process Explorer 查看文件占用情况" title="使用 Process Explorer 查看文件占用情况"></p><p>2、我留意到在上述的 <code>stackoverflow</code> 链接中，也有人建议先使用 <code>git gc</code> 来手动执行一下垃圾清理，把临时文件给清理掉，然才进行 <code>git pull</code> 操作。我没有测试过，但感觉也有道理，读者可以试试。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在使用 &lt;code&gt;Git&lt;/code&gt; 的时候，出现错误：&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Unlink of file &amp;apos;.git/objects/pack/pack-xx.idx&amp;apos; failed. Should I try again? (y/n)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;连续出现几十次，看起来像是 &lt;code&gt;Git&lt;/code&gt; 在操作索引文件时被拒绝了，可能是文件权限问题，或者文件被占用。&lt;/p&gt;&lt;p&gt;本文内容中涉及的 &lt;code&gt;Git&lt;/code&gt; 版本为：&lt;code&gt;2.18.0.windows.1&lt;/code&gt;，操作系统为：&lt;code&gt;Windows 7x64&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="基础技术知识" scheme="https://www.playpi.org/categories/basic-technical-knowledge/"/>
    
    
      <category term="Git" scheme="https://www.playpi.org/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>使用海龟绘图绘制一面五星红旗</title>
    <link href="https://www.playpi.org/2019100101.html"/>
    <id>https://www.playpi.org/2019100101.html</id>
    <published>2019-10-01T14:57:58.000Z</published>
    <updated>2019-10-23T14:57:58.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>今天是国庆节，中国正在举行建国七十周年大阅兵，很多人都在观看，我在家里也看了直播片段。在刷微博的过程中，无意中看到有人在介绍 <strong>海龟绘图 </strong>这个 <code>Python</code> 库，可以非常方便地绘制各种图形，其中有人提到可以绘制出一面五星红旗。</p><p>后来我查了一下，的确是可以，难度不大，只要理解基本的绘制流程即可，于是我尝试了一下，并成功绘制出一面五星红旗。本文记录过程，开发环境基于 <code>Python v3.8</code>、<code>Windows 10 x64</code>。</p><a id="more"></a><h1 id="绘制思路"><a href="# 绘制思路" class="headerlink" title="绘制思路"></a>绘制思路 </h1><p> 绘制思路很简单，不过在这里需要先理解坐标轴、画笔的颜色、背景色、角度等基础概念。</p><p>1、先设置弹框大小，也就是五星红旗的长、宽，单位是像素。</p><p>2、设置背景颜色为红色，设置五角星的线条、填充颜色都为黄色。</p><p>3、绘制中心的 1 个大五角星。</p><p>4、绘制边上的 4 个小五角星。</p><h1 id="绘制代码"><a href="# 绘制代码" class="headerlink" title="绘制代码"></a>绘制代码 </h1><p> 需要注意除了 <code>Python</code> 环境，还需要安装海龟绘图库，我使用的 <code>Python v3.8</code> 已经自带了这个库，如果读者有使用这个版本的 <code>Python</code> 则不需要再单独安装。如果是其它版本的 <code>Python</code>，可能缺失这个库，可以使用 <code>pip</code> 工具安装，参考安装命令：<code>pip install turtle</code>。</p><p>当然，可能还会有其它依赖缺失问题，不属于本文讨论的范围，请读者自行解决。</p><p>提醒读者，这里涉及到的代码已经被我上传至 <code>Github</code>，命名为：<code>__main__.py</code>，读者可以提前下载查看：<a href="https://github.com/iplaypi/iplaypipython/blob/master/iplaypipython/20191001/__main__.py" target="_blank" rel="noopener">main.py</a> 。</p><p>下面给出代码清单，包含注释，读者很容易看懂：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"># 从海龟绘图模块中导入全部函数 </span><br><span class="line"># 在 Python v3.8 中已经内置此模块，如果其它 Python 版本没有内置，需要使用 pip 安装 </span><br><span class="line">from turtle import *</span><br><span class="line"></span><br><span class="line"># 开始绘制五星红旗 </span><br><span class="line">def daw_flag ():</span><br><span class="line">    # 设置大小，4 个参数：宽度、高度、起始值 x 轴、起始值 y 轴 </span><br><span class="line">    setup (600, 400, 0, 0)</span><br><span class="line">    # 设置背景为红色 </span><br><span class="line">    bgcolor (&apos;red&apos;)</span><br><span class="line">    # 线条、填充颜色设置为黄色 </span><br><span class="line">    fillcolor (&apos;yellow&apos;)</span><br><span class="line">    color (&apos;yellow&apos;)</span><br><span class="line">    # 画笔运行速度 </span><br><span class="line">    speed (10)</span><br><span class="line"></span><br><span class="line">    # 大五角星绘制 </span><br><span class="line">    draw_star (-280, 100, 0, 150, 144, 0)</span><br><span class="line"></span><br><span class="line">    # 4 个小五角星绘制 </span><br><span class="line">    draw_star (-100, 180, 305, 50, 0, 144)</span><br><span class="line">    draw_star (-50, 110, 30, 50, 144, 0)</span><br><span class="line">    draw_star (-40, 50, 5, 50, 144, 0)</span><br><span class="line">    draw_star (-100, 10, 300, 50, 0, 144)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"># 绘制五角星的方法，根据实际情况传入参数 </span><br><span class="line">def draw_star (gotox_val, gotoy_val, heading_val=0, fd_val=50, rt_val=0, lt_val=0):</span><br><span class="line">    # 开始填充 </span><br><span class="line">    begin_fill ()</span><br><span class="line">    # 提起画笔，此时可以任意移动画笔位置 </span><br><span class="line">    up ()</span><br><span class="line">    # 移动至指定坐标 </span><br><span class="line">    goto (gotox_val, gotoy_val)</span><br><span class="line">    # 设置朝向角度 </span><br><span class="line">    if (0 != heading):</span><br><span class="line">        setheading (heading_val)</span><br><span class="line">    # 放下画笔，此时再移动就开始绘制 </span><br><span class="line">    down ()</span><br><span class="line">    # for 循环，绘制 5 条边 </span><br><span class="line">    for i in range (5):</span><br><span class="line">        # forward，向前移动画笔指定单位，像素 </span><br><span class="line">        fd (fd_val)</span><br><span class="line">        if (0 != rt_val):</span><br><span class="line">            # right，向右旋转指定单位，度数 </span><br><span class="line">            rt (rt_val)</span><br><span class="line">        if (0 != lt_val):</span><br><span class="line">            # left，向左旋转指定单位，度数 </span><br><span class="line">            lt (lt_val)</span><br><span class="line">    # 结束填充 </span><br><span class="line">    end_fill ()</span><br><span class="line"></span><br><span class="line"># 程序入口 </span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    print (&apos; 开始绘制五星红旗 & apos;)</span><br><span class="line">    daw_flag ()</span><br><span class="line">    print (&apos; 结束绘制五星红旗 & apos;)</span><br><span class="line">    exitonclick ()</span><br><span class="line">    # input (&apos; 暂停，等待输入（输入任意内容按回车键可退出）：&apos;)</span><br></pre></td></tr></table></figure><p>可以看到，代码中有 3 个函数：主函数 <code>__main__</code>、绘制五星红旗函数 <code>draw_flag</code>、绘制五角星函数 <code>draw_star</code>，这 3 个函数存在调用关系，共同绘制出一面五星红旗。</p><p>我这里故意把绘制速度设置小一点，读者在运行过程中可以清楚地看到绘制的过程，点的移动、线的绘制可以看得很清楚。</p><p>运行结果。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191031005634.png" alt="运行结果" title="运行结果"></p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注</h1><p>1、<code>Python</code> 官方网站参考：<a href="https://www.python.org" target="_blank" rel="noopener">Python</a> 。</p><p>2、在 <code>Windows</code> 平台安装 <code>Python</code> 需要注意版本的选择，是 32 位还是 64 位要搞清楚，不然后续会引发一系列麻烦，哪怕卸载重装也会有麻烦。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;今天是国庆节，中国正在举行建国七十周年大阅兵，很多人都在观看，我在家里也看了直播片段。在刷微博的过程中，无意中看到有人在介绍 &lt;strong&gt;海龟绘图 &lt;/strong&gt;这个 &lt;code&gt;Python&lt;/code&gt; 库，可以非常方便地绘制各种图形，其中有人提到可以绘制出一面五星红旗。&lt;/p&gt;&lt;p&gt;后来我查了一下，的确是可以，难度不大，只要理解基本的绘制流程即可，于是我尝试了一下，并成功绘制出一面五星红旗。本文记录过程，开发环境基于 &lt;code&gt;Python v3.8&lt;/code&gt;、&lt;code&gt;Windows 10 x64&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="知识改变生活" scheme="https://www.playpi.org/categories/knowledge-for-life/"/>
    
    
      <category term="Python" scheme="https://www.playpi.org/tags/Python/"/>
    
      <category term="Turtle" scheme="https://www.playpi.org/tags/Turtle/"/>
    
      <category term="national" scheme="https://www.playpi.org/tags/national/"/>
    
  </entry>
  
  <entry>
    <title>HBase 错误之 NoClassDefFoundError：ProtobufUtil</title>
    <link href="https://www.playpi.org/2019093001.html"/>
    <id>https://www.playpi.org/2019093001.html</id>
    <published>2019-09-30T12:34:04.000Z</published>
    <updated>2019-10-01T12:34:04.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --><p>背景说明：通过 <code>dubbo</code> 部署一个服务，服务中的业务逻辑会查询 <code>HBase</code> 表的数据，但是 <code>dubbo</code> 服务在初始化注册时，<code>HBase</code> 初始化的过程中会报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil</span><br></pre></td></tr></table></figure><p>本文涉及的开发环境，基于 <code>HBase v1.1.2</code>、<code>Zookeeper v3.4.6</code>、<code>dubbo v2.8.4</code>、<code>Hadoop v2.7.1</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 通过 <code>k8s</code> 多节点发布服务，但是只有在某一台机器上面出现错误【其它节点日志显示正常，也可以提供正常的服务】，发布后 <code>dubbo</code> 服务注册初始化时出现的错误如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">2019-09-19_18:03:49 [http-nio-28956-exec-2-SendThread (192.168.20.101:2181)] INFO zookeeper.ClientCnxn:852: Socket connection established to 192.168.20.101/192.168.20.101:2181, initiating session</span><br><span class="line">2019-09-19_18:03:49 [http-nio-28956-exec-2-SendThread (192.168.20.101:2181)] INFO zookeeper.ClientCnxn:1235: Session establishment complete on server 192.168.20.101/192.168.20.101:2181, sessionid = 0x36af032f505e830, negotiated timeout = 90000</span><br><span class="line">2019-09-19_18:03:50 [http-nio-28956-exec-2] WARN hdfs.DFSUtil:689: Namenode for hdfs-cluster remains unresolved for ID nn1.  Check your hdfs-site.xml file to ensure namenodes are configured properly.</span><br><span class="line">2019-09-19_18:03:50 [http-nio-28956-exec-9] ERROR filter.ExceptionFilter:87:  [DUBBO] Got unchecked and undeclared exception which called by 10.200.0.2. service: com.yyy.zzz.service.es.weibo.IXxxService, method: search, exception: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil, dubbo version: 2.8.4, current host: 127.0.0.1</span><br><span class="line">java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState (MetaTableLocator.java:482)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation (MetaTableLocator.java:167)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:598)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:579)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:558)</span><br><span class="line">at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation (ZooKeeperRegistry.java:61)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta (ConnectionManager.java:1192)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1159)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.relocateRegion (ConnectionManager.java:1133)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta (ConnectionManager.java:1338)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1162)</span><br><span class="line">at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.findAllLocationsOrFail (AsyncProcess.java:940)</span><br><span class="line">at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction (AsyncProcess.java:857)</span><br><span class="line">at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$100 (AsyncProcess.java:575)</span><br><span class="line">at org.apache.hadoop.hbase.client.AsyncProcess.submitAll (AsyncProcess.java:557)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.batch (HTable.java:933)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.batch (HTable.java:950)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:911)</span><br><span class="line">at com.yyy.zzz.commons.search.reader.hbase.BaseHBaseReader.batchGet (BaseHBaseReader.java:94)</span><br><span class="line">at com.yyy.zzz.commons.search.reader.hbase.weibo.WeiboContentHbaseReader.batchGet (WeiboContentHbaseReader.java:98)</span><br><span class="line">at com.yyy.zzz.commons.search.searcher.AbstractBaseSearcher.getContent (AbstractBaseSearcher.java:269)</span><br><span class="line">at com.yyy.zzz.commons.search.searcher.AbstractBaseSearcher.getInfo (AbstractBaseSearcher.java:188)</span><br><span class="line">at com.yyy.zzz.runner.search.BaseSearchRunner.search (BaseSearchRunner.java:89)</span><br><span class="line">at com.yyy.zzz.api.weibo.WeiboContentServiceImpl.search (WeiboContentServiceImpl.java:33)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.Wrapper3.invokeMethod (Wrapper3.java)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory$1.doInvoke (JavassistProxyFactory.java:46)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.AbstractProxyInvoker.invoke (AbstractProxyInvoker.java:72)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.InvokerWrapper.invoke (InvokerWrapper.java:53)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ExceptionFilter.invoke (ExceptionFilter.java:64)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.monitor.support.MonitorFilter.invoke (MonitorFilter.java:75)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.TimeoutFilter.invoke (TimeoutFilter.java:42)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.dubbo.filter.TraceFilter.invoke (TraceFilter.java:78)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ContextFilter.invoke (ContextFilter.java:70)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.GenericFilter.invoke (GenericFilter.java:132)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ClassLoaderFilter.invoke (ClassLoaderFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.EchoFilter.invoke (EchoFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.InvokerInvocationHandler.invoke (InvokerInvocationHandler.java:52)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.proxy1.search (proxy1.java)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke (Method.java:498)</span><br><span class="line">at org.jboss.resteasy.core.MethodInjectorImpl.invoke (MethodInjectorImpl.java:137)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget (ResourceMethodInvoker.java:288)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:242)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:229)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:356)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:179)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.ServletContainerDispatcher.service (ServletContainerDispatcher.java:220)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:56)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:51)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.rest.DubboHttpServer$RestHandler.handle (DubboHttpServer.java:86)</span><br><span class="line">at com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet.service (DispatcherServlet.java:64)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:291)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter (ApplicationFilterChain.java:206)</span><br><span class="line">at org.apache.catalina.core.StandardWrapperValve.invoke (StandardWrapperValve.java:219)</span><br><span class="line">at org.apache.catalina.core.StandardContextValve.invoke (StandardContextValve.java:106)</span><br><span class="line">at org.apache.catalina.authenticator.AuthenticatorBase.invoke (AuthenticatorBase.java:504)</span><br><span class="line">at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:142)</span><br><span class="line">at org.apache.catalina.valves.ErrorReportValve.invoke (ErrorReportValve.java:79)</span><br><span class="line">at org.apache.catalina.core.StandardEngineValve.invoke (StandardEngineValve.java:88)</span><br><span class="line">at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:534)</span><br><span class="line">at org.apache.coyote.http11.AbstractHttp11Processor.process (AbstractHttp11Processor.java:1081)</span><br><span class="line">at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process (AbstractProtocol.java:658)</span><br><span class="line">at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process (Http11NioProtocol.java:222)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun (NioEndpoint.java:1566)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run (NioEndpoint.java:1523)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run (TaskThread.java:61)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br></pre></td></tr></table></figure><p>注意查看重点的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2019-09-19_18:03:50 [http-nio-28956-exec-2] WARN hdfs.DFSUtil:689: Namenode for hdfs-cluster remains unresolved for ID nn1.  Check your hdfs-site.xml file to ensure namenodes are configured properly.</span><br><span class="line">java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil</span><br></pre></td></tr></table></figure><p>第一行是 <code>hdfs</code> 无法解析 <code>HA</code> 的域名，应该是系统环境问题；第二行是 <code>HBase</code> 初始化环境失败，看起来像是缺失依赖包或者依赖包冲突导致的 <code>NoClassDefFoundError</code>。</p><p>同时还出现了未知主机名异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">com.yyy.zzz.exception.es.EsConnException: java.net.UnknownHostException: host40: Temporary failure in name resolution</span><br><span class="line">at com.yyy.zzz.commons.infrastructure.client.EsClient.&lt;init&gt;(EsClient.java:46)</span><br><span class="line">at com.yyy.zzz.commons.infrastructure.client.EsClient.getInstance (EsClient.java:57)</span><br><span class="line">at com.yyy.zzz.commons.search.searcher.AbstractBaseSearcher.&lt;init&gt;(AbstractBaseSearcher.java:69)</span><br><span class="line">at com.yyy.zzz.commons.search.searcher.weibo.WeiboContentSearcher.&lt;init&gt;(WeiboContentSearcher.java:14)</span><br><span class="line">at com.yyy.zzz.commons.search.searcher.weibo.WeiboContentSearcher.getInstance (WeiboContentSearcher.java:22)</span><br><span class="line">at com.yyy.zzz.runner.search.weibo.WeiboContentSearchRunner.&lt;init&gt;(WeiboContentSearchRunner.java:26)</span><br><span class="line">at com.yyy.zzz.runner.search.weibo.WeiboContentSearchRunner.&lt;init&gt;(WeiboContentSearchRunner.java:20)</span><br><span class="line">at com.yyy.zzz.api.weibo.WeiboContentServiceImpl.search (WeiboContentServiceImpl.java:32)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.Wrapper3.invokeMethod (Wrapper3.java)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory$1.doInvoke (JavassistProxyFactory.java:46)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.AbstractProxyInvoker.invoke (AbstractProxyInvoker.java:72)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.InvokerWrapper.invoke (InvokerWrapper.java:53)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ExceptionFilter.invoke (ExceptionFilter.java:64)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.monitor.support.MonitorFilter.invoke (MonitorFilter.java:75)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.TimeoutFilter.invoke (TimeoutFilter.java:42)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.dubbo.filter.TraceFilter.invoke (TraceFilter.java:78)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ContextFilter.invoke (ContextFilter.java:70)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.GenericFilter.invoke (GenericFilter.java:132)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ClassLoaderFilter.invoke (ClassLoaderFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.EchoFilter.invoke (EchoFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.InvokerInvocationHandler.invoke (InvokerInvocationHandler.java:52)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.proxy1.search (proxy1.java)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke (Method.java:498)</span><br><span class="line">at org.jboss.resteasy.core.MethodInjectorImpl.invoke (MethodInjectorImpl.java:137)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget (ResourceMethodInvoker.java:288)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:242)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:229)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:356)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:179)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.ServletContainerDispatcher.service (ServletContainerDispatcher.java:220)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:56)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:51)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.rest.DubboHttpServer$RestHandler.handle (DubboHttpServer.java:86)</span><br><span class="line">at com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet.service (DispatcherServlet.java:64)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:291)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter (ApplicationFilterChain.java:206)</span><br><span class="line">at org.apache.catalina.core.StandardWrapperValve.invoke (StandardWrapperValve.java:219)</span><br><span class="line">at org.apache.catalina.core.StandardContextValve.invoke (StandardContextValve.java:106)</span><br><span class="line">at org.apache.catalina.authenticator.AuthenticatorBase.invoke (AuthenticatorBase.java:504)</span><br><span class="line">at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:142)</span><br><span class="line">at org.apache.catalina.valves.ErrorReportValve.invoke (ErrorReportValve.java:79)</span><br><span class="line">at org.apache.catalina.core.StandardEngineValve.invoke (StandardEngineValve.java:88)</span><br><span class="line">at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:534)</span><br><span class="line">at org.apache.coyote.http11.AbstractHttp11Processor.process (AbstractHttp11Processor.java:1081)</span><br><span class="line">at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process (AbstractProtocol.java:658)</span><br><span class="line">at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process (Http11NioProtocol.java:222)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun (NioEndpoint.java:1566)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run (NioEndpoint.java:1523)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run (TaskThread.java:61)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">Caused by: java.net.UnknownHostException: host40: Temporary failure in name resolution</span><br><span class="line">at java.net.Inet6AddressImpl.lookupAllHostAddr (Native Method)</span><br><span class="line">at java.net.InetAddress$2.lookupAllHostAddr (InetAddress.java:928)</span><br><span class="line">at java.net.InetAddress.getAddressesFromNameService (InetAddress.java:1323)</span><br><span class="line">at java.net.InetAddress.getAllByName0 (InetAddress.java:1276)</span><br><span class="line">at java.net.InetAddress.getAllByName (InetAddress.java:1192)</span><br><span class="line">at java.net.InetAddress.getAllByName (InetAddress.java:1126)</span><br><span class="line">at java.net.InetAddress.getByName (InetAddress.java:1076)</span><br><span class="line">at com.yyy.zzz.commons.infrastructure.client.EsClient.&lt;init&gt;(EsClient.java:43)</span><br><span class="line">... 64 more</span><br></pre></td></tr></table></figure><p>同时，在之后的请求中，只要是转发到这个服务节点的请求，就会出现如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">com.yyy.zzz.exception.hbase.HBaseException: java.lang.reflect.InvocationTargetException</span><br><span class="line">Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection (ConnectionFactory.java:240)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.createConnection (ConnectionManager.java:433)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.createConnection (ConnectionManager.java:426)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.getConnectionInternal (ConnectionManager.java:304)</span><br><span class="line">    at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:185)</span><br><span class="line">    at org.apache.hadoop.hbase.client.HTableFactory.createHTableInterface (HTableFactory.java:41)</span><br><span class="line">    ... 18 more</span><br></pre></td></tr></table></figure><p>通过排查代码，这个异常是在业务逻辑代码连接 <code>HBase</code> 表取数时出现的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hTableInterface.get (List&lt;Get&gt;)</span><br></pre></td></tr></table></figure><p>每一次连接 <code>HBase</code> 取数，都会有这个异常出现。</p><h1 id="问题排查"><a href="# 问题排查" class="headerlink" title="问题排查"></a>问题排查 </h1><p> 首先怀疑的是 <code>protobuf</code> 版本冲突问题，但是通过对比，发现只有一个确定版本的 <code>jar</code> 包，而且对比其它节点，并没有这个问题出现，最终否定了这个猜测。</p><p>接着尝试发送多次请求，查看日志，以下错误不再出现【也很合理，这些异常是在服务注册初始化时只出现一次】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil</span><br><span class="line">com.yyy.zzz.exception.es.EsConnException: java.net.UnknownHostException: host40: Temporary failure in name resolution</span><br></pre></td></tr></table></figure><p>反而出现的全部是 <code>HBase</code> 取数异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">com.yyy.zzz.exception.hbase.HBaseException: java.lang.reflect.InvocationTargetException</span><br><span class="line">Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection (ConnectionFactory.java:240)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.createConnection (ConnectionManager.java:433)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.createConnection (ConnectionManager.java:426)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.getConnectionInternal (ConnectionManager.java:304)</span><br><span class="line">    at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:185)</span><br><span class="line">    at org.apache.hadoop.hbase.client.HTableFactory.createHTableInterface (HTableFactory.java:41)</span><br><span class="line">    ... 18 more</span><br></pre></td></tr></table></figure><p>更神奇的是，只在一台节点上面有问题，其它相同功能的节点没问题。</p><p>最终，通过运维排查，从 <code>NoClassDefFoundError</code> 以及 <code>UnknownHostException</code> 发现了异常原因：在某个时间点发布服务时，恰好此时机器负载过高，导致 <code>DNS</code> 解析异常，于是 <code>dubbo</code> 服务在注册时无法获取 <code>hdfs</code> 信息。而 <code>HBase</code> 在初始化时需要依赖 <code>hdfs</code> 上面的某个 <code>hbase.version</code> 文件【用来确定 <code>HBase</code> 的版本】，导致 <code>HBase</code> 在初始化时无法找到这个文件，也就无法确定版本，最终没有加载 <code>ProtobufUtil</code> 类文件。</p><p><code>hdfs-site.xml</code> 配置文件中的重要内容如下，<code>nn1</code> 节点无法被识别：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.hdfs-cluster&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><code>hbase-site.xml</code> 配置文件中的重要内容如下，对于 <code>HBase</code> 来说，这个 <code>hdfs</code> 路径里面存放着重要的信息，如果无法读取它也就无法成功初始化 <code>HBase</code> 环境：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hdfs-cluster/apps/hbase/data&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>所以此后所有的请求需要连接 <code>HBase</code> 取数时，都会出现 <code>java.lang.reflect.InvocationTargetException</code> 异常。</p><p>这里会进一步引发一个严重的问题，由于 <code>dubbo</code> 服务在注册时出现问题没有退出，仍旧提供服务，但是这个服务是有问题的，每次需要连接 <code>HBase</code> 取数时都会出现异常，由于没有处理好异常，导致大量的 <code>Zookeeper</code> 连接没有关闭。</p><p>进一步导致当前机器的 <code>Zookeeper</code> 连接数接近 1000 个，严重影响了其它业务连接 <code>Zookeeper</code>，一律是等待、超时重试。</p><h1 id="问题解决"><a href="# 问题解决" class="headerlink" title="问题解决"></a>问题解决 </h1><p> 找到问题原因，就很容易解决了，重启对应的服务，观察初始化日志，一切正常。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;背景说明：通过 &lt;code&gt;dubbo&lt;/code&gt; 部署一个服务，服务中的业务逻辑会查询 &lt;code&gt;HBase&lt;/code&gt; 表的数据，但是 &lt;code&gt;dubbo&lt;/code&gt; 服务在初始化注册时，&lt;code&gt;HBase&lt;/code&gt; 初始化的过程中会报错：&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;本文涉及的开发环境，基于 &lt;code&gt;HBase v1.1.2&lt;/code&gt;、&lt;code&gt;Zookeeper v3.4.6&lt;/code&gt;、&lt;code&gt;dubbo v2.8.4&lt;/code&gt;、&lt;code&gt;Hadoop v2.7.1&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Hadoop" scheme="https://www.playpi.org/tags/Hadoop/"/>
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="Zookeeper" scheme="https://www.playpi.org/tags/Zookeeper/"/>
    
      <category term="dubbo" scheme="https://www.playpi.org/tags/dubbo/"/>
    
  </entry>
  
  <entry>
    <title>HBase 错误之 ConnectionLoss for hbase-unsecure</title>
    <link href="https://www.playpi.org/2019092901.html"/>
    <id>https://www.playpi.org/2019092901.html</id>
    <published>2019-09-29T13:11:53.000Z</published>
    <updated>2019-09-30T13:11:53.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --><p>在当前的业务中，需要连接 <code>HBase</code> 获取数据，但是最近在某一台节点上面的进程总是出现连接异常，类似下面：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2019-09-20_18:54:44 [http-nio-28956-exec-5] WARN zookeeper.ZKUtil:629: hconnection-0x8a9f6680x0, quorum=host1:2181,host10:2181,host11:2181,host61:2181,host62:2181, baseZNode=/hbase-unsecure Unable to get data of znode /hbase-unsecure/meta-region-server</span><br><span class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/meta-region-server</span><br><span class="line">at org.apache.zookeeper.KeeperException.create (KeeperException.java:99)</span><br></pre></td></tr></table></figure><p>看起来是连接超时，然后重试，日志中持续了多次。本文开发环境基于 <code>HBase v1.1.2</code>、<code>Zookeeper v3.4.6</code>、<code>Hadoop v2.7.1</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 一个正常的连接 <code>HBase</code> 取数的服务，在某个节点上出现大量的异常日志，无法连接到 <code>HBase</code>，一直在重试，同时观察到在其它节点上相同的服务却是正常的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br></pre></td><td class="code"><pre><span class="line">2019-09-20_18:54:44 [http-nio-28956-exec-5] ERROR zookeeper.RecoverableZooKeeper:277: ZooKeeper getData failed after 4 attempts</span><br><span class="line">2019-09-20_18:54:44 [http-nio-28956-exec-5] WARN zookeeper.ZKUtil:629: hconnection-0x8a9f6680x0, quorum=host1:2181,host10:2181,host11:2181,host61:2181,host62:2181, baseZNode=/hbase-unsecure Unable to get data of znode /hbase-unsecure/meta-region-server</span><br><span class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/meta-region-server</span><br><span class="line">at org.apache.zookeeper.KeeperException.create (KeeperException.java:99)</span><br><span class="line">at org.apache.zookeeper.KeeperException.create (KeeperException.java:51)</span><br><span class="line">at org.apache.zookeeper.ZooKeeper.getData (ZooKeeper.java:1155)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData (RecoverableZooKeeper.java:359)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData (ZKUtil.java:621)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState (MetaTableLocator.java:481)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation (MetaTableLocator.java:167)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:598)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:579)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:558)</span><br><span class="line">at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation (ZooKeeperRegistry.java:61)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta (ConnectionManager.java:1192)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1159)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations (RpcRetryingCallerWithReadReplicas.java:300)</span><br><span class="line">at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call (ScannerCallableWithReplicas.java:152)</span><br><span class="line">at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call (ScannerCallableWithReplicas.java:60)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries (RpcRetryingCaller.java:200)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.loadCache (ClientSmallReversedScanner.java:211)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.next (ClientSmallReversedScanner.java:185)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta (ConnectionManager.java:1256)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1162)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1146)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1103)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getRegionLocation (ConnectionManager.java:938)</span><br><span class="line">at org.apache.hadoop.hbase.client.HRegionLocator.getRegionLocation (HRegionLocator.java:83)</span><br><span class="line">at org.apache.hadoop.hbase.client.RegionServerCallable.prepare (RegionServerCallable.java:79)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries (RpcRetryingCaller.java:124)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:889)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:855)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:908)</span><br><span class="line">at com.xxx.yyy.commons.search.reader.hbase.BaseHBaseReader.batchGet (BaseHBaseReader.java:94)</span><br><span class="line">at com.xxx.yyy.commons.search.searcher.AbstractBaseSearcher.getContent (AbstractBaseSearcher.java:269)</span><br><span class="line">at com.xxx.yyy.commons.search.searcher.AbstractBaseSearcher.getInfo (AbstractBaseSearcher.java:194)</span><br><span class="line">at com.xxx.yyy.runner.search.BaseSearchRunner.combinaSearch (BaseSearchRunner.java:139)</span><br><span class="line">at com.xxx.yyy.api.newsforum.NewsForumPostServiceImpl.combinaSearch (NewsForumPostServiceImpl.java:53)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.Wrapper9.invokeMethod (Wrapper9.java)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory$1.doInvoke (JavassistProxyFactory.java:46)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.AbstractProxyInvoker.invoke (AbstractProxyInvoker.java:72)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.InvokerWrapper.invoke (InvokerWrapper.java:53)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ExceptionFilter.invoke (ExceptionFilter.java:64)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.monitor.support.MonitorFilter.invoke (MonitorFilter.java:75)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.TimeoutFilter.invoke (TimeoutFilter.java:42)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.dubbo.filter.TraceFilter.invoke (TraceFilter.java:78)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ContextFilter.invoke (ContextFilter.java:70)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.GenericFilter.invoke (GenericFilter.java:132)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ClassLoaderFilter.invoke (ClassLoaderFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.EchoFilter.invoke (EchoFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.InvokerInvocationHandler.invoke (InvokerInvocationHandler.java:52)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.proxy4.combinaSearch (proxy4.java)</span><br><span class="line">at sun.reflect.GeneratedMethodAccessor87.invoke (Unknown Source)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke (Method.java:498)</span><br><span class="line">at org.jboss.resteasy.core.MethodInjectorImpl.invoke (MethodInjectorImpl.java:137)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget (ResourceMethodInvoker.java:288)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:242)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:229)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:356)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:179)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.ServletContainerDispatcher.service (ServletContainerDispatcher.java:220)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:56)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:51)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.rest.DubboHttpServer$RestHandler.handle (DubboHttpServer.java:86)</span><br><span class="line">at com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet.service (DispatcherServlet.java:64)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:291)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter (ApplicationFilterChain.java:206)</span><br><span class="line">at org.apache.catalina.core.StandardWrapperValve.invoke (StandardWrapperValve.java:219)</span><br><span class="line">at org.apache.catalina.core.StandardContextValve.invoke (StandardContextValve.java:106)</span><br><span class="line">at org.apache.catalina.authenticator.AuthenticatorBase.invoke (AuthenticatorBase.java:504)</span><br><span class="line">at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:142)</span><br><span class="line">at org.apache.catalina.valves.ErrorReportValve.invoke (ErrorReportValve.java:79)</span><br><span class="line">at org.apache.catalina.core.StandardEngineValve.invoke (StandardEngineValve.java:88)</span><br><span class="line">at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:534)</span><br><span class="line">at org.apache.coyote.http11.AbstractHttp11Processor.process (AbstractHttp11Processor.java:1081)</span><br><span class="line">at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process (AbstractProtocol.java:658)</span><br><span class="line">at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process (Http11NioProtocol.java:222)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun (NioEndpoint.java:1566)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run (NioEndpoint.java:1523)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run (TaskThread.java:61)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">2019-09-20_18:54:45 [http-nio-28956-exec-5] ERROR zookeeper.ZooKeeperWatcher:655: hconnection-0x8a9f6680x0, quorum=host1:2181,host10:2181,host11:2181,host61:2181,host62:2181, baseZNode=/hbase-unsecure Received unexpected KeeperException, re-throwing exception</span><br><span class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/meta-region-server</span><br><span class="line">at org.apache.zookeeper.KeeperException.create (KeeperException.java:99)</span><br><span class="line">at org.apache.zookeeper.KeeperException.create (KeeperException.java:51)</span><br><span class="line">at org.apache.zookeeper.ZooKeeper.getData (ZooKeeper.java:1155)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData (RecoverableZooKeeper.java:359)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData (ZKUtil.java:621)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState (MetaTableLocator.java:481)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation (MetaTableLocator.java:167)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:598)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:579)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:558)</span><br><span class="line">at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation (ZooKeeperRegistry.java:61)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta (ConnectionManager.java:1192)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1159)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations (RpcRetryingCallerWithReadReplicas.java:300)</span><br><span class="line">at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call (ScannerCallableWithReplicas.java:152)</span><br><span class="line">at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call (ScannerCallableWithReplicas.java:60)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries (RpcRetryingCaller.java:200)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.loadCache (ClientSmallReversedScanner.java:211)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.next (ClientSmallReversedScanner.java:185)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta (ConnectionManager.java:1256)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1162)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1146)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1103)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getRegionLocation (ConnectionManager.java:938)</span><br><span class="line">at org.apache.hadoop.hbase.client.HRegionLocator.getRegionLocation (HRegionLocator.java:83)</span><br><span class="line">at org.apache.hadoop.hbase.client.RegionServerCallable.prepare (RegionServerCallable.java:79)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries (RpcRetryingCaller.java:124)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:889)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:855)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:908)</span><br><span class="line">at com.xxx.yyy.commons.search.reader.hbase.BaseHBaseReader.batchGet (BaseHBaseReader.java:94)</span><br><span class="line">at com.xxx.yyy.commons.search.searcher.AbstractBaseSearcher.getContent (AbstractBaseSearcher.java:269)</span><br><span class="line">at com.xxx.yyy.commons.search.searcher.AbstractBaseSearcher.getInfo (AbstractBaseSearcher.java:194)</span><br><span class="line">at com.xxx.yyy.runner.search.BaseSearchRunner.combinaSearch (BaseSearchRunner.java:139)</span><br><span class="line">at com.xxx.yyy.api.newsforum.NewsForumPostServiceImpl.combinaSearch (NewsForumPostServiceImpl.java:53)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.Wrapper9.invokeMethod (Wrapper9.java)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory$1.doInvoke (JavassistProxyFactory.java:46)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.AbstractProxyInvoker.invoke (AbstractProxyInvoker.java:72)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.InvokerWrapper.invoke (InvokerWrapper.java:53)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ExceptionFilter.invoke (ExceptionFilter.java:64)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.monitor.support.MonitorFilter.invoke (MonitorFilter.java:75)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.TimeoutFilter.invoke (TimeoutFilter.java:42)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.dubbo.filter.TraceFilter.invoke (TraceFilter.java:78)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ContextFilter.invoke (ContextFilter.java:70)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.GenericFilter.invoke (GenericFilter.java:132)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ClassLoaderFilter.invoke (ClassLoaderFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.EchoFilter.invoke (EchoFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.InvokerInvocationHandler.invoke (InvokerInvocationHandler.java:52)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.proxy4.combinaSearch (proxy4.java)</span><br><span class="line">at sun.reflect.GeneratedMethodAccessor87.invoke (Unknown Source)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke (Method.java:498)</span><br><span class="line">at org.jboss.resteasy.core.MethodInjectorImpl.invoke (MethodInjectorImpl.java:137)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget (ResourceMethodInvoker.java:288)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:242)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:229)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:356)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:179)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.ServletContainerDispatcher.service (ServletContainerDispatcher.java:220)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:56)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:51)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.rest.DubboHttpServer$RestHandler.handle (DubboHttpServer.java:86)</span><br><span class="line">at com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet.service (DispatcherServlet.java:64)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:291)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter (ApplicationFilterChain.java:206)</span><br><span class="line">at org.apache.catalina.core.StandardWrapperValve.invoke (StandardWrapperValve.java:219)</span><br><span class="line">at org.apache.catalina.core.StandardContextValve.invoke (StandardContextValve.java:106)</span><br><span class="line">at org.apache.catalina.authenticator.AuthenticatorBase.invoke (AuthenticatorBase.java:504)</span><br><span class="line">at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:142)</span><br><span class="line">at org.apache.catalina.valves.ErrorReportValve.invoke (ErrorReportValve.java:79)</span><br><span class="line">at org.apache.catalina.core.StandardEngineValve.invoke (StandardEngineValve.java:88)</span><br><span class="line">at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:534)</span><br><span class="line">at org.apache.coyote.http11.AbstractHttp11Processor.process (AbstractHttp11Processor.java:1081)</span><br><span class="line">at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process (AbstractProtocol.java:658)</span><br><span class="line">at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process (Http11NioProtocol.java:222)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun (NioEndpoint.java:1566)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run (NioEndpoint.java:1523)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run (TaskThread.java:61)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">2019-09-20_18:54:45 [http-nio-28956-exec-10-SendThread (host1:2181)] INFO zookeeper.ClientCnxn:975: Opening socket connection to server host1/192.168.20.101:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">2019-09-20_18:54:45 [http-nio-28956-exec-10-SendThread (host1:2181)] INFO zookeeper.ClientCnxn:852: Socket connection established to host1/192.168.20.101:2181, initiating session</span><br><span class="line">2019-09-20_18:54:45 [http-nio-28956-exec-10-SendThread (host1:2181)] WARN zookeeper.ClientCnxn:1102: Session 0x0 for server host1/192.168.20.101:2181, unexpected error, closing socket connection and attempting reconnect</span><br><span class="line">java.io.IOException: Connection reset by peer</span><br><span class="line">at sun.nio.ch.FileDispatcherImpl.read0 (Native Method)</span><br><span class="line">at sun.nio.ch.SocketDispatcher.read (SocketDispatcher.java:39)</span><br><span class="line">at sun.nio.ch.IOUtil.readIntoNativeBuffer (IOUtil.java:223)</span><br><span class="line">at sun.nio.ch.IOUtil.read (IOUtil.java:192)</span><br><span class="line">at sun.nio.ch.SocketChannelImpl.read (SocketChannelImpl.java:380)</span><br><span class="line">at org.apache.zookeeper.ClientCnxnSocketNIO.doIO (ClientCnxnSocketNIO.java:68)</span><br><span class="line">at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport (ClientCnxnSocketNIO.java:366)</span><br><span class="line">at org.apache.zookeeper.ClientCnxn$SendThread.run (ClientCnxn.java:1081)</span><br></pre></td></tr></table></figure><p>注意查看重点内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2019-09-20_18:54:44 [http-nio-28956-exec-5] ERROR zookeeper.RecoverableZooKeeper:277: ZooKeeper getData failed after 4 attempts</span><br><span class="line">2019-09-20_18:54:44 [http-nio-28956-exec-5] WARN zookeeper.ZKUtil:629: hconnection-0x8a9f6680x0, quorum=host1:2181,host10:2181,host11:2181,host61:2181,host62:2181, baseZNode=/hbase-unsecure Unable to get data of znode /hbase-unsecure/meta-region-server</span><br><span class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/meta-region-server</span><br><span class="line">...</span><br><span class="line">2019-09-20_18:54:45 [http-nio-28956-exec-10-SendThread (host1:2181)] WARN zookeeper.ClientCnxn:1102: Session 0x0 for server host1/192.168.20.101:2181, unexpected error, closing socket connection and attempting reconnect</span><br></pre></td></tr></table></figure><p>看起来是当前节点网络有问题，或者 <code>Zookeeper</code> 连接资源紧张。</p><p>与此同时，还有大量类似下面这种连接重试出现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-8-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e055f894, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-2-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e040a3c2, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-8-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e040a720, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-6-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x46d5ce1483b88cf, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-2-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e040a349, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-6-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e03f75aa, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-4-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e040a8f0, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br></pre></td></tr></table></figure><p>其实就是有进程在占用过多的 <code>Zookeeper</code> 连接，导致 <code>Zookeeper</code> 的 <code>Server</code> 端拒绝响应。</p><h1 id="问题排查"><a href="# 问题排查" class="headerlink" title="问题排查"></a>问题排查 </h1><p> 由于没有 <code>root</code> 权限，只能请运维帮忙排查，通过排查，发现当前主机创建的 <code>Zookeeper</code> 连接数过多，超过了设置的最大值。</p><p>使用 <code>netstat -antp | grep 2181 | wc -l</code> 命令，注意需要 <code>root</code> 用户的权限。这个命令统计的是所有 <code>Zookeeper</code> 连接【通过使用 2181 端口过滤】，包含等待的和正在通信的，如果查看正在通信的，加上一个 <code>grep ESTABLISHED</code> 过滤即可。</p><p>局部截图如下：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190930214400.png" alt="zk 连接进程查看" title="zk 连接进程查看"></p><p>由于直接发现了问题，所以也不用进一步查看 <code>Zookeeper</code> 的日志了。</p><p>至于为什么 <code>Zookeeper</code> 的连接数会这么多，罪魁祸首请读者参考我的另外一篇博客：<br><a href="https://www.playpi.org/2019093001.html">HBase 错误之 NoClassDefFoundError：ProtobufUtil</a> 。</p><p>由于当前节点创建的 <code>Zookeeper</code> 连接数过多，所以再创建新连接时无法顺利连接通信，一直等待重试。</p><h1 id="问题解决"><a href="# 问题解决" class="headerlink" title="问题解决"></a>问题解决 </h1><p> 问题排查出来，解决就简单了，直接找到问题程序，修复资源泄漏问题，然后重启，保证合理的 <code>Zookeeper</code> 连接数量，不要因为某一个程序的失误而影响到其它业务。</p><p>另外如果有必要查看 <code>Zookeeper</code> 日志，需要特别留意 <code>Zookeeper</code> 查看日志的方法，日志文件是不能被直接打开的，需要工具转换为文本日志，然后才能查看分析。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在当前的业务中，需要连接 &lt;code&gt;HBase&lt;/code&gt; 获取数据，但是最近在某一台节点上面的进程总是出现连接异常，类似下面：&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;2019-09-20_18:54:44 [http-nio-28956-exec-5] WARN zookeeper.ZKUtil:629: hconnection-0x8a9f6680x0, quorum=host1:2181,host10:2181,host11:2181,host61:2181,host62:2181, baseZNode=/hbase-unsecure Unable to get data of znode /hbase-unsecure/meta-region-server&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/meta-region-server&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	at org.apache.zookeeper.KeeperException.create (KeeperException.java:99)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;看起来是连接超时，然后重试，日志中持续了多次。本文开发环境基于 &lt;code&gt;HBase v1.1.2&lt;/code&gt;、&lt;code&gt;Zookeeper v3.4.6&lt;/code&gt;、&lt;code&gt;Hadoop v2.7.1&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Hadoop" scheme="https://www.playpi.org/tags/Hadoop/"/>
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="Zookeeper" scheme="https://www.playpi.org/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>爬山徒步：竹海丛林 - 广州第二峰鸡枕山</title>
    <link href="https://www.playpi.org/2019092201.html"/>
    <id>https://www.playpi.org/2019092201.html</id>
    <published>2019-09-22T08:03:49.000Z</published>
    <updated>2019-09-22T08:03:49.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>在 2019 年 9 月 21 日，我们几个小伙伴相约去广州鸡枕山登山徒步，给久坐的身体放松一下。<strong> 鸡枕山 </strong>位于广东省广州市从化区良口镇，海拔 1175 米，为广州市的第二高峰，仅次于天堂顶。另外有一点比较好的地方在于，鸡枕山虽然是广州第二高峰，但是它全程都被树木、竹林遮挡，基本不会被晒到，而且山路比较平缓，除了几处陡一点的路段，整体来说走起来非常舒适。</p><p>因此，这条线路非常适合没有登山经验的人，或者是平时缺少锻炼的人，或者体能差的人，这是一个比较好的入门路线。本文记录这次徒步的过程，给读者一个观察参考。</p><a id="more"></a><h1 id="集合"><a href="# 集合" class="headerlink" title="集合"></a>集合 </h1><p> 我们规定 07:00 在 <strong>客村 </strong>地铁站集合，签到、吃早餐、准备食物水等必需品。这时候有些人没有买够水的去买，没有买够食物的也去买，保证登山过程中能量补给。</p><h1 id="启程"><a href="# 启程" class="headerlink" title="启程"></a>启程 </h1><p>07:40，所有人到齐，准备出发，一个大巴装了 40 多人。</p><h1 id="到达"><a href="# 到达" class="headerlink" title="到达"></a> 到达 </h1><p>08:50 到达服务区，大家下车休息，还可以买水，09:00 继续出发。</p><p> 由于领队的麦克风坏掉了，所以没有进行互动，在过了服务区快要到达的时候，领队讲了一下注意事项。</p><p>在 09:50 到达山脚，做了一下热身，稍微休息几分钟，准备登山。</p><p>为了保留体力，避免运动过量，我们决定慢慢走，全程都是最后一批，其中一个领队就在我们后面。</p><h1 id="登山"><a href="# 登山" class="headerlink" title="登山"></a>登山 </h1><p>10:00 开始登山。</p><p> 在登山入口，我们的小队伍合影。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923005645.jpg" alt="山脚小合照" title="山脚小合照"></p><p>登山路过竹林，有人仰望天空，发现的心形竹林空隙。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923005740.jpg" alt="心形的竹林空隙" title="心形的竹林空隙"></p><p>走过竹林间的小路，看光影婆娑。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923005830.jpg" alt="竹林间的小路" title="竹林间的小路"></p><p>路过小水坝，领队抓拍到的如画风景，我是倒数第二个人。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923005941.jpg" alt="路过小水坝" title="路过小水坝"></p><p>这个小水坝是很小的，看看小水坝的蓄水池就知道了。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923010019.jpg" alt="小水坝的蓄水池" title="小水坝的蓄水池"></p><p>总是有人善于观察，看看路边的小花，斑驳的台阶。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923010102.jpg" alt="路边的小花" title="路边的小花"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923010113.jpg" alt="斑驳的台阶" title="斑驳的台阶"></p><p>路上少有的开阔视野，可以看看风景，远处的蓝蓝天空。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923010213.jpg" alt="蓝蓝的天空" title="蓝蓝的天空"></p><h1 id="午餐休息"><a href="# 午餐休息" class="headerlink" title="午餐休息"></a>午餐休息 </h1><p>12:00 到达登顶前的平台，可以休息吃午餐，养好体力准备登顶。</p><p> 领队竟然带了一个西瓜，背上来整整一个大西瓜，切开大家分了，竟然是冰冻的，爽口解渴。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011540.jpg" alt="切西瓜" title="切西瓜"></p><p>吃瓜群众，阳光洒在我全身，像个孩子。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011556.jpg" alt="吃瓜群众" title="吃瓜群众"></p><p>我带的西红柿，都给队友吃了。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011609.jpg" alt="吃西红柿" title="吃西红柿"></p><h1 id="登顶"><a href="# 登顶" class="headerlink" title="登顶"></a>登顶 </h1><p>12:40 开始登顶。</p><p> 最后有一段稍微陡峭的登顶路段，全程是暴露在阳光下的，只有灌木丛，慢慢登上去大概需要 20 分钟。</p><p>登顶大合照之一。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011649.jpg" alt="登顶大合照之一" title="登顶大合照之一"></p><p>登顶大合照之二。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011702.jpg" alt="登顶大合照之二" title="登顶大合照之二"></p><p>登顶小合照，我站在最高的石头上，眼睛被阳光照射，只能眯得很小。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011715.jpg" alt="登顶小合照" title="登顶小合照"></p><p>可以看到鸡枕山卫峰。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011731.jpg" alt="鸡枕山卫峰" title="鸡枕山卫峰"></p><p>山顶的平台被阳光直射，还是有点晒的，不过还好有风，感觉不是那么炎热，在上面遥看远方，风景如画，谈笑风生。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011904.jpg" alt="登顶远眺之一" title="登顶远眺之一"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011915.jpg" alt="登顶远眺之二" title="登顶远眺之二"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011924.jpg" alt="登顶远眺之三" title="登顶远眺之三"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011934.jpg" alt="登顶远眺之四" title="登顶远眺之四"></p><h1 id="下山"><a href="# 下山" class="headerlink" title="下山"></a>下山 </h1><p>13:30 开始下山，一路小树林、竹林、灌木丛、缓坡。</p><p> 下山会有几段灌木丛的路，大概是下图这样，感觉像是玉米地，每次路过时双手挡脸，要防止被叶子刮伤。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923012043.jpg" alt="灌木丛" title="灌木丛"></p><p>下山遇到的竹林，不过此时已经很累，无心观赏。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923012346.jpg" alt="下山遇到的竹林" title="下山遇到的竹林"></p><p>仰望竹林。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923012433.jpg" alt="仰望竹林" title="仰望竹林"></p><p>阳光洒下，一片片翠绿金黄的竹叶；微风袭来，一阵阵扑面而来的凉爽。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923012128.jpg" alt="翠绿金黄的竹林" title="翠绿金黄的竹林"></p><p>竟然在山脚遇到了一群走地鸡，公鸡的羽毛特别漂亮。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923012211.jpg" alt="走地鸡" title="走地鸡"></p><h1 id="集合返程"><a href="# 集合返程" class="headerlink" title="集合返程"></a>集合返程 </h1><p> 在 16:30 到达山脚。</p><p>返程到山脚，有一些人很早就到了，等了两个多小时。</p><p>大家换衣服、洗脸、去厕所，休息一会，然后喝水、吃东西，聊聊天。</p><p>一切准备就绪后，大家集合，准备返程。</p><h1 id="归来"><a href="# 归来" class="headerlink" title="归来"></a>归来 </h1><p>17:00 开始返程。</p><p> 在下山的路上，由于是曲折蜿蜒的 S 型路线，而且路比较窄，所以堵车很严重，特别在转弯处，还需要下去人去指挥，毕竟安全第一。</p><p>本来 10 分钟可以走完的路花了将近 1 小时。</p><p>最后在 20:00 到达广州。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在 2019 年 9 月 21 日，我们几个小伙伴相约去广州鸡枕山登山徒步，给久坐的身体放松一下。&lt;strong&gt; 鸡枕山 &lt;/strong&gt;位于广东省广州市从化区良口镇，海拔 1175 米，为广州市的第二高峰，仅次于天堂顶。另外有一点比较好的地方在于，鸡枕山虽然是广州第二高峰，但是它全程都被树木、竹林遮挡，基本不会被晒到，而且山路比较平缓，除了几处陡一点的路段，整体来说走起来非常舒适。&lt;/p&gt;&lt;p&gt;因此，这条线路非常适合没有登山经验的人，或者是平时缺少锻炼的人，或者体能差的人，这是一个比较好的入门路线。本文记录这次徒步的过程，给读者一个观察参考。&lt;/p&gt;
    
    </summary>
    
      <category term="游玩" scheme="https://www.playpi.org/categories/have-for-fun/"/>
    
    
      <category term="Guangzhou" scheme="https://www.playpi.org/tags/Guangzhou/"/>
    
      <category term="hike" scheme="https://www.playpi.org/tags/hike/"/>
    
      <category term="climbing" scheme="https://www.playpi.org/tags/climbing/"/>
    
  </entry>
  
  <entry>
    <title>Zookeeper 日志查看</title>
    <link href="https://www.playpi.org/2019092001.html"/>
    <id>https://www.playpi.org/2019092001.html</id>
    <published>2019-09-20T13:26:41.000Z</published>
    <updated>2020-02-11T13:26:41.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p><code>Zookeeper</code> 是大数据组件中不可或缺的一种组件，是一个开源的分布式协调服务，提供了诸如统一命名服务、配置管理、集群管理等功能，在很多场景中都可以见到它的身影。本文简单介绍 <code>Zookeeper</code> 的日志查看，开发环境基于 <code>v3.4.6</code>。</p><a id="more"></a><h1 id="开篇"><a href="# 开篇" class="headerlink" title="开篇"></a>开篇 </h1><p><code>Zookeeper</code> 服务器会产生三类日志：<strong> 事务日志 </strong>、<strong> 快照日志 </strong> 和 <strong>log4j 日志 </strong>。</p><p>在 <code>Zookeeper</code> 默认的配置文件 <code>zoo.cfg</code>【也可以修改文件名，在 <code>bin/zkEnv.sh</code> 指定使用的配置文件】中有一个配置项 <code>dataDir</code>，该配置项用于配置 <code>Zookeeper</code> 快照日志和事务日志的存储地址。</p><p>在官方提供的默认参考配置文件 <code>zoo_sample.cfg</code> 中【解压下载的安装包后，在 <code>conf</code> 目录下可以找到】，只有 <code>dataDir</code> 配置项，配置为：<code>dataDir=/tmp/zookeeper</code>。其实在实际应用中，还可以为事务日志专门配置存储地址，配置项名称为 <code>dataLogDir</code>，在 <code>zoo_sample.cfg</code> 中并未体现出来。</p><p>在没有 <code>dataLogDir</code> 配置项的时候，<code>Zookeeper</code> 默认将事务日志文件和快照日志文件都存储在 <code>dataDir</code> 对应的目录下。但是我们建议将事务日志【<code>dataLogDir</code>】与快照日志【<code>dataDir</code>】单独配置，存储时分开存储，因为当 <code>Zookeeper</code> 集群进行频繁的数据读写操作时，会产生大量的事务日志信息，将两类日志分开存储会提高系统的性能。而且，还可以允许将两类日志存储在不同的存储介质上，减少单一的磁盘压力。</p><p>总结，<code>dataDir</code> 表示快照日志目录，<code>dataLogDir</code> 表示事务日志目录【不配置的时候事务日志目录同 <code>dataDir</code>】。</p><p>而 <code>log4j</code> 用于记录 <code>Zookeeper</code> 集群服务器运行日志，该日志的配置项在安装包解压后的 <code>conf</code> 目录下的 <code>log4j.properties</code> 文件中【这个想必很多读者都熟悉了】。在 <code>bin/zkEnv.sh</code> 文件中有使用到一个变量 <code>ZOO_LOG_DIR=.</code>，表示 <code>log4j</code> 日志文件与执行程序【<code>zkServer.sh</code>】在同一目录下，当执行 <code>zkServer.sh</code> 时，在该目录下会产生 <code>zookeeper.out</code> 日志文件；另外还有一个变量 <code>ZOO_LOG4J_PROP</code> 表示日志级别。而这些变量，都可以在 <code>conf/zookeeper-env.sh</code> 文件中提前设置好，例如：<code>export ZOO_LOG_DIR=/var/log/zookeeper</code>。</p><p>下面主要介绍 <strong>事务日志 </strong>与 <strong>快照日志 </strong>。</p><h1 id="事务日志"><a href="# 事务日志" class="headerlink" title="事务日志"></a>事务日志 </h1><p> 事务日志是指 <code>Zookeeper</code> 系统在正常运行过程中，针对所有的更新操作，在返回客户端 <strong>更新成功 </strong>的响应前，<code>Zookeeper</code> 会保证已经将本次更新操作的事务日志写到磁盘上，只有这样，整个更新操作才会生效。</p><p>根据上文所述，可以通过 <code>zoo.cfg</code> 文件中的 <code>dataLogDir</code> 配置项找到事务日志存储的路径：<code>dataLogDir=/cloud/data1/hadoop/zookeeper</code>【如果没有配置就看 <code>dataDir</code> 参数】，在 <code>dataLogDir</code> 对应的目录下存在一个文件夹 <code>version-2</code>，该文件夹中保存着所有事务日志文件，例如：<code>log.6305208d3f</code>。</p><p>事务日志文件的命名规则为 <code>log.**</code>，文件大小为 <code>64MB</code>【超过后就会生成新的事务日志文件】，<code>**</code> 表示写入该日志文件的第一个事务的 <code>ID</code>【也可以说触发该日志文件的事务】，十六进制表示。日志文件的个数与参数 <code>autopurge.snapRetainCount</code> 配置有关，默认是 3 个，本文最后也会讲到。</p><h2 id="事务日志可视化"><a href="# 事务日志可视化" class="headerlink" title="事务日志可视化"></a>事务日志可视化 </h2><p><code>Zookeeper</code> 的事务日志为二进制文件，不能通过 <code>vim</code> 等工具直接访问，其实可以通过 <code>Zookeeper</code> 自带的 <code>jar</code> 包中的工具类读取事务日志文件。</p><p> 首先将 <code>libs</code> 中的 <code>slf4j-api-1.6.1.jar</code> 文件和 <code>Zookeeper</code> 根目录下的 <code>Zookeeper-3.4.6.jar</code> 文件复制到临时文件夹 <code>tmplibs</code> 中【不复制也可以，只要明确 <code>jar</code> 包位置，引用一下即可】，然后执行如下命令，将原始事务日志内容输出至 <code>6305208d3f.log</code> 文件中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -classpath .:./slf4j-api-1.6.1.jar:./zookeeper-3.4.6.2.2.6.0-2800.jar org.apache.zookeeper.server.LogFormatter ./log.6305208d3f &gt; 6305208d3f.log</span><br></pre></td></tr></table></figure><p>实际上就是手动调用 <code>Zookeeper</code> 包里面的实现类，把二进制日志内容格式化，转换为人类可以理解的日志。<code>-classpath</code> 就是在 <code>Java</code> 中设置第三方 <code>jar</code> 包的方式。</p><p>产生可以查看的日志文件一份：<code>6305208d3f.log</code>。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200212010141.png" alt="事务日志文件转换" title="事务日志文件转换"></p><h2 id="事务日志分析"><a href="# 事务日志分析" class="headerlink" title="事务日志分析"></a>事务日志分析 </h2><p> 声明一下，每次对 <code>Zookeeper</code> 操作导致的状态改变都会产生一个 <code>zxid</code>，即 <code>ZooKeeper Transaction Id</code>。</p><p>接着就可以挑一些日志进行简单分析一下，输出前 10 行，最好能挑到前后有关联的，例如同一个会话的打开、关闭操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">ZooKeeper Transactional Log File with dbid 0 txnlog format version 2</span><br><span class="line">19-9-20 下午 06 时 04 分 05 秒 session 0x46d0bf6e8cc50b8 cxid 0x1 zxid 0x6305208d3f error -101</span><br><span class="line"></span><br><span class="line">19-9-20 下午 06 时 04 分 05 秒 session 0x56af0a7efa70533 cxid 0x2 zxid 0x6305208d40 closeSession null</span><br><span class="line">19-9-20 下午 06 时 04 分 05 秒 session 0x46d0bf6e8cc50b8 cxid 0x2 zxid 0x6305208d41 closeSession null</span><br><span class="line">19-9-20 下午 06 时 04 分 05 秒 session 0x56af0a7efa70534 cxid 0x3 zxid 0x6305208d42 closeSession null</span><br><span class="line">19-9-20 下午 06 时 04 分 05 秒 session 0x66af3690a390cc4 cxid 0x0 zxid 0x6305208d43 createSession 4000</span><br><span class="line"></span><br><span class="line">19-9-20 下午 06 时 04 分 05 秒 session 0x66af3690a390cc4 cxid 0x3 zxid 0x6305208d44 closeSession null</span><br><span class="line">19-9-20 下午 06 时 04 分 05 秒 session 0x76af0f5294b008c cxid 0x6c3549 zxid 0x6305208d45 setData &apos;/storm/supervisors/a7d11d05-8b97-4bc5-ad43-f43cd28f98cc,#ffffffacffffffed05737202b6261636b747970652e73746f726d2e6461656d6f6e2e636... 太长省略，2025456</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200212010948.png" alt="事务日志内容查看" title="事务日志内容查看"></p><p>第一行：<code>ZooKeeper Transactional Log File with dbid 0 txnlog format version 2</code>，这是每个事务日志文件都有的日志头，输出了 <code>dbid</code> 还有 <code>version</code>。</p><p>第二行：<code>... session 0x46d0bf6e8cc50b8 cxid 0x1 zxid 0x6305208d3f error -101</code>，这也就是具体的事务日志内容了，这里是说某一时刻有一个 <code>sessionid</code> 为 <code>0x46d0bf6e8cc50b8</code>，<code>cxid</code> 为 <code>0x1</code>，<code>zxid</code> 为 <code>0x6305208d3f</code> 的请求，但是出错了。</p><p>继续看第五行【第三行是空白】：<code>... session 0x46d0bf6e8cc50b8 cxid 0x2 zxid 0x6305208d41 closeSession null</code>，还是第二行那个 <code>sessionid</code>，请求类型为 <code>closeSession</code>，表示关闭了会话。</p><p>看第七行：<code>... session 0x66af3690a390cc4 cxid 0x0 zxid 0x6305208d43 createSession 4000</code>，这个请求是 <code>createSession</code> 类型，表示创建会话，超时时间为 4000 毫秒。</p><p>直接看第十行：<code>... session 0x76af0f5294b008c cxid 0x6c3549 zxid 0x6305208d45 setData ...</code>，请求类型是 <code>setData</code>，表示写入数据，数据内容是经过 <code>ASCII</code> 编码的。</p><h1 id="快照日志"><a href="# 快照日志" class="headerlink" title="快照日志"></a>快照日志 </h1><p><code>Zookeeper</code> 的数据在内存中是以树形结构进行存储的，而快照就是每隔一段时间会把整个 <code>DataTree</code> 的数据序列化后，存储在磁盘中，这就是 <code>Zookeeper</code> 的快照日志。</p><p><code>Zookeeper</code> 快照日志的存储路径同样可以在 <code>zoo.cfg</code> 中查看，如上文所示，访问 <code>dataDir</code> 对应的路径就可以看到 <code>version-2</code> 文件夹。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dataDir=/cloud/data1/hadoop/zookeeper</span><br></pre></td></tr></table></figure><p><code>Zookeeper</code> 快照日志文件的命名规则为 <code>snapshot.**</code>，其中 <code>**</code> 表示 <code>Zookeeper</code> 触发快照的那个瞬间，提交的最后一个事务的 <code>ID</code>，类似于前面的事务日志文件命名规则，文件示例：<code>snapshot.6501d41a74</code>。</p><h2 id="快照日志可视化"><a href="# 快照日志可视化" class="headerlink" title="快照日志可视化"></a> 快照日志可视化 </h2><p> 与事务日志文件一样，快照日志文件不可直接查看，需要通过 <code>Zookeeper</code> 为快照日志文件提供的可视化工具转换，对应的类为 <code>org.apache.zookeeper.server</code> 包中的 <code>SnapshotFormatter</code>，接下来就使用该工具转换该事务日志文件，并举例解释部分内容。</p><p>执行命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -classpath .:./slf4j-api-1.6.1.jar:./zookeeper-3.4.6.2.2.6.0-2800.jar org.apache.zookeeper.server.SnapshotFormatter ./snapshot.6501d41a74 &gt; 6501d41a74.snapshot.log</span><br></pre></td></tr></table></figure><p>把快照文件转换为普通的文本文件，结果输出到 <code>6501d41a74.snapshot.log</code> 文件中。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200212215807.png" alt="快照日志文件转换" title="快照日志文件转换"></p><h2 id="快照日志分析"><a href="# 快照日志分析" class="headerlink" title="快照日志分析"></a>快照日志分析 </h2><p> 由于快照日志文件的内容比较长，把 <code>Zookeeper</code> 的所有节点内容全部输出，主要是因为 <code>Zookeeper</code> 的目录比较多，所以只需要挑两个出来即可分析，其它目录都是类似的内容。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">ZNode Details (count=26650):</span><br><span class="line">----</span><br><span class="line">/</span><br><span class="line">  cZxid = 0x00000000000000</span><br><span class="line">  ctime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">  mZxid = 0x00000000000000</span><br><span class="line">  mtime = Thu Jan 01 08:00:00 CST 1970</span><br><span class="line">  pZxid = 0x00006401bcbf78</span><br><span class="line">  cversion = 75</span><br><span class="line">  dataVersion = 0</span><br><span class="line">  aclVersion = 0</span><br><span class="line">  ephemeralOwner = 0x00000000000000</span><br><span class="line">  dataLength = 0</span><br><span class="line">----</span><br><span class="line">/prc</span><br><span class="line">  cZxid = 0x000063029f34a0</span><br><span class="line">  ctime = Sat Jul 27 14:14:01 CST 2019</span><br><span class="line">  mZxid = 0x000063029f34a0</span><br><span class="line">  mtime = Sat Jul 27 14:14:01 CST 2019</span><br><span class="line">  pZxid = 0x000063029f34a1</span><br><span class="line">  cversion = 1</span><br><span class="line">  dataVersion = 0</span><br><span class="line">  aclVersion = 0</span><br><span class="line">  ephemeralOwner = 0x00000000000000</span><br><span class="line">  no data</span><br><span class="line">----</span><br><span class="line">... 省略很多内容 </span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200212220328.png" alt="快照日志内容查看" title="快照日志内容查看"></p><p>看到第一行：<code>ZNode Details (count=26650):</code>，表示 <code>ZNode</code> 节点的个数，我这里有 26650 个，太多了。</p><p>从 <code>----</code> 开始到下一个 <code>----</code> 结束，就是一个 <code>ZNode</code> 的信息，包含路径、创建时间、数据长度等等，下面简单说明【以上面的 <code>/prc</code> 为例】：</p><ul><li><code>/prc</code>，路径 </li><li><code>cZxid</code>，创建节点时的 <code>Zxid</code></li><li><code>ctime</code>，创建节点的时间</li><li><code>mZxid</code>，节点最近一次更新对应的 <code>Zxid</code></li><li><code>mtime</code>，节点最近一次更新的时间</li><li><code>pZxid</code>，父节点的 <code>Zxid</code></li><li><code>cversion</code>，子节点更新次数</li><li><code>dataVersion</code>，数据更新次数</li><li><code>aclVersion</code>，节点 <code>acl</code> 更新次数</li><li><code>ephemeralOwner</code>，如果节点为 <code>ephemeral</code> 节点则该值为 <code>sessionid</code>，否则为 0</li><li><code>no data</code>，表示没有数据，如果有的话会显示 <code>dataLength = xx</code>，即数据长度为 <code>xx</code></li></ul><p> 在快照文件的末尾，会有很多如下格式的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Session Details (sid, timeout, ephemeralCount):</span><br><span class="line">0x56efad063889c37, 60000, 0</span><br><span class="line">0x1703047aa123b27, 60000, 1</span><br><span class="line">0x4701974cffc62fa, 90000, 0</span><br><span class="line">0x56efad063885c32, 90000, 0</span><br><span class="line">0x4701974cffc62fc, 90000, 0</span><br><span class="line">0x56efad063885c33, 90000, 0</span><br><span class="line">0x66f1c157df79788, 10000, 0</span><br><span class="line">0x4701974cffc62fe, 90000, 0</span><br><span class="line">0x76efad06454f9ed, 90000, 0</span><br><span class="line">0x4701974cffc62ff, 90000, 0</span><br><span class="line">... 省略很多个 </span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200212223216.png" alt="快照日志文件末尾" title="快照日志文件末尾"></p><p>这里表达的是当前抓取快照日志文件的时间，<code>Zookeeper</code> 中 <code>session</code> 的详情，第一个 <code>session</code> 的超时时间是 60000 毫秒，<code>ephemeral</code> 节点为 0；第二个 <code>session</code> 的超时时间是 60000 毫秒，<code>ephemeral</code> 节点为 1。</p><h1 id="清理机制"><a href="# 清理机制" class="headerlink" title="清理机制"></a>清理机制 </h1><p> 有两个参数，从不同维度考虑，配置在 <code>zoo.cfg</code> 文件中，实现日志文件的清理。</p><ul><li><code>autopurge.purgeInterval=24</code>，在 <code>v3.4.0</code> 及之后的版本，<code>Zookeeper</code> 提供了自动清理事务日志文件和快照日志文件的功能，这个参数指定了清理频率，单位是小时，需要配置一个 1 或更大的整数。默认是 0，表示不开启自动清理功能</li><li><code>autopurge.snapRetainCount=30</code>，参数指定了需要保留的事务日志文件和快照日志文件的数目，默认是保留 3 个，和 <code>autopurge.purgeInterval</code> 搭配使用</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;&lt;code&gt;Zookeeper&lt;/code&gt; 是大数据组件中不可或缺的一种组件，是一个开源的分布式协调服务，提供了诸如统一命名服务、配置管理、集群管理等功能，在很多场景中都可以见到它的身影。本文简单介绍 &lt;code&gt;Zookeeper&lt;/code&gt; 的日志查看，开发环境基于 &lt;code&gt;v3.4.6&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Zookeeper" scheme="https://www.playpi.org/tags/Zookeeper/"/>
    
      <category term="log" scheme="https://www.playpi.org/tags/log/"/>
    
  </entry>
  
  <entry>
    <title>HBase 异常：RpcRetryingCaller-Call exception</title>
    <link href="https://www.playpi.org/2019091701.html"/>
    <id>https://www.playpi.org/2019091701.html</id>
    <published>2019-09-17T13:23:49.000Z</published>
    <updated>2019-09-17T14:23:49.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --><p>今天天气不错，我心情很好，但是在测试代码功能的时候遇到了一个问题，浪费了一些时间，感到惋惜。还好，最终解决了问题，只是集群环境的问题。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 我的本意是想写一个 <code>MapReduce</code> 程序来扫描 <code>HBase</code> 数据，统计一些信息，但是在测试的时候发现程序卡住了，等了几分钟之后开始出现连续的超时日志，我感觉是连接 <code>HBase</code> 超时，无法读取表的元信息。</p><p><code>MapReduce</code> 扫描数据报错，报错信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">18:06:43.022 [main] INFO  o.a.h.h.util.RegionSizeCalculator - Calculating region sizes for table &quot;YOUR_TABLE_NAME&quot;.</span><br><span class="line">18:07:52.298 [htable-pool2-t1] INFO  o.a.h.hbase.client.RpcRetryingCaller - Call exception, tries=10, retries=35, started=69237 ms ago, cancelled=false, msg=row &apos;YOUR_TABLE_NAME,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br><span class="line">18:08:12.336 [htable-pool2-t1] INFO  o.a.h.hbase.client.RpcRetryingCaller - Call exception, tries=11, retries=35, started=89279 ms ago, cancelled=false, msg=row &apos;YOUR_TABLE_NAME,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br><span class="line">18:08:32.358 [htable-pool2-t1] INFO  o.a.h.hbase.client.RpcRetryingCaller - Call exception, tries=12, retries=35, started=109301 ms ago, cancelled=false, msg=row &apos;YOUR_TABLE_NAME,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br><span class="line">18:08:52.522 [htable-pool2-t1] INFO  o.a.h.hbase.client.RpcRetryingCaller - Call exception, tries=13, retries=35, started=129465 ms ago, cancelled=false, msg=row &apos;YOUR_TABLE_NAME,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br><span class="line">18:09:12.560 [htable-pool2-t1] INFO  o.a.h.hbase.client.RpcRetryingCaller - Call exception, tries=14, retries=35, started=149503 ms ago, cancelled=false, msg=row &apos;YOUR_TABLE_NAME,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190917203629.png" alt="MapReduce 报错信息" title="MapReduce 报错信息"></p><p>此时我又去检查正在运行的 <code>Spark</code> 程序写入数据有没有问题，发现也是在写入 <code>HBase</code> 时有同样的错误，报错日志如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">19/09/17 18:22:49 INFO RpcRetryingCaller: Call exception, tries=10, retries=35, started=68431 ms ago, cancelled=false, msg=row &apos;YOUR_TABLE_NAME,14df3e2b1626e6e02fc1d772eb34f8ad,99999999999999&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br><span class="line">19/09/17 18:23:09 INFO RpcRetryingCaller: Call exception, tries=11, retries=35, started=88446 ms ago, cancelled=false, msg=row &apos;YOUR_TABLE_NAME,14df3e2b1626e6e02fc1d772eb34f8ad,99999999999999&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br><span class="line">19/09/17 18:23:29 INFO RpcRetryingCaller: Call exception, tries=12, retries=35, started=108564 ms ago, cancelled=false, msg=row &apos;YOUR_TABLE_NAME,14df3e2b1626e6e02fc1d772eb34f8ad,99999999999999&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br><span class="line">19/09/17 18:23:49 INFO RpcRetryingCaller: Call exception, tries=13, retries=35, started=128578 ms ago, cancelled=false, msg=row &apos;YOUR_TABLE_NAME,14df3e2b1626e6e02fc1d772eb34f8ad,99999999999999&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br><span class="line">19/09/17 18:24:09 INFO RpcRetryingCaller: Call exception, tries=14, retries=35, started=148657 ms ago, cancelled=false, msg=row &apos;YOUR_TABLE_NAME,14df3e2b1626e6e02fc1d772eb34f8ad,99999999999999&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190917203619.png" alt="Spark 报错信息" title="Spark 报错信息"></p><p>接着我想使用 <code>HBase</code> 自带的 <code>RowCounter</code> 执行 <code>MapReduce</code> 任务扫描数据，测试一下，使用命令：<br><code>hbase org.apache.hadoop.hbase.mapreduce.RowCounter &#39;YOUR_TABLE_NAME&#39;</code>。</p><p>结果也是超时报错，报错信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed after attempts=36, exceptions:</span><br><span class="line">Tue Sep 17 18:10:02 CST 2019, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=68315: row &apos;YOUR_TABLE_NAME,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br><span class="line"></span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.throwEnrichedException (RpcRetryingCallerWithReadReplicas.java:271)</span><br><span class="line">at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call (ScannerCallableWithReplicas.java:203)</span><br><span class="line">at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call (ScannerCallableWithReplicas.java:60)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries (RpcRetryingCaller.java:200)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientScanner.call (ClientScanner.java:320)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientScanner.nextScanner (ClientScanner.java:295)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientScanner.initializeScannerInConstruction (ClientScanner.java:160)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientScanner.&lt;init&gt;(ClientScanner.java:155)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.getScanner (HTable.java:821)</span><br><span class="line">at org.apache.hadoop.hbase.client.MetaScanner.metaScan (MetaScanner.java:193)</span><br><span class="line">at org.apache.hadoop.hbase.client.MetaScanner.metaScan (MetaScanner.java:89)</span><br><span class="line">at org.apache.hadoop.hbase.client.MetaScanner.allTableRegions (MetaScanner.java:324)</span><br><span class="line">at org.apache.hadoop.hbase.client.HRegionLocator.getAllRegionLocations (HRegionLocator.java:88)</span><br><span class="line">at org.apache.hadoop.hbase.util.RegionSizeCalculator.init (RegionSizeCalculator.java:94)</span><br><span class="line">at org.apache.hadoop.hbase.util.RegionSizeCalculator.&lt;init&gt;(RegionSizeCalculator.java:81)</span><br><span class="line">at org.apache.hadoop.hbase.mapreduce.TableInputFormatBase.getSplits (TableInputFormatBase.java:256)</span><br><span class="line">at org.apache.hadoop.hbase.mapreduce.TableInputFormat.getSplits (TableInputFormat.java:237)</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits (JobSubmitter.java:301)</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits (JobSubmitter.java:318)</span><br><span class="line">at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal (JobSubmitter.java:196)</span><br><span class="line">at org.apache.hadoop.mapreduce.Job$10.run (Job.java:1290)</span><br><span class="line">at org.apache.hadoop.mapreduce.Job$10.run (Job.java:1287)</span><br><span class="line">at java.security.AccessController.doPrivileged (Native Method)</span><br><span class="line">at javax.security.auth.Subject.doAs (Subject.java:422)</span><br><span class="line">at org.apache.hadoop.security.UserGroupInformation.doAs (UserGroupInformation.java:1709)</span><br><span class="line">at org.apache.hadoop.mapreduce.Job.submit (Job.java:1287)</span><br><span class="line">at org.apache.hadoop.mapreduce.Job.waitForCompletion (Job.java:1308)</span><br><span class="line">at org.apache.hadoop.hbase.mapreduce.RowCounter.main (RowCounter.java:210)</span><br><span class="line">Caused by: java.net.SocketTimeoutException: callTimeout=60000, callDuration=68315: row &apos;YOUR_TABLE_NAME,,00000000000000&apos; on table &apos;hbase:meta&apos; at region=hbase:meta,,1.1588230740, hostname=dev6,16020,1565930591664, seqNum=0</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries (RpcRetryingCaller.java:159)</span><br><span class="line">at org.apache.hadoop.hbase.client.ResultBoundedCompletionService$QueueingFuture.run (ResultBoundedCompletionService.java:65)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">Caused by: java.net.ConnectException: Connection refused</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190917203602.png" alt="RowCounter 报错信息" title="RowCounter 报错信息"></p><p>由此可以断定，<code>HBase</code> 集群有问题了。</p><h1 id="问题分析解决"><a href="# 问题分析解决" class="headerlink" title="问题分析解决"></a>问题分析解决 </h1><p> 首先查看集群的服务是不是还正常，一看果然不正常，<code>RegionServer</code> 已经挂了，那就好办了，直接重启即可。</p><p>由于是在测试环境，平时不太关注，所以没有注意到服务已经挂了，让运维人员花了 1 分钟帮忙重启一下，确认后没有问题。</p><p>最后打开 <code>RegionServer</code> 管理界面，查看集群信息，恢复正常。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190917203513.png" alt="RegionServer 状态正常" title="RegionServer 状态正常"></p><p>我的 <code>MapReduce</code> 任务又欢快地跑起来了。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190917203533.png" alt="MapReduce 正常执行" title="MapReduce 正常执行"></p><p>再试了一下 <code>RowCounter</code> 任务，也可以正常执行。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190917203542.png" alt="RowCounter 正常执行" title="RowCounter 正常执行"></p><p>至此，这个小问题解决。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 在搜索资料的过程中，也有人说是以下原因，但是我这里不是这个原因，所以记录下来，仅供读者参考：</p><ul><li>添加 <code>jar</code> 包，<code>com.yammer.metrics</code> -&gt; <code>metrics-core</code></li><li><code>hosts</code> 添加 <code>ip</code> 映射</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:04 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;今天天气不错，我心情很好，但是在测试代码功能的时候遇到了一个问题，浪费了一些时间，感到惋惜。还好，最终解决了问题，只是集群环境的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="RegionServer" scheme="https://www.playpi.org/tags/RegionServer/"/>
    
      <category term="RowCounter" scheme="https://www.playpi.org/tags/RowCounter/"/>
    
  </entry>
  
  <entry>
    <title>CentOS7 自动安装 Shadowsocks 脚本</title>
    <link href="https://www.playpi.org/2019082801.html"/>
    <id>https://www.playpi.org/2019082801.html</id>
    <published>2019-08-28T15:48:21.000Z</published>
    <updated>2019-09-19T15:48:21.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --><p>以前我整理过一篇博客，详细叙述了如何自己搭建梯子，图文并茂，可以参见：<a href="https://www.playpi.org/2018111601.html">使用 Vultr 搭建 Shadowsocks（VPS 搭建 SS）</a> 。里面有涉及到购买一台云服务器后该如何操作：初始化环境、安装 <code>Shadowsocks</code>、配置参数、安装防火墙、启动服务、检查服务状态等等步骤。</p><p>虽然过程很详细，只要几个命令就可以完成 <code>Shadowsocks</code> 服务的搭建，但是对于没有技术基础又不想折腾的读者来说，还是有点困难。所以我把安装过程整理成一个自动化的 <code>Shell</code> 脚本，读者下载下来之后，直接运行即可，在运行过程中如果需要询问交互，例如填写密码、端口号等，读者直接填写即可，或者直接使用默认的设置。</p><a id="more"></a><p>首先说明，使用这个自动化 <code>Shell</code> 脚本，零基础的读者也可以自行安装 <code>Shadowsocks</code>，整个安装过程不到五分钟，非常友好而高效，运行脚本后慢慢等待即可，当然别忘记填写必要信息。</p><p>本脚本已经被我上传至 <code>GitHub</code>，读者可以下载查看并使用：<a href="https://github.com/iplaypi/iplaypistudy/tree/master/iplaypistudy-normal/src/bin/20190828" target="_blank" rel="noopener">auto_deploy_shadowsocks.sh</a> ，需要注意的是，这个自动化 <code>Shell</code> 脚本只针对 <code>CentOS 7x64</code> 操作系统有效，其它操作系统我没有测试，不保证能用。所以为了稳妥起见，请读者还是参考我上面给出的那篇博客来创建云主机。</p><h1 id="自动化安装"><a href="# 自动化安装" class="headerlink" title="自动化安装"></a>自动化安装 </h1><p> 下载 <code>GitHub</code> 上面的脚本时，如果有类似 <code>Shell</code> 的环境，就不用浏览器下载了，在 <code>Shell</code> 中可以直接使用 <code>wget</code> 命令下载，使用如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/iplaypi/iplaypistudy/master/iplaypistudy-normal/src/bin/20190828/auto_deploy_shadowsocks.sh</span><br></pre></td></tr></table></figure><p>下载下来后接着直接运行即可，使用 <code>sh auto_deploy_shadowsocks.sh</code> 。</p><p>下面简单描述自动化脚本的思路：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1、提示用户输入端口号、密码，并读取输入，没有输入则使用默认值 </span><br><span class="line">2、利用端口号、密码，生成 /etc/shadowsocks.json 配置文件 </span><br><span class="line">3、安装 shadowsocks 以及其它组件：m2crypto、pip、firewalld</span><br><span class="line">4、启动防火墙，开启必要的端口 </span><br><span class="line">5、检测当前是否有运行的 shadowsocks 服务，有则杀死 </span><br><span class="line">6、后台启动 shadowsocks 服务 </span><br><span class="line">7、输出部署成功的信息，如果部署失败，需要进一步查看日志文件 </span><br><span class="line">8、处理 server 酱通知 </span><br></pre></td></tr></table></figure><p>脚本内容整理如下，重要的地方已经注释清楚【这里要特别注意脚本中的换行符号，一律使用 <code>\\n</code> 的形式，否则会引起错误】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"># 注意本脚本中的换行符号，一律使用 \n 的形式，否则会引起错误 </span><br><span class="line"># 日志路径，如果安装失败需要查看日志，是否有异常 / 报错信息 </span><br><span class="line">export log_path=/etc/auto_deploy_shadowsocks.log</span><br><span class="line"># 设置端口号，从键盘接收参数输入，默认为 2018,-e 参数转义开启高亮显示 </span><br><span class="line">echo -n -e &apos;\033 [36mPlease enter PORT [2018 default]:\033 [0m&apos;</span><br><span class="line">read port</span><br><span class="line">if [! -n &quot;$port&quot;];then</span><br><span class="line">    echo &quot;port will be set to 2018&quot;</span><br><span class="line">    port=2018</span><br><span class="line">else</span><br><span class="line">    echo &quot;port will be set to $port&quot;</span><br><span class="line">fi</span><br><span class="line"># 设置密码，从键盘接收参数输入，默认为 pengfeivpn,-e 参数转义开启高亮显示 </span><br><span class="line">echo -n -e &apos;\033 [36mPlease enter PASSWORD [pengfeivpn default]:\033 [0m&apos;</span><br><span class="line">read pwd</span><br><span class="line">if [! -n &quot;$pwd&quot;];then</span><br><span class="line">    echo &quot;password will be set to 123456&quot;</span><br><span class="line">    pwd=pengfeivpn</span><br><span class="line">else</span><br><span class="line">    echo &quot;password will be set to $pwd&quot;</span><br><span class="line">fi</span><br><span class="line"># 创建 shadowsocks.json 配置文件，只开一个端口，server 可以是 0.0.0.0</span><br><span class="line">echo &quot;****************start generate /etc/shadowsocks.json&quot;</span><br><span class="line">cat&gt;/etc/shadowsocks.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;server&quot;:&quot;0.0.0.0&quot;,</span><br><span class="line">    &quot;server_port&quot;:$port,</span><br><span class="line">    &quot;local_address&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;local_port&quot;:1080,</span><br><span class="line">    &quot;password&quot;:&quot;$pwd&quot;,</span><br><span class="line">    &quot;timeout&quot;:300,</span><br><span class="line">    &quot;method&quot;:&quot;aes-256-cfb&quot;,</span><br><span class="line">    &quot;fast_open&quot;: false</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">echo &quot;****************start install shadowsocks and other tools&quot;</span><br><span class="line"># 安装 shadowsocks / 防火墙，携带 - y 参数表示自动同意安装，无需交互询问 </span><br><span class="line"># 日志全部输出到上面指定的日志文件中 </span><br><span class="line">echo &quot;&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">echo &quot;********************************&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">echo &quot;start deploy shadowsocks,date is:&quot;$(date +% Y-% m-% d-% X) &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">echo &quot;********************************&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">echo &quot;&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">echo &quot;******************start install m2crypto&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">ret=`yum install -y m2crypto python-setuptools &gt;&gt; $&#123;log_path&#125; 2&gt;&amp;1`</span><br><span class="line">echo &quot;&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">echo &quot;******************start install pip&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">ret=`easy_install pip &gt;&gt; $&#123;log_path&#125; 2&gt;&amp;1`</span><br><span class="line">echo &quot;&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">echo &quot;******************start install shadowsocks&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">ret=`pip install shadowsocks &gt;&gt; $&#123;log_path&#125; 2&gt;&amp;1`</span><br><span class="line">echo &quot;&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">echo &quot;******************start install firewalld&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">ret=`yum install -y firewalld &gt;&gt; $&#123;log_path&#125; 2&gt;&amp;1`</span><br><span class="line">echo &quot;&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">echo &quot;******************start start firewalld&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">ret=`systemctl start firewalld &gt;&gt; $&#123;log_path&#125; 2&gt;&amp;1`</span><br><span class="line">echo &quot;&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line">echo &quot;******************start reload firewall&quot; &gt;&gt; $&#123;log_path&#125;</span><br><span class="line"># 开启端口 </span><br><span class="line">ret=`firewall-cmd --permanent --zone=public --add-port=22/tcp &gt;&gt; $&#123;log_path&#125; 2&gt;&amp;1`</span><br><span class="line">ret=`firewall-cmd --permanent --zone=public --add-port=$port/tcp &gt;&gt; $&#123;log_path&#125; 2&gt;&amp;1`</span><br><span class="line">ret=`firewall-cmd --reload &gt;&gt; $&#123;log_path&#125; 2&gt;&amp;1`</span><br><span class="line">echo &quot;****************start check shadowsocks&quot;</span><br><span class="line"># 如果有相同功能的进程则先杀死，$? 表示上个命令的退出状态，或者函数的返回值 </span><br><span class="line">ps -ef | grep ssserver | grep shadowsocks | grep -v grep</span><br><span class="line">if [$? -eq 0];then</span><br><span class="line">    ps -ef | grep ssserver | grep shadowsocks | awk &apos;&#123; print $2 &#125;&apos; | xargs kill -9</span><br><span class="line">fi</span><br><span class="line"># 后台启动，-d 表示守护进程 </span><br><span class="line">/usr/bin/ssserver -c /etc/shadowsocks.json -d start</span><br><span class="line"># 启动成功 </span><br><span class="line">if [$? -eq 0];then</span><br><span class="line"># 获取本机 ip 地址 </span><br><span class="line">ip=`ip addr | grep &apos;state UP&apos; -A2 | tail -n1 | awk &apos;&#123;print $2&#125;&apos; | cut -f1 -d &apos;/&apos;`</span><br><span class="line">clear</span><br><span class="line">cat&lt;&lt;EOF</span><br><span class="line">***************Congratulation!*****************</span><br><span class="line">shadowsocks deployed successfully!</span><br><span class="line"></span><br><span class="line">IP:$ip</span><br><span class="line">PORT:$port</span><br><span class="line">PASSWORD:$pwd</span><br><span class="line">METHOD:aes-256-cfb</span><br><span class="line"></span><br><span class="line">*****************JUST ENJOY IT!****************</span><br><span class="line">EOF</span><br><span class="line"># 建议开启 server 酱自动通知，推送到微信，就可以直接复制信息转发给别人了 </span><br><span class="line"># 不开启请把以下内容注释掉，注释内容持续到 & apos;server 酱通知完成 & apos;</span><br><span class="line"># 关于 server 酱的使用请参考:https://sc.ftqq.com</span><br><span class="line"># 注意 server_key 不要泄露，泄漏后可以去官网重置 </span><br><span class="line">echo &quot;************** 开始处理 server 酱通知 & quot;</span><br><span class="line">server_key=SCU60861T303e1c479df6cea9e95fc54d210232565d7dbbf075750</span><br><span class="line"># 传输 2 个参数:text/desp,desp 使用 markdown 语法 (注意换行符要使用 2 个换行)</span><br><span class="line">cat&gt;./shadowsocks_msg.txt&lt;&lt;EOF</span><br><span class="line">text=shadowsocks 服务部署启动完成 </span><br><span class="line">&amp;desp=</span><br><span class="line">- IP 地址：$ip</span><br><span class="line"></span><br><span class="line">- 端口号：$port</span><br><span class="line"></span><br><span class="line">- 密码：$pwd</span><br><span class="line"></span><br><span class="line">- 加密方式：aes-256-cfb</span><br><span class="line">EOF</span><br><span class="line">curl -X POST --data-binary @./shadowsocks_msg.txt  https://sc.ftqq.com/$server_key.send</span><br><span class="line">echo &quot;&quot;</span><br><span class="line">echo &quot;**************server 酱通知处理完成 & quot;</span><br><span class="line"># 失败 </span><br><span class="line">else</span><br><span class="line">clear</span><br><span class="line">cat&lt;&lt;EOF</span><br><span class="line">**************Failed,retry please!*************</span><br><span class="line"></span><br><span class="line">cat /etc/ss.log to get something you need.</span><br><span class="line"></span><br><span class="line">**************Failed,retry please!*************</span><br><span class="line">EOF</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本的输出信息如下【我手动设置端口号为 2019，密码使用默认值】，表示安装完成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">[root@playpi ~]# sh auto_deploy_shadowsocks.sh </span><br><span class="line">Please enter PORT [2018 default]:2019</span><br><span class="line">port will be set to 2019</span><br><span class="line">Please enter PASSWORD [pengfeivpn default]:</span><br><span class="line">password will be set to 123456</span><br><span class="line">****************start generate /etc/shadowsocks.json</span><br><span class="line">****************start install shadowsocks and other tools</span><br><span class="line">****************start check shadowsocks</span><br><span class="line">root     13980     1  0 11:07 ?        00:00:00 /usr/bin/python/usr/bin/ssserver -c /etc/shadowsocks.json -d start</span><br><span class="line">INFO: loading config from /etc/shadowsocks.json</span><br><span class="line">2019-09-29 11:09:29 INFO     loading libcrypto from libcrypto.so.10</span><br><span class="line">started</span><br><span class="line">***************Congratulation!*****************</span><br><span class="line">shadowsocks deployed successfully!</span><br><span class="line"></span><br><span class="line">IP:45.32.79.20</span><br><span class="line">PORT:2019</span><br><span class="line">PASSWORD:pengfeivpn</span><br><span class="line">METHOD:aes-256-cfb</span><br><span class="line"></span><br><span class="line">*****************JUST ENJOY IT!****************</span><br><span class="line">************** 开始处理 server 酱通知 </span><br><span class="line">&#123;&quot;errno&quot;:0,&quot;errmsg&quot;:&quot;success&quot;,&quot;dataset&quot;:&quot;done&quot;&#125;</span><br><span class="line">**************server 酱通知完成 </span><br><span class="line">[root@playpi ~]#</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190929211341.png" alt="自动安装成功" title="自动安装成功"></p><p>同时，<code>server</code> 酱也接收到通知，可以很方便地直接转发给需要的人了。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190929211352.png" alt="server 酱的通知" title="server 酱的通知"></p><h1 id="自动更换端口重启"><a href="# 自动更换端口重启" class="headerlink" title="自动更换端口重启"></a>自动更换端口重启 </h1><p> 在使用 <code>Shadowsocks</code> 的时候，有时候会遇到一个问题，端口被封了【<code>ip</code> 被封另外说，只能销毁主机新建】，特别是国家严厉管控非法 <code>VPN</code> 的时候，当然我这是属于误封，因为我只是用来学习、测试接口，这时候解决办法也简单，尝试更换一个端口即可。</p><p>步骤其实很简单，停止服务、更改配置文件、开启新端口、重启服务，但是作为一个追求效率的人，我还是想把操作简化一下，最好敲下一行命令等着就行【执行脚本的前提是 <code>Shadowsocks</code> 以及相关工具已经安装完成】。</p><p>其实把前面的步骤稍微整理一下，就变成了一个简单的脚本，直接执行即可。脚本已经被我上传至 <code>GitHub</code>，在 <code>Shell</code> 中可以直接使用 <code>wget</code> 命令下载，使用如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/iplaypi/iplaypistudy/master/iplaypistudy-normal/src/bin/20190828/auto_restart_shadowsocks.sh</span><br></pre></td></tr></table></figure><p>下载下来后接着直接运行即可，使用 <code>sh auto_restart_shadowsocks.sh</code> 。</p><p>下面简单描述自动化脚本的思路：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">1、提示用户输入端口号、密码，并读取输入，没有输入则使用默认值 </span><br><span class="line">2、利用端口号、密码，生成 /etc/shadowsocks.json 配置文件 </span><br><span class="line">3、启动防火墙，开启必要的端口（`Shadowsocks` 以及相关工具无需再安装）</span><br><span class="line">4、使用 stop 停止 shadowsocks 服务 </span><br><span class="line">5、再次检测当前是否有运行的 shadowsocks 服务，有则杀死 </span><br><span class="line">6、后台启动 shadowsocks 服务 </span><br><span class="line">7、输出部署成功的信息，如果部署失败，需要进一步查看日志文件 </span><br><span class="line">8、处理 server 酱通知 </span><br></pre></td></tr></table></figure><p>脚本内容整理如下，重要的地方已经注释清楚【这里要特别注意脚本中的换行符号，一律使用 <code>\\n</code> 的形式，否则会引起错误】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"># 注意本脚本中的换行符号，一律使用 \n 的形式，否则会引起错误 </span><br><span class="line"># 日志路径，如果安装失败需要查看日志，是否有异常 / 报错信息 </span><br><span class="line">export log_path=/etc/auto_restart_shadowsocks.log</span><br><span class="line"># 设置端口号，从键盘接收参数输入，默认为 2018,-e 参数转义开启高亮显示 </span><br><span class="line">echo -n -e &apos;\033 [36mPlease enter PORT [2018 default]:\033 [0m&apos;</span><br><span class="line">read port</span><br><span class="line">if [! -n &quot;$port&quot;];then</span><br><span class="line">    echo &quot;port will be set to 2018&quot;</span><br><span class="line">    port=2018</span><br><span class="line">else</span><br><span class="line">    echo &quot;port will be set to $port&quot;</span><br><span class="line">fi</span><br><span class="line"># 设置密码，从键盘接收参数输入，默认为 pengfeivpn,-e 参数转义开启高亮显示 </span><br><span class="line">echo -n -e &apos;\033 [36mPlease enter PASSWORD [pengfeivpn default]:\033 [0m&apos;</span><br><span class="line">read pwd</span><br><span class="line">if [! -n &quot;$pwd&quot;];then</span><br><span class="line">    echo &quot;password will be set to pengfeivpn&quot;</span><br><span class="line">    pwd=pengfeivpn</span><br><span class="line">else</span><br><span class="line">    echo &quot;password will be set to $pwd&quot;</span><br><span class="line">fi</span><br><span class="line"># 创建 shadowsocks.json 配置文件，只开一个端口，server 可以是 0.0.0.0</span><br><span class="line">echo &quot;****************start generate /etc/shadowsocks.json&quot;</span><br><span class="line">cat&gt;/etc/shadowsocks.json&lt;&lt;EOF</span><br><span class="line">&#123;</span><br><span class="line">    &quot;server&quot;:&quot;0.0.0.0&quot;,</span><br><span class="line">    &quot;server_port&quot;:$port,</span><br><span class="line">    &quot;local_address&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">    &quot;local_port&quot;:1080,</span><br><span class="line">    &quot;password&quot;:&quot;$pwd&quot;,</span><br><span class="line">    &quot;timeout&quot;:300,</span><br><span class="line">    &quot;method&quot;:&quot;aes-256-cfb&quot;,</span><br><span class="line">    &quot;fast_open&quot;: false</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">echo &quot;****************start open port&quot;</span><br><span class="line"># 开启端口 </span><br><span class="line">ret=`firewall-cmd --permanent --zone=public --add-port=$port/tcp &gt;&gt; $&#123;log_path&#125; 2&gt;&amp;1`</span><br><span class="line">ret=`firewall-cmd --reload &gt;&gt; $&#123;log_path&#125; 2&gt;&amp;1`</span><br><span class="line"># 正常停掉 shadowsocks 服务 </span><br><span class="line">echo &quot;****************start stop shadowsocks&quot;</span><br><span class="line">/usr/bin/ssserver -c /etc/shadowsocks.json -d stop</span><br><span class="line">echo &quot;****************start check shadowsocks&quot;</span><br><span class="line"># 如果有相同功能的进程则先杀死，$? 表示上个命令的退出状态，或者函数的返回值 </span><br><span class="line">ps -ef | grep ssserver | grep shadowsocks | grep -v grep</span><br><span class="line">if [$? -eq 0];then</span><br><span class="line">    ps -ef | grep ssserver | grep shadowsocks | awk &apos;&#123; print $2 &#125;&apos; | xargs kill -9</span><br><span class="line">fi</span><br><span class="line"># 后台启动，-d 表示守护进程 </span><br><span class="line">/usr/bin/ssserver -c /etc/shadowsocks.json -d start</span><br><span class="line"># 启动成功 </span><br><span class="line">if [$? -eq 0];then</span><br><span class="line"># 获取本机 ip 地址 </span><br><span class="line">ip=`ip addr | grep &apos;state UP&apos; -A2 | tail -n1 | awk &apos;&#123;print $2&#125;&apos; | cut -f1 -d &apos;/&apos;`</span><br><span class="line">clear</span><br><span class="line">cat&lt;&lt;EOF</span><br><span class="line">***************Congratulation!*****************</span><br><span class="line">shadowsocks restart successfully!</span><br><span class="line"></span><br><span class="line">IP:$ip</span><br><span class="line">PORT:$port</span><br><span class="line">PASSWORD:$pwd</span><br><span class="line">METHOD:aes-256-cfb</span><br><span class="line"></span><br><span class="line">*****************JUST ENJOY IT!****************</span><br><span class="line">EOF</span><br><span class="line"># 建议开启 server 酱自动通知，推送到微信，就可以直接复制信息转发给别人了 </span><br><span class="line"># 不开启请把以下内容注释掉，注释内容持续到 & apos;server 酱通知完成 & apos;</span><br><span class="line"># 关于 server 酱的使用请参考:https://sc.ftqq.com</span><br><span class="line"># 注意 server_key 不要泄露，泄漏后可以去官网重置 </span><br><span class="line">echo &quot;************** 开始处理 server 酱通知 & quot;</span><br><span class="line">server_key=SCU60861T303e1c479df6cea9e95fc54d210232565d7dbbf075750</span><br><span class="line"># 传输 2 个参数:text/desp,desp 使用 markdown 语法 (注意换行符要使用 2 个换行)</span><br><span class="line">cat&gt;./shadowsocks_msg.txt&lt;&lt;EOF</span><br><span class="line">text=shadowsocks 服务更换端口重新启动完成 </span><br><span class="line">&amp;desp=</span><br><span class="line">- IP 地址：$ip</span><br><span class="line"></span><br><span class="line">- 端口号：$port</span><br><span class="line"></span><br><span class="line">- 密码：$pwd</span><br><span class="line"></span><br><span class="line">- 加密方式：aes-256-cfb</span><br><span class="line">EOF</span><br><span class="line">curl -X POST --data-binary @./shadowsocks_msg.txt  https://sc.ftqq.com/$server_key.send</span><br><span class="line">echo &quot;&quot;</span><br><span class="line">echo &quot;**************server 酱通知处理完成 & quot;</span><br><span class="line"># 失败 </span><br><span class="line">else</span><br><span class="line">clear</span><br><span class="line">cat&lt;&lt;EOF</span><br><span class="line">**************Failed,retry please!*************</span><br><span class="line"></span><br><span class="line">cat /etc/ss.log to get something you need.</span><br><span class="line"></span><br><span class="line">**************Failed,retry please!*************</span><br><span class="line">EOF</span><br><span class="line">fi</span><br></pre></td></tr></table></figure><p>执行脚本的输出信息如下【需要手动设置新的端口号，我设置为 2020，密码仍旧使用默认值】，表示重启完成：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">[root@playpi ~]# sh auto_restart_shadowsocks.sh </span><br><span class="line">Please enter PORT [2018 default]:2020</span><br><span class="line">port will be set to 2020</span><br><span class="line">Please enter PASSWORD [pengfeivpn default]:</span><br><span class="line">password will be set to pengfeivpn</span><br><span class="line">****************start generate /etc/shadowsocks.json</span><br><span class="line">****************start open port</span><br><span class="line">****************start stop shadowsocks</span><br><span class="line">INFO: loading config from /etc/shadowsocks.json</span><br><span class="line">stopped</span><br><span class="line">****************start check shadowsocks</span><br><span class="line">INFO: loading config from /etc/shadowsocks.json</span><br><span class="line">2019-10-02 10:53:17 INFO     loading libcrypto from libcrypto.so.10</span><br><span class="line">started</span><br><span class="line">***************Congratulation!*****************</span><br><span class="line">shadowsocks restart successfully!</span><br><span class="line"></span><br><span class="line">IP:45.32.79.20</span><br><span class="line">PORT:2020</span><br><span class="line">PASSWORD:pengfeivpn</span><br><span class="line">METHOD:aes-256-cfb</span><br><span class="line"></span><br><span class="line">*****************JUST ENJOY IT!****************</span><br><span class="line">************** 开始处理 server 酱通知 </span><br><span class="line">&#123;&quot;errno&quot;:0,&quot;errmsg&quot;:&quot;success&quot;,&quot;dataset&quot;:&quot;done&quot;&#125;</span><br><span class="line">**************server 酱通知处理完成 </span><br><span class="line">[root@playpi ~]#</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191002185742.png" alt="重启成功" title="重启成功"></p><p>同时，<code>server</code> 酱也接收到通知，可以很方便地直接转发给需要的人了。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191002185749.png" alt="server 酱的通知" title="server 酱的通知"></p><h1 id="监控服务"><a href="# 监控服务" class="headerlink" title="监控服务"></a>监控服务 </h1><p> 鉴于国家管控越来越严格，有时候会误伤到我们的 <code>VPS</code>，毕竟我只是用来学习技术、测试接口，没有做什么违法的事，有时候突然挂掉了我也不知道，直到需要用到的时候才发现已经挂掉了，这时候还要去折腾，重启甚至更换 <code>ip</code>，影响心情，也影响做事的效率。</p><p>那么有没有可能做一个简单的监控服务，每隔一段时间检测一下服务是否正常，如果不正常则发送通知。如果连续多次不正常，则发送通知提醒更换端口重启；如果是 <code>ip</code> 被封，此时重启没有用了，应该发送通知，提醒重新更换主机。</p><p>使用 <code>Shell</code> 可以做一个简化的版本，脚本已经被我上传至 <code>GitHub</code>，在 <code>Shell</code> 中可以直接使用 <code>wget</code> 命令下载，使用如下命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://raw.githubusercontent.com/iplaypi/iplaypistudy/master/iplaypistudy-normal/src/bin/20190828/auto_monitor_shadowsocks.sh</span><br></pre></td></tr></table></figure><p>下载下来后接着直接运行即可，使用 <code>sh auto_monitor_shadowsocks.sh</code> 。</p><p>当然，这个监控脚本是要放在常用的主机上面运行，或者是在自己的电脑后台运行，但是为了确保一直后台运行，还是放在远程服务器上比较好，例如公司的公共服务器、阿里云主机等，这样就可以一直运行并监控【确保运行在家庭的网络环境中或者公司的网络环境中，否则监控结果没有意义】。</p><p>下面简单描述自动化脚本的思路：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">1、执行脚本时输入 ip、端口号、周期，然后每隔指定时间按照如下流程检测一次 </span><br><span class="line">2、使用 ping 检测 ip 是否可用 </span><br><span class="line">3、如果 ip 不可用，通过 server 酱通知；如果 ip 可用，进一步检测端口是否可用 </span><br><span class="line">4、如果端口不可用，记录并通过 server 酱通知；如果端口可用，不做操作 </span><br><span class="line">5、步骤 4 中如果端口不可用连续超过 3 次，才发送通知 </span><br><span class="line">6、如果更换了 ip 或者端口，此监控脚本需要重启，从头重新开始检测 </span><br></pre></td></tr></table></figure><p>脚本内容整理如下，重要的地方已经注释清楚【这里要特别注意脚本中的换行符号，一律使用 <code>\\n</code> 的形式，否则会引起错误】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">#!/bin/bash</span><br><span class="line"># 脚本接收 3 个参数:ip/port/ 执行周期 (默认 10 分钟), 切记放在后台运行 </span><br><span class="line"># 注意本脚本中的换行符号，一律使用 \n 的形式，否则会引起错误 </span><br><span class="line"># 日志路径，如果安装失败需要查看日志，是否有异常 / 报错信息 </span><br><span class="line"># 最少 2 个参数，否则直接退出 </span><br><span class="line">if [2 -gt $#];then</span><br><span class="line">  echo &quot;must enter ip and port&quot;</span><br><span class="line">  exit 1</span><br><span class="line">fi</span><br><span class="line">ip=$1</span><br><span class="line">port=$2</span><br><span class="line">log_path=/etc/auto_monitor_shadowsocks.log</span><br><span class="line"># 设置执行周期，默认 10 分钟，如果参数有指定则使用 </span><br><span class="line">circle_time=$3</span><br><span class="line">if [-z $circle_time];then</span><br><span class="line">  circle_time=10m</span><br><span class="line">fi</span><br><span class="line">echo &quot;ip will be set to [$ip],port will be set to [$port],circle_time will be set to [$circle_time]&quot;</span><br><span class="line"># 变量，标记是否通知以及通知内容 </span><br><span class="line">notice=0</span><br><span class="line">notice_msg=&quot;&quot;</span><br><span class="line"># 变量，标记 ip / 端口的失败次数 </span><br><span class="line">ip_fail_num=0</span><br><span class="line">port_fail_num=0</span><br><span class="line"># while 循环 </span><br><span class="line">while :</span><br><span class="line">do</span><br><span class="line">  # 查看 ip 是否正常 </span><br><span class="line">  ping=`ping -c 1 $ip |grep loss |awk &apos;&#123;print $6&#125;&apos; |awk -F &quot;%&quot; &apos;&#123;print $1&#125;&apos;`</span><br><span class="line">  # ip 不可用 </span><br><span class="line">  if [$ping -eq 100];then</span><br><span class="line">    ip_fail_num=`expr $ip_fail_num + 1`</span><br><span class="line">    echo ping [$ip] at $(date +% Y-% m-% d% t% X) fail &gt;&gt; $log_path</span><br><span class="line">    notice=1</span><br><span class="line">    notice_msg=`echo ping [$ip] at $(date +% Y-% m-% d% t% X) 失败，累计次数：[$ip_fail_num]，请更换主机 `</span><br><span class="line">    #ip 可用 </span><br><span class="line">  else</span><br><span class="line">    echo ping [$ip] at $(date +% Y-% m-% d% t% X) ok &gt;&gt; $log_path</span><br><span class="line">    ip_fail_num=0</span><br><span class="line">    # 接着判断端口是否可用，使用 nc 工具，超时时间为 20 秒 </span><br><span class="line">    `nc -v -z -w 20 $ip $port`</span><br><span class="line">    # 端口不可用 </span><br><span class="line">    if [0 -ne $?];then</span><br><span class="line">      port_fail_num=`expr $port_fail_num + 1`</span><br><span class="line">      echo nc [$ip:$port] at $(date +% Y-% m-% d% t% X) fail &gt;&gt; $log_path</span><br><span class="line">      if [$port_fail_num -gt 3];then</span><br><span class="line">        notice=1</span><br><span class="line">        notice_msg=`echo nc [$ip:$port] at $(date +% Y-% m-% d% t% X) 失败，累计次数：[$port_fail_num]，请更换端口 `</span><br><span class="line">      fi</span><br><span class="line">    # 端口可用 </span><br><span class="line">    else</span><br><span class="line">      echo nc [$ip:$port] at $(date +% Y-% m-% d% t% X) ok &gt;&gt; $log_path</span><br><span class="line">      port_fail_num=0</span><br><span class="line">    fi</span><br><span class="line">  fi</span><br><span class="line"># 建议开启 server 酱自动通知，推送到微信 </span><br><span class="line"># 不开启请把以下内容注释掉，注释内容持续到 & apos;server 酱通知完成 & apos;</span><br><span class="line"># 关于 server 酱的使用请参考:https://sc.ftqq.com</span><br><span class="line"># 注意 server_key 不要泄露，泄漏后可以去官网重置 </span><br><span class="line">if [1 -eq $notice];then</span><br><span class="line">  echo &quot;************** 开始处理 server 酱通知 & quot; &gt;&gt; $log_path</span><br><span class="line">  server_key=SCU60861T303e1c479df6cea9e95fc54d210232565d7dbbf075750</span><br><span class="line">  # 传输 2 个参数:text/desp,desp 使用 markdown 语法 (注意换行符要使用 2 个换行)</span><br><span class="line">cat&gt;./shadowsocks_msg.txt&lt;&lt;EOF</span><br><span class="line">text=shadowsocks 定时监控服务消息 </span><br><span class="line">&amp;desp=</span><br><span class="line">$notice_msg</span><br><span class="line">EOF</span><br><span class="line">  curl -X POST --data-binary @./shadowsocks_msg.txt https://sc.ftqq.com/$server_key.send &gt;&gt; $log_path</span><br><span class="line">  echo &quot;&quot; &gt;&gt; $log_path</span><br><span class="line">  echo &quot;**************server 酱通知处理完成 & quot; &gt;&gt; $log_path</span><br><span class="line">  notice=0</span><br><span class="line">  notice_msg=&quot;&quot;</span><br><span class="line">fi</span><br><span class="line">sleep $circle_time</span><br><span class="line">done</span><br></pre></td></tr></table></figure><p>执行脚本后，每隔 10 分钟检测一下 <code>ip</code> 或者端口是否可以正常访问。如果正常什么都不做；如果端口不正常则记录，如果端口连续 3 次不正常则发送故障报告，提醒更换端口；如果 <code>ip</code> 不正常则发送故障报告，提醒更换主机。</p><p>下面列举一些 <code>server</code> 酱的通知示例。</p><p>端口连续不可用。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191003005805.png" alt="端口连续不可用" title="端口连续不可用"></p><p><code>ip</code> 不可用。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191003005759.png" alt="ip 不可用" title="ip 不可用"></p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注</h1><p>1、Server 酱的使用有限制，每天限制 1000 条信息，所以千万不能写个死循环狂发信息，会被拉黑的。</p><p>2、使用 <code>wget</code> 下载文件时，如果本地文件已经存在，会自动新建一个文件，文件很多，有时候会显得很乱，如果想覆盖下载，可以使用 <code>-N</code> 参数，或者使用 <code>-O your_file_name</code> 参数指定本地文件名。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Wed Mar 18 2020 16:23:05 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;以前我整理过一篇博客，详细叙述了如何自己搭建梯子，图文并茂，可以参见：&lt;a href=&quot;https://www.playpi.org/2018111601.html&quot;&gt;使用 Vultr 搭建 Shadowsocks（VPS 搭建 SS）&lt;/a&gt; 。里面有涉及到购买一台云服务器后该如何操作：初始化环境、安装 &lt;code&gt;Shadowsocks&lt;/code&gt;、配置参数、安装防火墙、启动服务、检查服务状态等等步骤。&lt;/p&gt;&lt;p&gt;虽然过程很详细，只要几个命令就可以完成 &lt;code&gt;Shadowsocks&lt;/code&gt; 服务的搭建，但是对于没有技术基础又不想折腾的读者来说，还是有点困难。所以我把安装过程整理成一个自动化的 &lt;code&gt;Shell&lt;/code&gt; 脚本，读者下载下来之后，直接运行即可，在运行过程中如果需要询问交互，例如填写密码、端口号等，读者直接填写即可，或者直接使用默认的设置。&lt;/p&gt;
    
    </summary>
    
      <category term="知识改变生活" scheme="https://www.playpi.org/categories/knowledge-for-life/"/>
    
    
      <category term="Shell" scheme="https://www.playpi.org/tags/Shell/"/>
    
      <category term="shadowsocks" scheme="https://www.playpi.org/tags/shadowsocks/"/>
    
      <category term="firewalld" scheme="https://www.playpi.org/tags/firewalld/"/>
    
      <category term="Shadowsocks" scheme="https://www.playpi.org/tags/Shadowsocks/"/>
    
      <category term="CentOS" scheme="https://www.playpi.org/tags/CentOS/"/>
    
  </entry>
  
</feed>
