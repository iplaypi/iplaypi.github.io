<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>虾丸派</title>
  
  <subtitle>烂笔头</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://www.playpi.org/"/>
  <updated>2020-05-01T09:06:06.000Z</updated>
  <id>https://www.playpi.org/</id>
  
  <author>
    <name>虾丸派</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Elasticsearch 的 fielddata 入门指引</title>
    <link href="https://www.playpi.org/2020050101.html"/>
    <id>https://www.playpi.org/2020050101.html</id>
    <published>2020-05-01T09:06:06.000Z</published>
    <updated>2020-05-01T09:06:06.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>昨天查看 <code>Elasticsearch</code> 集群监控，发现有几个 <code>Elasticsearch</code> 节点的 <code>JVM Heap</code> 异常，上下波动非常频繁，进一步查看 <code>GC</code>，发现 <code>Full GC</code> 非常频繁，每分钟达到 5-10 次，而累加耗时有 10-20 秒，也就是说有 17%-33% 的时间都在做 <code>Full GC</code>，这显然是不健康的。</p><p>进一步查看 <code>CPU</code> 使用情况，发现 <code>CPU</code> 使用率由正常的 20% 左右，达到现在的 50%-70%，这也是不正常的。</p><p>排查方向是看是否有大量的 <code>aggregations</code> 请求或者排序 <code>sort</code> 操作，这种情况才会造成大量的内存占用【<code>fielddata</code> 缓存】。当然其它情况也有可能，但概率低【例如大查询，<code>query</code> 语句复杂，数据量大】，具体情况需要具体对待。排查的过程就不说了，由此做引子，先简单对 <code>fielddata</code> 缓存做入门指引。</p><p>首先声明，本文记录的内容是 <code>fielddata</code> 缓存数据、熔断器，不是 <code>text</code> 开启 <code>fielddata</code> 属性那种含义，给出 3 个链接：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-fielddata.html" target="_blank" rel="noopener">modules-fielddata</a>、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/circuit-breaker.html" target="_blank" rel="noopener">circuit-breaker</a>、<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/cluster-nodes-stats.html" target="_blank" rel="noopener">cluster-nodes-stats</a> 。</p><a id="more"></a><h1 id="问题引入"><a href="# 问题引入" class="headerlink" title="问题引入"></a>问题引入 </h1><p> 昨天将要迎来小长假，仔细看了一眼 <code>Elasticsearch</code> 集群的监控，发现某几个 <code>Elasticsearch</code> 节点的 <code>CPU</code> 使用率、<code>JVM</code> 内存的指标都有异常，初步排查下来和 <code>aggregations</code> 请求有关，而根本原因和 <code>fielddata</code> 缓存有关。</p><p>异常的监控 </p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200506014112.png" alt="CPU 使用率" title="CPU 使用率"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200506014105.png" alt="Full GC 次数" title="Full GC 次数"></p><p> 观察服务端集群的日志，发现有频繁的 <code>Full GC</code> 流程：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">[2020-05-01T17:01:03,243][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206644 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:03,296][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778952590 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:03,333][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206644 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:03,373][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778442411 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:03,411][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206644 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:03,526][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206644 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:03,665][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206644 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:03,963][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778442411 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:04,037][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206644 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:04,518][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206644 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:04,713][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778372453 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:04,715][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12777862273 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:04,821][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12777862273 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:05,059][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12777626506 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:05,470][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206751 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:05,530][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206751 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:05,693][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778442518 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:09,173][WARN][o.e.m.j.JvmGcMonitorService] [es1] [gc][1567372] overhead, spent [665ms] collecting in the last [1s]</span><br><span class="line">[2020-05-01T17:01:12,000][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778442518 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:12,410][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206751 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:12,652][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206751 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:17,947][WARN][o.e.m.j.JvmGcMonitorService] [es1] [gc][1567379] overhead, spent [2.4s] collecting in the last [2.7s]</span><br><span class="line">[2020-05-01T17:01:20,295][WARN][o.e.i.b.fielddata] [fielddata] New used memory 12778206751 [11.9gb] for data of [_uid] would be larger than configured breaker: 12759701913 [11.8gb], breaking</span><br><span class="line">[2020-05-01T17:01:34,847][WARN][o.e.m.j.JvmGcMonitorService] [es1] [gc][1567395] overhead, spent [1.6s] collecting in the last [1.8s]</span><br><span class="line">[2020-05-01T17:01:41,989][WARN][o.e.m.j.JvmGcMonitorService] [es1] [gc][1567402] overhead, spent [638ms] collecting in the last [1.1s]</span><br><span class="line">[2020-05-01T17:01:51,991][WARN][o.e.m.j.JvmGcMonitorService] [es1] [gc][1567412] overhead, spent [563ms] collecting in the last [1s]</span><br><span class="line">[2020-05-01T17:02:04,601][WARN][o.e.m.j.JvmGcMonitorService] [es1] [gc][1567422] overhead, spent [2.8s] collecting in the last [3.6s]</span><br><span class="line">[2020-05-01T17:02:16,809][WARN][o.e.m.j.JvmGcMonitorService] [es1] [gc][1567433] overhead, spent [1.7s] collecting in the last [2.2s]</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200506014039.png" alt="频繁 Full GC" title="频繁 Full GC"></p><p>日志中显示的 <code>breaking</code>，即熔断，和 <code>fielddata</code> 缓存有关，而根源是 <code>_uid</code> 字段。</p><p>下面简单介绍 <code>fielddata</code> 缓存的内容。</p><h1 id="入门指引"><a href="# 入门指引" class="headerlink" title="入门指引"></a>入门指引 </h1><h2 id="含义"><a href="# 含义" class="headerlink" title="含义"></a> 含义 </h2><p><code>feilddata</code> 缓存的含义：<code>Elasticsearch</code> 节点在处理 <code>aggregations</code>、<code>sort</code>、自定义脚本等请求时，为了快速计算，需要把相关的值全部加载到 <code>JVM</code> 的内存中，而且一般情况下不会释放。</p><p> 原文：</p><blockquote><p>The field data cache is used mainly when sorting on or computing aggregations on a field. It loads all the field values to memory in order to provide fast document based access to those values. The field data cache can be expensive to build for a field, so its recommended to have enough memory to allocate it, and to keep it loaded.</p></blockquote><p>如果对一个多值的字段做此操作，必然需要很大的内存，例如极端一点的 <code>id</code> 字段【值唯一，多少条数据也就有多少个值】、时间戳字段【值的可能性很多】。</p><p>其实，这种数据就是正排索引，刚好与 <code>Elasticsearch</code> 的倒排索引相反，它不是为了快速检索，而是为了计算值的分布、排序。</p><h2 id="相关配置"><a href="# 相关配置" class="headerlink" title="相关配置"></a>相关配置 </h2><p> 提示：这些配置大部分可以通过 <code>put</code> 的方式更新到 <code>Elasticsearch</code> 集群，立即生效。</p><p>1、<code>indices.fielddata.cache.size</code>，字段可以占用缓存的最大值，默认无边界。如果这里不手动设置，建议把熔断器设置好，否则集群在大量的 <code>aggregations</code> 请求下很容易挂掉。</p><p>官方解释：</p><blockquote><p>The max size of the field data cache, eg 30% of node heap space, or an absolute value, eg 12GB. Defaults to unbounded.</p></blockquote><p>注意：这个是静态设置，必须在 <code>Elasticsearch</code> 群集中的每个数据节点上进行配置，所以无法实时更新。建议设置时取值比 <code>indices.breaker.fielddata.limit</code> 稍小，否则这个参数也就没有意义了【到达 <code>indices.breaker.fielddata.limit</code> 已经被熔断了】。</p><p>2、<code>indices.breaker.fielddata.limit</code>，<code>fielddata</code> 占用内存的熔断器，超过后出发垃圾回收机制，回收内存。</p><blockquote><p>Limit for fielddata breaker, defaults to 60% of JVM heap</p></blockquote><p><code>indices.breaker.fielddata.overhead</code>，一个系数【我也没搞明白，大概是内存占用估算时需要乘以它】。</p><blockquote><p>A constant that all field data estimations are multiplied with to determine a final estimation. Defaults to 1.03</p></blockquote><p>3、<code>indices.breaker.total.limit</code>，总的内存限制，它可以保护所有的请求、处理过程。</p><blockquote><p>Starting limit for overall parent breaker, defaults to 70% of JVM heap.</p></blockquote><p>4、其它几个相关参数。</p><ul><li><code>indices.breaker.request.limit</code>，请求的内存占用限制 </li><li><code>indices.breaker.request.overhead</code>，与上面那个参数相关的系数</li><li><code>network.breaker.inflight_requests.limit</code>，处理请求的内存占用限制</li><li><code>network.breaker.inflight_requests.overhead</code>，与上面那个参数相关的系数</li></ul><h2 id="接口查看示例"><a href="# 接口查看示例" class="headerlink" title="接口查看示例"></a> 接口查看示例 </h2><p> 可以从不同的角度查看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> 查看所有的 </span><br><span class="line">GET /_cat/fielddata</span><br><span class="line"> 过滤字段的 </span><br><span class="line">GET /_cat/fielddata?v&amp;fields=uid&amp;pretty</span><br><span class="line"></span><br><span class="line"># Fielddata summarised by node</span><br><span class="line">GET /_nodes/stats/indices/fielddata?fields=field1,field2</span><br><span class="line"></span><br><span class="line"># Fielddata summarised by node and index</span><br><span class="line">GET /_nodes/stats/indices/fielddata?level=indices&amp;fields=field1,field2</span><br><span class="line"></span><br><span class="line"># Fielddata summarised by node, index, and shard</span><br><span class="line">GET /_nodes/stats/indices/fielddata?level=shards&amp;fields=field1,field2</span><br><span class="line"></span><br><span class="line"># You can use wildcards for field names</span><br><span class="line">GET /_nodes/stats/indices/fielddata?fields=field*</span><br></pre></td></tr></table></figure><p>下面给出 2 种查询示例 </p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200506013927.png" alt="stats 查看" title="stats 查看"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200506013952.png" alt="cat 查看" title="cat 查看"></p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a> 备注 </h1><p>1、关于熔断的异常。</p><p> 如果执行的查询刚好遇到熔断，会返回到客户端异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    &quot;error&quot;: &quot;... CircuitBreakingException [[FIELDDATA] Data too large, data for [proccessDate] would be larger than limit of [12759701913/11.8gb]]; &#125;]&quot;,</span><br><span class="line">    &quot;status&quot;: 500</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>2、对于一般的字段，很难达到大量的内存占用，而多值的字段则很容易，例如 <code>id</code>、时间戳等字段，取值范围太广了，无论是聚合还是排序，都需要大内存【而且对于这种字段做聚合、排序显然是无意义的】。</p><p>此外还有一种内置元字段，可能也会被误操作加载到内存中去了【上述现象就是这种】，关于 <code>_uid</code> 元字段的内容请参考官网【<code>v7.x</code> 之后不再有这个字段，因为取消了 <code>type</code>】：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/mapping-uid-field.html" target="_blank" rel="noopener">mapping-uid-field</a> 。</p><p>由于 <code>fielddata</code> 缓存优先级非常高，<code>JVM</code> 做 <code>Full GC</code> 时很难把它清理掉，所以会永远占用着内存，如果内存使用已经达到了上限，则就会引发频繁的 <code>Full GC</code>，严重点 <code>Elasticsearch</code> 节点可能会挂掉。</p><p>所以，手动清除缓存也是有必要的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">curl localhost:9200/index/_cache/clear?pretty&amp;filter=false&amp;field_data=true&amp;fields=_uid,site_name</span><br><span class="line"></span><br><span class="line"> 关于 `&amp;bloom=false` 参数的问题，要看当前 `Elasticsearch` 版本是否支持，`v5.6.x` 是不支持了。</span><br></pre></td></tr></table></figure><p>3、关于 <code>fielddata</code> 属性【不是本文描述的 <code>fielddata</code> 缓存】。</p><p>关于 <code>text</code> 类型字段的 <code>fielddata</code> 解释，官网链接：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/fielddata.html" target="_blank" rel="noopener">fielddata</a> 。</p><p>如果对没有开启 <code>fielddata</code> 属性的 <code>text</code> 字段执行聚合、排序等操作，会抛出异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Fielddata is disabled on text fields by default. Set fielddata=true on [your_field_name] in order to load fielddata in memory by uninverting the inverted index. Note that this can however use significant memory.</span><br></pre></td></tr></table></figure><p>类型有 <code>paged_bytes</code>【默认的】、<code>fst</code>、<code>doc_values</code> 等几种。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;昨天查看 &lt;code&gt;Elasticsearch&lt;/code&gt; 集群监控，发现有几个 &lt;code&gt;Elasticsearch&lt;/code&gt; 节点的 &lt;code&gt;JVM Heap&lt;/code&gt; 异常，上下波动非常频繁，进一步查看 &lt;code&gt;GC&lt;/code&gt;，发现 &lt;code&gt;Full GC&lt;/code&gt; 非常频繁，每分钟达到 5-10 次，而累加耗时有 10-20 秒，也就是说有 17%-33% 的时间都在做 &lt;code&gt;Full GC&lt;/code&gt;，这显然是不健康的。&lt;/p&gt;&lt;p&gt;进一步查看 &lt;code&gt;CPU&lt;/code&gt; 使用情况，发现 &lt;code&gt;CPU&lt;/code&gt; 使用率由正常的 20% 左右，达到现在的 50%-70%，这也是不正常的。&lt;/p&gt;&lt;p&gt;排查方向是看是否有大量的 &lt;code&gt;aggregations&lt;/code&gt; 请求或者排序 &lt;code&gt;sort&lt;/code&gt; 操作，这种情况才会造成大量的内存占用【&lt;code&gt;fielddata&lt;/code&gt; 缓存】。当然其它情况也有可能，但概率低【例如大查询，&lt;code&gt;query&lt;/code&gt; 语句复杂，数据量大】，具体情况需要具体对待。排查的过程就不说了，由此做引子，先简单对 &lt;code&gt;fielddata&lt;/code&gt; 缓存做入门指引。&lt;/p&gt;&lt;p&gt;首先声明，本文记录的内容是 &lt;code&gt;fielddata&lt;/code&gt; 缓存数据、熔断器，不是 &lt;code&gt;text&lt;/code&gt; 开启 &lt;code&gt;fielddata&lt;/code&gt; 属性那种含义，给出 3 个链接：&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-fielddata.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;modules-fielddata&lt;/a&gt;、&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/5.6/circuit-breaker.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;circuit-breaker&lt;/a&gt;、&lt;a href=&quot;https://www.elastic.co/guide/en/elasticsearch/reference/5.6/cluster-nodes-stats.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;cluster-nodes-stats&lt;/a&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="fielddata" scheme="https://www.playpi.org/tags/fielddata/"/>
    
      <category term="aggregations" scheme="https://www.playpi.org/tags/aggregations/"/>
    
      <category term="JVM" scheme="https://www.playpi.org/tags/JVM/"/>
    
  </entry>
  
  <entry>
    <title>Search Guard 安装部署实践</title>
    <link href="https://www.playpi.org/2020042701.html"/>
    <id>https://www.playpi.org/2020042701.html</id>
    <published>2020-04-27T09:30:52.000Z</published>
    <updated>2020-05-02T09:30:52.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>最近 <code>Elasticsearch</code> 集群出了一个小事故，根本原因在于对集群请求的监控不完善，以及对 <code>Elasticsearch</code> 访问权限无监控【目前是使用 <code>LDAP</code> 账号就可以登录访问，而且操作权限很大，这有很大的隐患】。因此，最近准备上线 <code>Search Guard</code>，先在测试环境部署安装了一遍，并测试了相关服务，整理了如下安装部署文档，以供读者参考。</p><p>开发环境基于 <code>Elasticsearch v5.6.8</code>、<code>Search Guard v5.6.8-19.1</code>、<code>Java v1.8</code> 。</p><a id="more"></a><h1 id="相关包、文档参考"><a href="# 相关包、文档参考" class="headerlink" title="相关包、文档参考"></a>相关包、文档参考 </h1><h2 id="Search-Guard- 下载"><a href="#Search-Guard- 下载" class="headerlink" title="Search Guard 下载"></a>Search Guard 下载</h2><p><code>Search Guard</code> 首页：<a href="https://docs.search-guard.com/v5/search-guard-versions" target="_blank" rel="noopener">Search Guard versions</a> 。</p><p> 仓库在线包：<a href="https://oss.sonatype.org/service/local/repositories/releases/content/com/floragunn/search-guard-5/5.6.8-19.1/search-guard-5-5.6.8-19.1.zip" target="_blank" rel="noopener">repositories</a> ，也可以使用坐标：<code>com.floragunn:search-guard-5:5.6.8-19.1</code>。</p><p>此外，这个官方的在线包没了：<a href="https://releases.floragunn.com/search-guard-5/5.6.16-19.4/search-guard-5-5.6.16-19.4.zip" target="_blank" rel="noopener">search-guard-5</a> ，版本选择：<code>v5.6.8-19.1</code>。</p><h2 id="证书工具下载"><a href="# 证书工具下载" class="headerlink" title="证书工具下载"></a>证书工具下载 </h2><p> 证书工具 <code>tlstool</code>：<a href="https://search.maven.org/search?q=a:search-guard-tlstool" target="_blank" rel="noopener">search-guard-tlstool</a> ，版本选择：<code>v1.7</code>。</p><h2 id="相关文档参考"><a href="# 相关文档参考" class="headerlink" title="相关文档参考"></a>相关文档参考 </h2><p><code>offline-tls-tool</code>：<a href="https://docs.search-guard.com/latest/offline-tls-tool" target="_blank" rel="noopener">offline-tls-tool</a> 。</p><p><code>Search Guard</code>：<a href="https://docs.search-guard.com/latest/installation-windows#install-search-guard-on-elasticsearch" target="_blank" rel="noopener">installation-windows</a> 。</p><p><code>Search Guard sgadmin.sh</code> 命令参数：<a href="https://docs.search-guard.com/latest/sgadmin" target="_blank" rel="noopener">sgadmin</a> 。</p><h1 id="回滚"><a href="# 回滚" class="headerlink" title="回滚"></a> 回滚 </h1><p> 安装部署过程中，避免不了小概率的异常，所以需要考虑回滚操作。</p><p><code>Elasticsearch</code> 如果需要回滚，不需要卸载插件，不需要删除配置，直接在 <code>Elasticsearch</code> 配置中添加参数，关闭 <code>Search Guard</code> 插件的使用：<code>searchguard.disabled: true</code>，再重启 <code>Elasticsearch</code> 集群即可。</p><p>如果有其它强关联的业务服务，提前准备好对应的分支或者 <code>tag</code>，重新部署即可，不需要变更代码。</p><h1 id="部署操作"><a href="# 部署操作" class="headerlink" title="部署操作"></a>部署操作 </h1><p> 前提：不支持单台停机滚动安装，需要重启所有 <code>Elasticsearch</code> 节点，当然，停机时间很短暂。</p><p>以下记录安装、配置、部署流程。</p><p>提示：所有的证书文件、配置文件【包括 <code>Elasticsearch</code> 节点的、<code>Search Guard</code> 插件的】都可以提前准备好，<code>Search Guard</code> 插件也可以提前安装好。重启 <code>Elasticsearch</code> 集群后，在黄色状态下，开始激活 <code>Search Guard</code> 插件，可能需要一点时间，几分钟到十几分钟。</p><h2 id="准备证书"><a href="# 准备证书" class="headerlink" title="准备证书"></a>准备证书 </h2><p> 证书需要提前生成好，需要为所有的 <code>Elasticsearch</code> 节点都生成证书，即每个节点都有自己独立的证书文件。</p><p>附件为配置示例以及生成的证书示例，已经被我上传至 <code>GitHub</code>，读者可以下载查看：<a href="https://github.com/iplaypi/iplaypistudy/tree/master/iplaypistudy-normal/src/resource/20200427" target="_blank" rel="noopener">证书配置以及证书文件 </a> ，测试主机为：<code>dev4、dev5、dev6</code>，域名为：<code>playpi.org</code>。</p><h3 id="下载解压"><a href="# 下载解压" class="headerlink" title="下载解压"></a> 下载解压 </h3><p> 解压到本地磁盘中，任何主机都可以，提前做好。</p><p>注意每个目录的作用：</p><ul><li><code>config</code>，配置文件，需要根据自己的情况更改一些配置 </li><li><code>tools</code>，脚本文件，生成使用</li><li><code>deps</code>，依赖文件，工具需要的依赖包</li></ul><h3 id="准备配置文件"><a href="# 准备配置文件" class="headerlink" title="准备配置文件"></a> 准备配置文件 </h3><p> 配置 <code>ca</code> 证书、<code>Elasticsearch</code> 节点、客户端信息，<code>Elasticsearch</code> 节点信息包含：<code>name</code>、<code>dn</code>、<code>dns</code>、<code>ip</code>，具体参考附件内容。</p><p>以下给出 <code>Elasticsearch</code> 节点、客户端信息的示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">###</span><br><span class="line">### Nodes</span><br><span class="line">###</span><br><span class="line">#</span><br><span class="line"># Specify the nodes of your ES cluster here</span><br><span class="line">#      </span><br><span class="line">nodes:</span><br><span class="line">  - name: dev4</span><br><span class="line">    dn: CN=dev4.playpi.com,OU=Ops,O=Playpi Com\, Inc.,DC=playpi,DC=com</span><br><span class="line">    ip: 192.168.1.4</span><br><span class="line">    dns: dev4.playpi.com</span><br><span class="line">  - name: dev5</span><br><span class="line">    dn: CN=dev5.playpi.com,OU=Ops,O=Playpi Com\, Inc.,DC=playpi,DC=com</span><br><span class="line">    ip: 192.168.1.5</span><br><span class="line">    dns: dev5.playpi.com</span><br><span class="line">  - name: dev6</span><br><span class="line">    dn: CN=dev6.playpi.com,OU=Ops,O=Playpi Com\, Inc.,DC=playpi,DC=com</span><br><span class="line">    ip: 192.168.1.6</span><br><span class="line">    dns: dev6.playpi.com</span><br><span class="line"></span><br><span class="line">###</span><br><span class="line">### Clients</span><br><span class="line">###</span><br><span class="line">#</span><br><span class="line"># Specify the clients that shall access your ES cluster with certificate authentication here</span><br><span class="line">#</span><br><span class="line"># At least one client must be an admin user (i.e., a super-user). Admin users can</span><br><span class="line"># be specified with the attribute admin: true    </span><br><span class="line">#        </span><br><span class="line">clients:</span><br><span class="line">  - name: client-admin</span><br><span class="line">    dn: CN=client-admin.playpi.com,OU=Ops,O=Playpi Com\, Inc.,DC=playpi,DC=com</span><br><span class="line">    admin: true</span><br><span class="line">  - name: client-custom</span><br><span class="line">    dn: CN=client-custom.playpi.com,OU=Ops,O=Playpi Com\, Inc.,DC=playpi,DC=com</span><br></pre></td></tr></table></figure><h3 id="生成证书文件"><a href="# 生成证书文件" class="headerlink" title="生成证书文件"></a>生成证书文件 </h3><p> 执行解压目录内的工具：<code>./tools/sgtlstool.sh -c &lt;path&gt;/tlsconfig.yml -ca -crt</code>，生成证书文件，输出目录为 <code>out</code>。</p><p>参数含义：</p><ul><li>-c，指定配置文件位置 </li><li>-ca，创建并指定本地证书授权中心【如果已经存在本地证书授权中心则不需要】</li><li>-crt，使用本地证书授权中心创建证书</li></ul><p> 除了 <code>root</code> 证书总计有 3 个文件【包含 <code>readme</code>】，<code>client</code> 总计有 5 个文件【包含 <code>readme</code>】，此外每个 <code>Elasticsearch</code> 节点对应有 5 个文件【包含 <code>config</code>】。</p><p>创建成功后输出日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Root certificate has been sucessfully created.</span><br><span class="line">The passwords of the private key files have been auto generated. You can find the passwords in root-ca.readme.</span><br><span class="line"></span><br><span class="line">Created 6 node certificates.</span><br><span class="line">Passwords for the private keys of the node certificates have been auto-generated. The passwords are stored in the config snippet files.</span><br><span class="line">Created 2 client certificates.</span><br><span class="line">Passwords for the private keys of the client certificates have been auto-generated. The passwords are stored in the file &quot;client-certificates.readme&quot;</span><br></pre></td></tr></table></figure><p>输出证书文件 </p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200504162349.png" alt="生成证书文件一览" title="生成证书文件一览"></p><p> 备注 <code>Windows</code> 操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.\tools\sgtlstool.bat -c .\config\tlsconfig.yml -ca -crt</span><br></pre></td></tr></table></figure><h3 id="检查证书文件"><a href="# 检查证书文件" class="headerlink" title="检查证书文件"></a>检查证书文件 </h3><p> 上面的截图中已经给出 <code>out</code> 目录中生成的文件，下面简单描述一下。</p><p>根据 <code>tlsconfig.yml</code> 配置会生成以下文件：</p><ul><li>1 份根证书文件【3 个文件，用于 <code>Elasticsearch</code> 配置】：<code>root-ca.key</code>、<code>root-ca.pem</code>、<code>root-ca.readme</code></li><li>每个 <code>node</code> 生成 1 份证书文件【5 个文件，用于 <code>Elasticsearch</code> 配置】：<code>&lt;node&gt;.key</code>、<code>&lt;node&gt;.pem</code>、<code>&lt;node&gt;_http.key</code>、<code>&lt;node&gt;_http.pem</code>【若密码生成方式设为 <code>auto</code>，密码在各个 <code>node</code> 的 <code>&lt;node&gt;_elasticsearch_config_snippet.yml</code> 文件中找】</li><li>客户端 1 份证书文件【5 个文件，用于 <code>sgadmin.sh</code> 执行激活】：<code>client-admin</code>.key、<code>client-admin.pem</code>、<code>client-custom.key</code>、<code>client-custom.pem</code>【若密码生成方式设为 <code>auto</code>，密码在 <code>client_certificates.readme</code> 中找】</li><li>其中，每个 <code>&lt;node&gt;_elasticsearch_config_snippet.yml</code> 文件中的内容，可以直接复制粘贴到每个 <code>Elasticsearch</code> 节点的配置文件中【视情况变更部分配置，例如 <code>searchguard.ssl.http.enabled: false</code>】</li></ul><p>除了用肉眼检查文件个数是否正确，还要检查证书文件是否合法【校验】，可以使用自带的工具：<code>/tools/sgtlsdiag.sh</code> 。</p><p>例如：<code>./tools/sgtlsdiag.sh -ca out/root-ca.pem -crt out/dev4.pem</code>。</p><ul><li>-ca，创建并指定本地证书授权中心【如果已经存在本地证书授权中心则不需要】</li><li>-crt，指定 <code>Elasticsearch</code> 节点证书文件 </li></ul><p> 此外，还可以检查 <code>Elasticsearch</code> 节点的配置是否正确：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./tools/sgtlsdiag.sh -es /etc/elasticsearch/elasticsearch.yml</span><br></pre></td></tr></table></figure><ul><li>-es，指定 <code>Elasticsearch</code> 配置文件 </li></ul><h2 id="安装 -Search-Guard"><a href="# 安装 -Search-Guard" class="headerlink" title="安装 Search Guard"></a> 安装 Search Guard</h2><p>提示：以下列出的是常规的安装、配置流程，实际操作中，可以提前把一切工作做好【证书生成、配置、插件安装】，然后直接重启 <code>Elasticsearch</code> 集群、激活 <code>Search Guard</code>，实际停机时间很短【保守估计 30 分钟以内，等待集群状态恢复绿色需要几小时到十几小时，视集群的分片恢复能力而定】。</p><p><code>Search Guard</code> 用户权限配置文件参考附件，已被我上传至 <code>GitHub</code>：<a href="https://github.com/iplaypi/iplaypistudy/tree/master/iplaypistudy-normal/src/resource/20200427" target="_blank" rel="noopener">Search Guard config</a> 。</p><h3 id="禁止重分配"><a href="# 禁止重分配" class="headerlink" title="禁止重分配"></a>禁止重分配 </h3><p> 防止停掉 <code>Elasticsearch</code> 节点时自动重分配。</p><p>可以在安装 <code>Search Guard</code> 插件之后，并且准备好配置文件之后再执行。</p><p>配置更新：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT /_cluster/settings/</span><br><span class="line">&#123;</span><br><span class="line">    &quot;transient&quot;: &#123;</span><br><span class="line">        &quot;cluster.routing.allocation.enable&quot;: &quot;none&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="停止集群"><a href="# 停止集群" class="headerlink" title="停止集群"></a>停止集群 </h3><p> 可以在安装 <code>Search Guard</code> 插件之后，并且准备好配置文件之后再执行。</p><p>由运维人员操作。</p><h3 id="每个节点安装插件"><a href="# 每个节点安装插件" class="headerlink" title="每个节点安装插件"></a>每个节点安装插件 </h3><p> 在线安装：在每个 <code>Elasticsearch</code> 节点上，执行：<code>./bin/elasticsearch-plugin install -b com.floragunn:search-guard-5:5.6.8-19.1</code>【在此使用坐标，或者使用仓库地址链接也可以】。</p><p>离线安装：下载安装包到本地，用 <code>zip</code> 包离线安装【在 <code>Windows</code> 系统上面试过：报 <code>unknown plugin</code>，暂未找到原因】。</p><h3 id="拷贝证书文件"><a href="# 拷贝证书文件" class="headerlink" title="拷贝证书文件"></a>拷贝证书文件 </h3><p> 将前面生成的证书文件拷贝至 <code>Elasticsearch</code> 的 <code>config</code> 目录。</p><p>包含以下内容：</p><ul><li>1 份根证书文件【2 个文件】：<code>root-ca.key</code> 和 <code>root-ca.pem</code></li><li>对应 <code>Elasticsearch</code> 节点的 1 份证书文件【4 个文件】：<code>&lt;node&gt;.key</code>、<code>&lt;node&gt;.pem</code>、 <code>&lt;node&gt;_http.key</code>、<code>&lt;node&gt;_http.pem</code></li><li>注意：<code>client</code> 客户端 4 个证书文件不需要拷贝，只把 <code>admin</code> 权限的 2 个文件拷贝到某一个节点即可，用于激活管理 <code>Search Guard</code></li><li>4 个证书文件【普通权限 2 个、<code>admin</code> 权限 2 个】，实际会用到 <code>admin</code> 权限的 2 个：<code>client-admin.key</code>、<code>client-admin.pem</code>【这 4 个证书文件实际是给通过 <code>tcp</code> 访问的客户端使用的，此外在执行 <code>sgadmin.sh</code> 的时候也需要使用】</li></ul><h3 id="修改集群配置"><a href="# 修改集群配置" class="headerlink" title="修改集群配置"></a>修改集群配置 </h3><p> 修改 <code>elasticsearch.yml</code> 配置文件，见备注部分，建议从 <code>&lt;node&gt;_elasticsearch_config_snippet.yml</code> 中直接复制粘贴，再根据实际情况更改部分内容。</p><p>注意：如果要保留 <code>http</code> 方式访问，参数 <code>searchguard.ssl.http.enabled</code> 要设为 <code>false</code>，否则访问会被拒绝。</p><p>测试环境这里选择关闭 <code>https</code> 访问，继续使用 <code>http</code> 访问。</p><h3 id="重启集群"><a href="# 重启集群" class="headerlink" title="重启集群"></a>重启集群 </h3><p> 此过程重要，需要仔细观察，必要时回退。</p><p>1、重启，并观察。</p><p>如果重启顺利，会在 <code>Elasticsearch</code> 节点日志中看到提示激活 <code>Search Guard</code> 的信息，不顺利的话可能是 <code>Search Guard</code> 相关的参数配置有误，检查修改即可。</p><p>如果还有其它无法解决的异常情况，考虑回退。</p><p>2、配置 <code>Search Guard</code>，所有的 <code>Elasticsearch</code> 节点都需要，可以提前做好。</p><p>在 <code>Elasticsearch</code> 的 <code>plugins/search-guard-5/sgconfig</code> 目录下，配置好 3 个与权限相关的文件，密码密文由 <code>hash.sh</code> 工具转换：</p><ul><li><code>sg_internal_users.yml</code>，用户定义，指定用户名、密码 </li><li><code>sg_roles.yml</code>，角色定义，用来限制权限，指定 2 种角色</li><li><code>sg_roles_mapping.yml</code>，映射关系，指定用户所属的角色，即完成真正的用户权限管理</li></ul><p> 这里使用的是默认的内部认证，即 <code>basic_internal_auth_domain</code>，在 <code>sgconfig/sg_config.yml</code> 中默认配置，如果使用其它认证方式【例如：<code>ODAP</code>、<code>kerberos_auth_domain</code>】，自行更改。</p><p>3、激活 <code>Search Guard</code>，只需要选择某 1 个 <code>Elasticsearch</code> 节点，并且需要前面 <code>client-admin</code> 客户端 2 个证书，在 <code>Elasticsearch</code> 集群重启后，在 <code>Elasticsearch</code> 节点的 <code>plugins/search-guard-5/tools</code> 目录下，执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./sgadmin.sh -icl -nhnv -h dev4 -p 9300 -cd ../sgconfig/-cacert ../../../config/root-ca.pem -cert ../../../config/client-admin.pem -key ../../../config/client-admin.key -keypass JzgDTQIzoTDE</span><br></pre></td></tr></table></figure><p>注意各个参数的含义、取值，提前准备好即可：</p><ul><li>-h，主机名，默认 <code>localhost</code>，指定 1 个 <code>Elasticsearch</code> 节点即可，用于 <code>tcp</code> 通信 </li><li>-p，端口号，不是 <code>http</code>，是 <code>tcp</code> 的，默认 9300</li><li>-icl，忽略集群名字，不会严格校验</li><li>-nhnv，忽略主机名验证</li><li>-keypass，客户端密码，从 <code>client-admin</code> 客户端的 <code>readme</code> 文件中找</li><li>-cd，配置文件目录</li><li>-arc，接受红色状态的集群</li><li>-dci，删除 <code>searchguard</code> 索引，用于激活失败重新激活时删除已经创建的索引</li><li>-cn，集群名字</li><li>-sniff，嗅探节点</li><li>-er，设置索引副本数，默认自动扩展</li><li>-era，开启索引的副本自动扩展，<code>auto_expand_replicas</code></li><li>-dra，关闭索引的副本自动扩展，<code>auto_expand_replicas</code></li></ul><p> 备注 <code>Windows</code> 系统的操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.\sgadmin.bat -icl -nhnv -cd ..\sgconfig\ -cacert ..\..\..\config\root-ca.pem -cert ..\..\..\config\client-admin.pem -key ..\..\..\config\client-admin.key -keypass xx</span><br></pre></td></tr></table></figure><p>重启 <code>Elasticsearch</code> 节点后，可以看到 <code>Elasticsearch</code> 节点日志中，一直在提示执行 <code>sgsdmin.sh</code> 激活 <code>Search Guard</code> 插件：<code>Not yet initialized (you may need to run sgadmin)</code>。</p><p>如果此时有客户端访问 <code>Elasticsearch</code> 节点，会被拒绝：<code>speaks transport plaintext instead of ssl, will close the channel</code>。</p><p>整体日志内容截取如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[2020-04-24T22:03:56,500][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44120) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:03:56,956][ERROR][c.f.s.a.BackendRegistry] Not yet initialized (you may need to run sgadmin)</span><br><span class="line">[2020-04-24T22:03:58,548][ERROR][c.f.s.a.BackendRegistry] Not yet initialized (you may need to run sgadmin)</span><br><span class="line">[2020-04-24T22:03:58,803][ERROR][c.f.s.a.BackendRegistry] Not yet initialized (you may need to run sgadmin)</span><br><span class="line">[2020-04-24T22:04:00,674][ERROR][c.f.s.a.BackendRegistry] Not yet initialized (you may need to run sgadmin)</span><br><span class="line">[2020-04-24T22:04:01,573][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44160) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:02,502][ERROR][c.f.s.a.BackendRegistry] Not yet initialized (you may need to run sgadmin)</span><br><span class="line">[2020-04-24T22:04:06,586][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44190) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:06,811][ERROR][c.f.s.a.BackendRegistry] Not yet initialized (you may need to run sgadmin)</span><br><span class="line">[2020-04-24T22:04:10,062][ERROR][c.f.s.a.BackendRegistry] Not yet initialized (you may need to run sgadmin)</span><br><span class="line">[2020-04-24T22:04:11,592][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44206) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:12,210][ERROR][c.f.s.a.BackendRegistry] Not yet initialized (you may need to run sgadmin)</span><br><span class="line">[2020-04-24T22:04:16,600][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44250) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:21,609][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44284) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:26,617][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44308) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:31,624][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44340) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:36,632][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44370) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:41,639][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44388) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:46,647][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44422) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:51,656][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44462) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:04:56,666][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44488) speaks transport plaintext instead of ssl, will close the channel</span><br><span class="line">[2020-04-24T22:05:01,624][ERROR][c.f.s.a.BackendRegistry] Not yet initialized (you may need to run sgadmin)</span><br><span class="line">[2020-04-24T22:05:01,677][WARN][c.f.s.s.t.SearchGuardSSLNettyTransport] [dev6] Someone (/192.168.1.62:44524) speaks transport plaintext instead of ssl, will close the channel</span><br></pre></td></tr></table></figure><p>由于 <code>Search Guard</code> 需要创建自己的索引，如果关闭了自动分配，新创建的索引从分片可能无法分配，<code>Search Guard</code> 激活过程会卡住，可以选择 <code>-esa</code> 参数，如果集群分片很多，需要开启，否则 <code>SearchGuard</code> 的主分片等待初始化需要很久。如果 <code>Elasticsearch</code> 集群默认是开启了自动分配，无需关心此问题。</p><p>通过测试发现 <code>Search Guard</code> 开启了分片自动扩展，实际初始时只有 1 个主分片，可以分配成功。但是注意副本数太多，自动扩展【<code>index.auto_expand_replicas</code> 设置为 <code>0-all</code>】是根据可用 <code>Elasticsearch</code> 节点来设置副本数的，实际中没必要【设置为 <code>false</code>】，并且把副本数减小【参数 <code>index.number_of_replicas</code>】，例如设置为 2 或者 3。</p><p>集群黄色的时候创建索引无法成功，一直在等待【大概率是因为测试环境的分片太多，加上刚刚重启，分配太慢了，等了至少 20 分钟还在等待，看 <code>Elasticsearch</code> 节点的日志，<code>put mapping</code> 超时】，后来等集群绿色的时候很快创建成功。</p><p>顺利执行 <code>sgadmin.sh</code> 激活插件的日志：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">Search Guard Admin v5</span><br><span class="line">Will connect to dev6:9300 ... done</span><br><span class="line"> </span><br><span class="line">### LICENSE NOTICE Search Guard ###</span><br><span class="line"> </span><br><span class="line">If you use one or more of the following features in production</span><br><span class="line">make sure you have a valid Search Guard license</span><br><span class="line">(See https://floragunn.com/searchguard-validate-license)</span><br><span class="line"> </span><br><span class="line">* Kibana Multitenancy</span><br><span class="line">* LDAP authentication/authorization</span><br><span class="line">* Active Directory authentication/authorization</span><br><span class="line">* REST Management API</span><br><span class="line">* JSON Web Token (JWT) authentication/authorization</span><br><span class="line">* Kerberos authentication/authorization</span><br><span class="line">* Document- and Fieldlevel Security (DLS/FLS)</span><br><span class="line">* Auditlogging</span><br><span class="line"> </span><br><span class="line">In case of any doubt mail to &lt;sales@floragunn.com&gt;</span><br><span class="line">###################################</span><br><span class="line">Elasticsearch Version: 5.6.8</span><br><span class="line">Search Guard Version: 5.6.8-19.1</span><br><span class="line">Contacting elasticsearch cluster &apos;elasticsearch&apos; and wait for YELLOW clusterstate ...</span><br><span class="line">Clustername: dev_es_cluster</span><br><span class="line">Clusterstate: GREEN</span><br><span class="line">Number of nodes: 3</span><br><span class="line">Number of data nodes: 3</span><br><span class="line">searchguard index does not exists, attempt to create it ... done (0-all replicas)</span><br><span class="line">Populate config from /opt/package/elasticsearch-dev_es_cluster/plugins/search-guard-5/sgconfig</span><br><span class="line">Will update &apos;config&apos; with ../sgconfig/sg_config.yml</span><br><span class="line">   SUCC: Configuration for &apos;config&apos; created or updated</span><br><span class="line">Will update &apos;roles&apos; with ../sgconfig/sg_roles.yml</span><br><span class="line">   SUCC: Configuration for &apos;roles&apos; created or updated</span><br><span class="line">Will update &apos;rolesmapping&apos; with ../sgconfig/sg_roles_mapping.yml</span><br><span class="line">   SUCC: Configuration for &apos;rolesmapping&apos; created or updated</span><br><span class="line">Will update &apos;internalusers&apos; with ../sgconfig/sg_internal_users.yml</span><br><span class="line">   SUCC: Configuration for &apos;internalusers&apos; created or updated</span><br><span class="line">Will update &apos;actiongroups&apos; with ../sgconfig/sg_action_groups.yml</span><br><span class="line">   SUCC: Configuration for &apos;actiongroups&apos; created or updated</span><br><span class="line">Done with success</span><br></pre></td></tr></table></figure><p>4、后续修改权限，如果后续需要新增帐号、变更密码等操作，直接更新 <code>Search Guard</code> 的 3 个配置文件，重新执行一次激活步骤即可。</p><p>也就说 <code>Search Guard</code> 插件安装激活成功后，帐号权限可以使用 <code>sgadmin.sh</code> 管理，通过更新配置文件，支持添加、删除角色、帐号。</p><p>5、开启 <code>Elasticsearch</code> 重分配，在重启 <code>Elasticsearch</code> 集群恢复绿色之后再执行，如果集群默认是开启的，会自动开启，无需关心。</p><p>配置更新：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">PUT /_cluster/settings/</span><br><span class="line">&#123;</span><br><span class="line">    &quot;transient&quot;: &#123;</span><br><span class="line">        &quot;cluster.routing.allocation.enable&quot;: &quot;all&quot;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="简单验证"><a href="# 简单验证" class="headerlink" title="简单验证"></a>简单验证 </h3><p> 可以打开集群详情页面：<code>http://dev4:9200</code> ，提示需要输入用户名、密码。</p><p>或者打开 <code>Search Guard</code> 帐号详情页面进行简单验证：<code>http://dev4:9200/_searchguard/authinfo</code> ，提示需要输入用户名、密码查看帐号信息。</p><p>启动后，如果没有激活 <code>Search Guard</code>，无法查看 <code>Elasticsearch</code> 集群的状态，集群详情页面显示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Search Guard not initialized (SG11). See https://github.com/floragunncom/search-guard-docs/blob/master/sgadmin.md</span><br></pre></td></tr></table></figure><p>只要登录后，此时使用 <code>head</code> 插件或者 <code>sense</code> 工具都可以正常访问。</p><h3 id="部分配置信息"><a href="# 部分配置信息" class="headerlink" title="部分配置信息"></a>部分配置信息 </h3><p> 提示：</p><p>1、<code>network.host: 0.0.0.0</code>，避免执行 <code>sgadmin.sh</code> 命令时报错。</p><p>2、<code>xpack.security.enabled: false</code>，禁用 <code>xpack</code>。</p><p>3、<code>es-head</code> 支持：<code>http://[es-head]:9100/?base_uri=https://[es-node]:9200&amp;auth_user=xx&amp;auth_password=yy</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">http.cors.enabled: true</span><br><span class="line">http.cors.allow-origin: &quot;*&quot;</span><br><span class="line">http.cors.allow-headers: &quot;Authorization,X-Requested-With,-Content-Length,Content-Type&quot;</span><br></pre></td></tr></table></figure><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 以下配置文件信息仅供参考，实际部署时，<code>Elasticsearch</code> 配置信息随证书而生成，直接复制即可，权限配置信息由实际情况而定。</p><p>参考链接：<a href="https://docs.search-guard.com/v5/internal-users-database" target="_blank" rel="noopener">internal-users-database</a> 。</p><h2 id="用户配置"><a href="# 用户配置" class="headerlink" title="用户配置"></a>用户配置 </h2><p><code>sg_internal_users.yml</code> 为用户信息配置，包含用户名、密码及角色【这里的角色是后台角色，不是 <code>sg</code> 角色，目前用不到，不用配置，以最终的 <code>sg_roles_mapping.yml</code> 为准】。</p><p> 密码 <code>hash</code> 使用自带的脚本生成：<code>./tools/hasher.sh -p mycleartextpassword</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">admin:</span><br><span class="line">    hash: $2a$12$qDsJtWx/IIkqhOVZXKh4M.bBLpjBmG6tL00vNhsfb4WS6wH7M1M3C</span><br><span class="line">    #password is: admin-!#%</span><br><span class="line">    #roles:</span><br><span class="line">    #    - admin</span><br><span class="line">    #    - can-read-all</span><br><span class="line">    #    - can-write-all</span><br><span class="line"></span><br><span class="line">custom:</span><br><span class="line">    hash: $2a$12$URJTPgsK9v7iYcq/dAYJVeH2t/VftkoHr2DraNnYS/ooqW3sZrJhS</span><br><span class="line">    #password is: custom-$@~</span><br><span class="line">    #roles:</span><br><span class="line">    #    - custom</span><br><span class="line">    #    - can-read-all</span><br></pre></td></tr></table></figure><h2 id="角色配置"><a href="# 角色配置" class="headerlink" title="角色配置"></a>角色配置 </h2><p><code>sg_roles.yml</code> 为角色权限配置【定义 2 种角色】，可自定义角色名及其权限。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">admin:</span><br><span class="line">    cluster:</span><br><span class="line">        - UNLIMITED</span><br><span class="line">    indices:</span><br><span class="line">        &apos;*&apos;:</span><br><span class="line">            &apos;*&apos;:</span><br><span class="line">                - UNLIMITED</span><br><span class="line"></span><br><span class="line">custom:</span><br><span class="line">    cluster:</span><br><span class="line">        - CLUSTER_MONITOR</span><br><span class="line">        - CLUSTER_COMPOSITE_OPS_RO</span><br><span class="line">        - indices:data/read/scroll*</span><br><span class="line">    indices:</span><br><span class="line">        &apos;*&apos;:</span><br><span class="line">            &apos;*&apos;:</span><br><span class="line">                - READ</span><br><span class="line">                - SEARCH</span><br></pre></td></tr></table></figure><h2 id="关联权限配置"><a href="# 关联权限配置" class="headerlink" title="关联权限配置"></a> 关联权限配置 </h2><p><code>sg_roles_mapping.yml</code> 角色、用户的映射【2 个用户分属于 2 种角色】，必须在这里配置映射，只在第一个 <code>sg_internal_users.yml</code> 文件配置用户的后台角色不生效。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">admin:</span><br><span class="line">    users:</span><br><span class="line">        - admin</span><br><span class="line"></span><br><span class="line">custom:</span><br><span class="line">    users:</span><br><span class="line">        - custom</span><br></pre></td></tr></table></figure><h2 id="Elasticsearch- 配置"><a href="#Elasticsearch- 配置" class="headerlink" title="Elasticsearch 配置"></a>Elasticsearch 配置</h2><p> 在 <code>elasticsearch.yml</code> 中增加以下与 <code>Search Guard</code> 相关的配置，以 <code>dev4</code> 作为示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">searchguard.ssl.transport.pemcert_filepath: dev4.pem</span><br><span class="line">searchguard.ssl.transport.pemkey_filepath: dev4.key</span><br><span class="line">searchguard.ssl.transport.pemkey_password: 9NdKF2PBoU8A</span><br><span class="line">searchguard.ssl.transport.pemtrustedcas_filepath: root-ca.pem</span><br><span class="line">searchguard.ssl.transport.enforce_hostname_verification: false</span><br><span class="line">searchguard.ssl.transport.resolve_hostname: false</span><br><span class="line">searchguard.ssl.http.enabled: false</span><br><span class="line">searchguard.ssl.http.pemcert_filepath: dev4_http.pem</span><br><span class="line">searchguard.ssl.http.pemkey_filepath: dev4_http.key</span><br><span class="line">searchguard.ssl.http.pemkey_password: YVI8mGC654TQ</span><br><span class="line">searchguard.ssl.http.pemtrustedcas_filepath: root-ca.pem</span><br><span class="line">searchguard.nodes_dn:</span><br><span class="line">- CN=dev4.playpi.com,OU=Ops,O=Playpi Com\, Inc.,DC=playpi,DC=com</span><br><span class="line">- CN=dev5.playpi.com,OU=Ops,O=Playpi Com\, Inc.,DC=playpi,DC=com</span><br><span class="line">- CN=dev6.playpi.com,OU=Ops,O=Playpi Com\, Inc.,DC=playpi,DC=com</span><br><span class="line">searchguard.authcz.admin_dn:</span><br><span class="line">- CN=client-admin.playpi.com,OU=Ops,O=Playpi Com\, Inc.,DC=playpi,DC=com</span><br></pre></td></tr></table></figure><h2 id="一些问题"><a href="# 一些问题" class="headerlink" title="一些问题"></a>一些问题 </h2><p>0、在线生成证书文件，<code>Search Guard</code> 也提供了在线生成证书文件的工具，见：<a href="https://search-guard.com/tls-certificate-generator" target="_blank" rel="noopener">tls-certificate-generator</a> ，但是如果 <code>Elasticsearch</code> 节点很多，配置也就多，还是通过离线工具自己生成比较方便，效果是一样的。</p><p>1、嗅探问题，可以同步开启，集群节点自动更新，避免单个 <code>Elasticsearch</code> 节点出问题出现超时异常。</p><p><code>TransportClient</code> 方式有参数 <code>client.transport.sniff</code> 对应，设置为 <code>true</code> 即可。</p><p><code>HTTP</code> 方式有 <code>Sniffer.builder ()</code> 方法，可以使用：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">RestClientBuilder builder = RestClient.builder (hosts);</span><br><span class="line">RestHighLevelClient restHighLevelClient = new RestHighLevelClient (builder);</span><br><span class="line">Sniffer sniffer = Sniffer.builder (restHighLevelClient.getLowLevelClient ()).build ();</span><br></pre></td></tr></table></figure><p>2、节点变更新增证书</p><p> 如果有 <code>Elasticsearch</code> 节点被移除，则可以直接移除。但是如果有 <code>Elasticsearch</code> 节点需要被添加进入集群，证书怎么生成？</p><p>也是可以的，即可以手动生成证书文件，但是要保留当前生成的 <code>ca</code> 授权中心，即第一次生成证书时指定 <code>-ca</code> 参数输出到 <code>out</code> 目录的 <code>root-xx</code> 这 3 个文件。都很重要，一定要保留【要把 <code>config</code>、<code>out</code> 目录保留，甚至整个 <code>search-guard-tlstool</code> 目录保留，以后可以直接使用】。</p><p>在 <code>config</code> 中，就是一些配置文件，很重要，在 <code>out</code> 中，其中 <code>root-ca.readme</code> 用来查看密码，很重要，<code>root-ca.pem</code>、<code>root-ca.key</code> 是秘钥文件，也很重要。</p><p>以后需要添加 <code>Elasticsearch</code> 节点时，需要申请证书，必须利用这个 <code>ca</code> 授权中心，否则生成的证书无法使用。</p><p>准备完成后，具体操作：</p><p>把 <code>root-ca.readme</code> 中的密码配置在 <code>config/tlsconfig.yml</code> 文件的 <code>ca -&gt; root -&gt; pkPassword</code> 值上面【表示用这个密码、<code>root-ca</code> 来生成新的证书，第一次使用时配置的是 <code>auto</code>】，然后根据 <code>Elasticsearch</code> 节点名称配置 <code>nodes</code> 项。</p><p>然后生成证书时，去掉 <code>-ca</code> 参数，则会默认使用本地的 <code>ca</code>，即 <code>out</code> 里面的 <code>root-ca</code>，它会自动对比配置中的 <code>nodes</code> 节点和 <code>out</code> 目录中以前的证书文件，如果存在则跳过，不存在时会生成【即为新 <code>Elasticsearch</code> 节点生成可用的证书】。</p><p>备注 <code>Windows</code> 操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">.\tools\sgtlstool.bat -c .\config\tlsconfig.yml -crt</span><br><span class="line"></span><br><span class="line"> 输出日志中会显示跳过了什么，生成了什么。</span><br><span class="line"></span><br><span class="line">xx.key does already exist. Skipping creation of certificate for yy</span><br><span class="line">...</span><br><span class="line">...</span><br><span class="line">Created 2 node certificates.</span><br><span class="line">Passwords for the private keys of the node certificates have been auto-generated.</span><br><span class="line">The passwords are stored in the config snippet files.</span><br></pre></td></tr></table></figure><p>3、单物理机多节点证书问题 </p><p> 开发环境中的 <code>Elasticsearch</code> 节点是一台物理机上有 2 个 <code>Elasticsearch</code> 节点，它们的节点名称不一样，但是 <code>ip</code> 是一样的，这种仍旧需要生成 2 份证书【每个 <code>Elasticsearch</code> 节点 1 份】，配置时全部使用 <code>Elasticsearch</code> 节点的名字来配置，多个 <code>node</code> 的 <code>ip</code> 地址可以一样。</p><p>4、用户更新 </p><p> 以后如果需要变更 <code>SaerchGuard</code> 的用户，例如新增、删除、添加权限，不需要重启 <code>Elasticsearch</code> 集群了，直接更改与权限相关的那几个配置文件就行，然后使用 <code>./sgadmin.sh</code> 工具重新激活一次即可。</p><p>5、<code>TransportClient</code> 方式使用起来比较麻烦，需要证书文件，以及很多配置【类似于 <code>Search Guard</code> 在 <code>Elasticsearch</code> 中的那些配置】，本质是通过 <code>tcp</code> 与 <code>Elasticsearch</code> 进行连接【所以不需要密码了，证书已经表明了合法用户】。详细使用方式以及权限管理参考：<a href="https://search-guard.com/searchguard-elasicsearch-transport-clients" target="_blank" rel="noopener">transport-clients</a> 。</p><p>条件描述：</p><blockquote><p>The Transport Client needs to identify itself against the cluster by sending a trusted TLS certificate<br>For that, you need to specify the location of your keystore and truststore containing the respective certificates<br>A role with appropriate permissions has to be configured in Search Guard, either based on the hostname of the client, or the DN of the certificate</p></blockquote><p>当然，集群层面也需要开启相应的配置。</p><p>首先在配置文件 <code>sgconfig/sg_config.yml</code> 中开启认证方式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">transport_auth_domain:</span><br><span class="line">  enabled: true</span><br><span class="line">  order: 2</span><br><span class="line">  http_authenticator:</span><br><span class="line">  authentication_backend:</span><br><span class="line">    type: internal</span><br></pre></td></tr></table></figure><p>其次需要证书完整的 DN 信息，配置在 <code>sgconfig/sg_internal_users.yml</code> 文件中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CN=root-ca.playpi.com, OU=Ops, O=&quot;Playpi Com, Inc.&quot;, DC=playpi, DC=com</span><br><span class="line">    hash: $2a$12$1HqHxm3QTfzwkse7vwzhFOV4gDv787cZ8BwmCwNEyJhn0CZoo8VVu</span><br></pre></td></tr></table></figure><p>当然，如果忘记了 <code>DN</code> 信息，可以使用 <code>Java</code> 自带的工具获取：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">keytool -printcert -file ./config/client-custom.pem</span><br></pre></td></tr></table></figure><p>同样，需要在角色映射文件 <code>sgconfig/sg_roles_mapping.yml</code> 中配置：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">custom:</span><br><span class="line">    users:</span><br><span class="line">        - custom</span><br><span class="line">        - &apos;root-ca.playpi.com, OU=Ops, O=&quot;Playpi Com, Inc.&quot;, DC=playpi, DC=com&apos;</span><br><span class="line"></span><br><span class="line"> 注意单引号的使用 </span><br></pre></td></tr></table></figure><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;最近 &lt;code&gt;Elasticsearch&lt;/code&gt; 集群出了一个小事故，根本原因在于对集群请求的监控不完善，以及对 &lt;code&gt;Elasticsearch&lt;/code&gt; 访问权限无监控【目前是使用 &lt;code&gt;LDAP&lt;/code&gt; 账号就可以登录访问，而且操作权限很大，这有很大的隐患】。因此，最近准备上线 &lt;code&gt;Search Guard&lt;/code&gt;，先在测试环境部署安装了一遍，并测试了相关服务，整理了如下安装部署文档，以供读者参考。&lt;/p&gt;&lt;p&gt;开发环境基于 &lt;code&gt;Elasticsearch v5.6.8&lt;/code&gt;、&lt;code&gt;Search Guard v5.6.8-19.1&lt;/code&gt;、&lt;code&gt;Java v1.8&lt;/code&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="HTTP" scheme="https://www.playpi.org/tags/HTTP/"/>
    
      <category term="SearchGuard" scheme="https://www.playpi.org/tags/SearchGuard/"/>
    
      <category term="TLS" scheme="https://www.playpi.org/tags/TLS/"/>
    
  </entry>
  
  <entry>
    <title>在 Elasticsearch 中指定查询返回的字段</title>
    <link href="https://www.playpi.org/2020030201.html"/>
    <id>https://www.playpi.org/2020030201.html</id>
    <published>2020-03-02T12:03:44.000Z</published>
    <updated>2020-03-02T12:03:44.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:17 GMT+0800 (China Standard Time) --><p>在 <code>Elasticsearch</code> 中，有参数可以指定查询结果返回的字段，这样可以使查询结果更简约，看起来更清晰。如果是大批量 <code>scroll</code> 取数，还可以减少数据在网络中的传输，从而降低网络 <code>IO</code>。本文使用简单的查询来举例，演示环境基于 <code>Elasticsearch v5.6.8</code>。</p><a id="more"></a><h1 id="演示"><a href="# 演示" class="headerlink" title="演示"></a>演示 </h1><p> 我的演示环境里面有一个索引 <code>my-index-user</code>，里面是用户的信息，字段有姓名、年龄、性别、城市等。</p><p>现在我根据用户 <code>id</code> 查询数据，使用 <code>_source</code> 参数指定返回 4 个字段：<code>item_id</code>、<code>gender</code>、<code>city</code>、<code>birthday</code>。</p><p>查询条件：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">POST my-index-user/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;terms&quot;: &#123;</span><br><span class="line">      &quot;item_id&quot;: [</span><br><span class="line">        &quot;63639783663&quot;,</span><br><span class="line">        &quot;59956667929&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;_source&quot;: [</span><br><span class="line">    &quot;item_id&quot;,</span><br><span class="line">    &quot;gender&quot;,</span><br><span class="line">    &quot;city&quot;,</span><br><span class="line">    &quot;birthday&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>查询结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;took&quot;: 2,</span><br><span class="line">  &quot;timed_out&quot;: false,</span><br><span class="line">  &quot;_shards&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 3,</span><br><span class="line">    &quot;successful&quot;: 3,</span><br><span class="line">    &quot;skipped&quot;: 0,</span><br><span class="line">    &quot;failed&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;hits&quot;: &#123;</span><br><span class="line">    &quot;total&quot;: 2,</span><br><span class="line">    &quot;max_score&quot;: 7.937136,</span><br><span class="line">    &quot;hits&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">        &quot;_type&quot;: &quot;user&quot;,</span><br><span class="line">        &quot;_id&quot;: &quot;23c659fde1a2c02b3618eaa92fcd7106&quot;,</span><br><span class="line">        &quot;_score&quot;: 7.937136,</span><br><span class="line">        &quot;_source&quot;: &#123;</span><br><span class="line">          &quot;birthday&quot;: &quot;1994-01-01&quot;,</span><br><span class="line">          &quot;city&quot;: &quot; 成都 & quot;,</span><br><span class="line">          &quot;item_id&quot;: &quot;63639783663&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;_index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">        &quot;_type&quot;: &quot;user&quot;,</span><br><span class="line">        &quot;_id&quot;: &quot;75e3db1f4ab288d38de3ab80bfba8ecd&quot;,</span><br><span class="line">        &quot;_score&quot;: 7.937136,</span><br><span class="line">        &quot;_source&quot;: &#123;</span><br><span class="line">          &quot;birthday&quot;: &quot;1982-01-01&quot;,</span><br><span class="line">          &quot;gender&quot;: &quot;1&quot;,</span><br><span class="line">          &quot;city&quot;: &quot; 渭南 & quot;,</span><br><span class="line">          &quot;item_id&quot;: &quot;59956667929&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200302205417.png" alt="查询结果指定字段" title="查询结果指定字段"></p><p>可以看到，查到的数据只返回了 4 个字段。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 除了 <code>_source</code> 参数外，还有其它的参数也可以达到同样的效果，在 <code>v2.4</code> 以及之前的版本，可以使用 <code>fields</code> 参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">POST my-index-user/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;terms&quot;: &#123;</span><br><span class="line">      &quot;item_id&quot;: [</span><br><span class="line">        &quot;63639783663&quot;,</span><br><span class="line">        &quot;59956667929&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;fields&quot;: [</span><br><span class="line">    &quot;user_name&quot;,</span><br><span class="line">    &quot;item_id&quot;,</span><br><span class="line">    &quot;gender&quot;,</span><br><span class="line">    &quot;city&quot;,</span><br><span class="line">    &quot;birthday&quot;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>下图是我找了一个低版本 <code>Elasticsearch</code> 集群测试了一下：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200302205758.png" alt="fields 参数过滤字段" title="fields 参数过滤字段"></p><p>不过在 <code>v5.x</code> 以及之后的版本不再支持这个参数：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200302205500.png" alt="不支持 fields 参数" title="不支持 fields 参数"></p><p>异常信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">The field [fields] is no longer supported, please use [stored_fields] to retrieve stored fields or _source filtering if the field is not stored</span><br></pre></td></tr></table></figure><p>注意这里提及的 <code>stored_fields</code> 参数用处有点鸡肋，还是需要 <code>_source</code> 参数配合。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:17 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在 &lt;code&gt;Elasticsearch&lt;/code&gt; 中，有参数可以指定查询结果返回的字段，这样可以使查询结果更简约，看起来更清晰。如果是大批量 &lt;code&gt;scroll&lt;/code&gt; 取数，还可以减少数据在网络中的传输，从而降低网络 &lt;code&gt;IO&lt;/code&gt;。本文使用简单的查询来举例，演示环境基于 &lt;code&gt;Elasticsearch v5.6.8&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="fields" scheme="https://www.playpi.org/tags/fields/"/>
    
      <category term="source" scheme="https://www.playpi.org/tags/source/"/>
    
      <category term="stored_fields" scheme="https://www.playpi.org/tags/stored-fields/"/>
    
  </entry>
  
  <entry>
    <title>在技术之路上的一些小技巧</title>
    <link href="https://www.playpi.org/2020021301.html"/>
    <id>https://www.playpi.org/2020021301.html</id>
    <published>2020-02-12T17:26:47.000Z</published>
    <updated>2020-02-12T17:26:47.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>本文记录一些工作当中常用的小工具、小技巧，有时候自己可以查看备忘，同时也给读者参考。当然，内容是在不断补充的，涉及的维度也会扩展。</p><a id="more"></a><h1 id="Python"><a href="#Python" class="headerlink" title="Python"></a>Python</h1><p><code>Python</code> 是一种脚本语言，想要运行 <code>Python</code> 脚本就需要先安装 <code>Python</code> 环境【就像运行 <code>Java</code> 程序需要 <code>JRE</code> 一样】，安装后别忘记配置环境变量。</p><p>官方网站：<a href="https://www.python.org/downloads" target="_blank" rel="noopener">python</a> 。</p><p>唯一需要注意的是安装时先核对脚本的语法，是安装 <code>v2.7.X</code> 版本还是 <code>v3.X</code> 版本，它们的语法有时候不兼容，导致脚本无法跑通。</p><h2 id="虚拟环境"><a href="# 虚拟环境" class="headerlink" title="虚拟环境"></a>虚拟环境 </h2><p> 在某个项目中，使用 <code>Python</code> 构造 <code>Elasticsearch</code> 请求获取结果数据，但是脚本的内容是针对 <code>Python v3.X</code> 环境的，而服务器上是 <code>Python v2.7</code> 版本的环境，而且个人没有权限更改【如果自己通过编译安装指定的版本，后续使用时如果缺失第三方模块还是需要手动安装，如果又引发模块之间的依赖，手动安装过程会崩溃的】，所以可以在服务器上面虚拟一个指定版本的 <code>Python</code> 环境出来。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">-- 在任意目录，虚拟出 Python 2.7 环境 </span><br><span class="line">virtualenv py2.7</span><br><span class="line">-- 进入环境 </span><br><span class="line">cd py2.7/</span><br><span class="line">-- 激活环境 </span><br><span class="line">source bin/activate</span><br><span class="line">-- 如果缺失第三方模块，直接安装即可 </span><br><span class="line">pip install requests</span><br><span class="line">-- 取消环境 </span><br><span class="line">source bin/deactive</span><br></pre></td></tr></table></figure><h2 id="缓存问题"><a href="# 缓存问题" class="headerlink" title="缓存问题"></a>缓存问题 </h2><p> 假如跑一个脚本，把输出重定向到一个文件中：<code>nohup python test.py &gt; nohup.out 2&gt;&amp;1 &amp;</code>，结果发现 <code>nohup.out</code> 中显示不出来 <code>python</code> 程序中 <code>print</code> 的内容【或者是不及时显示，隔一段时间才会有一点内容】。</p><p>这是因为 <code>python</code> 的输出有缓冲机制，导致我们从 <code>nohup.out</code> 中并不能够马上看到输出。其实运行 <code>python</code> 脚本的时候有个 <code>-u</code> 参数，使得 <code>python</code> 不启用缓冲，把程序中的输出内容实时输出到文件：<code>nohup python -u test.py &gt; nohup.out 2&gt;&amp;1 &amp;</code> 。</p><h2 id="编码问题"><a href="# 编码问题" class="headerlink" title="编码问题"></a>编码问题 </h2><p><code>python</code> 的 <code>chardet</code> 模块，可以查看字符串的编码：<code>chardet.detect (data)</code> 。</p><h2 id="处理文本文件"><a href="# 处理文本文件" class="headerlink" title="处理文本文件"></a> 处理文本文件 </h2><p> 在处理 <code>csv</code> 文件时，遇到异常：<code>Error: field larger than field limit &lt;131072&gt;</code>，这其实是因为 <code>Python</code> 读取 <code>csv</code> 文件时有一个默认设置，某列的内容最大长度为 131072【基于 <code>Python v2.5</code>】。</p><p>重点在于 <code>Error: field larger than field limit &lt;131072&gt;</code>，是因为 <code>csv</code> 文件中的某个字段的长度大于 <code>csv</code> 框架能处理的最大长度，需要在文件头加上参数配置代码：<code>csv.field_size_limit (sys.maxint)</code> ，这样就可以支持 <code>int</code> 的最大值所对应的长度了。</p><p>此外，由于 <code>csv</code> 文件的特殊性，<code>csv</code> 是特定分隔符分隔的文本文件，可能会导致内容混乱的状况，比如某一列包含了大量的换行符【又没有使用双引号封装起来】，会导致 <code>csv</code> 文件认为就是多行数据，这样这些行都是错误的数据【因为列数不够】，如果有连续的换行符，会导致空白行出现，<code>Python</code> 也会报错：<code>line contains NULL byte</code> 。</p><h1 id="Excel- 的知识"><a href="#Excel- 的知识" class="headerlink" title="Excel 的知识"></a>Excel 的知识 </h1><p> 对于分析人员，经常会用到 <code>Excel</code> 工具，用来看数据，或者操作数据。</p><p>但是，有时候文本内容过多，不再适合使用 <code>Excel</code> 操作了，不仅很卡，而且有崩溃的风险。</p><p>根据我的经验，以最多 5 万行内容，最大 <code>20MB</code> 为好，如果是 <code>Mac OS</code> 系统，需要更少。</p><p>下面列举一下 <code>Excel</code> 支持的内容上限：</p><p><code>Excel</code> 2003 支持的最大行数是 65536，<code>Excel</code> 2007 支持的最大行数是 1048576 。</p><p><code>Excel</code> 2003 支持的最大列数是 256，<code>Excel</code> 2007 以及以上支持的最大列数是 16384 。</p><p>更多其它规范参考官方介绍：<a href="https://support.office.com/zh-cn/article/Excel-% E8% A7%84% E8%8C%83% E4% B8%8E% E9%99%90% E5%88% B6-1672b34d-7043-467e-8e27-269d656771c3#ID0EBABAAA=Office_2010" target="_blank" rel="noopener">Excel 规范与限制 </a> 。</p><h1 id="文件编码转换工具"><a href="# 文件编码转换工具" class="headerlink" title="文件编码转换工具"></a> 文件编码转换工具 </h1><p><code>iconv</code> 需要单独安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-- 转换文件内容编码，从 gb18030 转到 utf-8</span><br><span class="line">iconv --list 查看支持的编码 </span><br><span class="line">iconv -f gb18030 -t utf-8 oldfile -o newfile</span><br><span class="line"></span><br><span class="line">-- 查看文件编码内容简要信息 </span><br><span class="line">file filename</span><br></pre></td></tr></table></figure><h1 id="文件名编码转换工具"><a href="# 文件名编码转换工具" class="headerlink" title="文件名编码转换工具"></a> 文件名编码转换工具 </h1><p> 在 <code>Linux</code> 系统中，如果把一个以中文命名的文件上传上去【特别是从 <code>Windows</code> 系统上传上去】，文件名可能会乱码，导致无法使用 <code>Shell</code> 操作文件，此时可以使用 <code>xx</code> 工具转换文件名的编码。</p><p><code>convmv</code> 需要单独安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">-- 转换文件名编码，从 utf-8 转到 gbk</span><br><span class="line">convmv --list 查看支持的编码 </span><br><span class="line">convmv -f utf-8 -t gbk filename 显示效果 </span><br><span class="line">convmv -f utf-8 -t gbk --notest filename 真的转换 </span><br><span class="line"></span><br><span class="line">-- 查看文件编码内容简要信息 </span><br><span class="line">file filename</span><br></pre></td></tr></table></figure><h1 id="常用命令"><a href="# 常用命令" class="headerlink" title="常用命令"></a>常用命令 </h1><h2 id="Shell"><a href="#Shell" class="headerlink" title="Shell"></a>Shell</h2><h3 id="进程目录"><a href="# 进程目录" class="headerlink" title="进程目录"></a> 进程目录 </h3><p> 根据进程编号找到启动进程的目录，从而判断进程的启动路径。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pwdx pid</span><br></pre></td></tr></table></figure><h3 id="负载查看"><a href="# 负载查看" class="headerlink" title="负载查看"></a>负载查看 </h3><p> 对于操作系统上面的进程导致的机器负载升高，可以查看到底是哪个进程或者线程导致的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">xxx</span><br><span class="line">yyy</span><br></pre></td></tr></table></figure><h3 id="进程查看"><a href="# 进程查看" class="headerlink" title="进程查看"></a>进程查看 </h3><p> 查看某个进程的启动目录：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-- 筛选进程名称，获取 pid</span><br><span class="line">jps | grep &apos;process name&apos;</span><br><span class="line">-- 根据 pid 查看目录 </span><br><span class="line">pwdx &apos;pid&apos;</span><br><span class="line">-- 进入目录 </span><br><span class="line">cd &apos;dir&apos;</span><br></pre></td></tr></table></figure><h3 id="使用 -sed- 操作文本文件"><a href="# 使用 -sed- 操作文本文件" class="headerlink" title="使用 sed 操作文本文件"></a>使用 sed 操作文本文件 </h3><p> 搜索替换之类的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> 把文件中的 xxx 全部替换为 yyy，不带 -i 参数直接输出内容，不会更改文件 </span><br><span class="line">sed -i &quot;s/xxx/yyy/g&quot; your_file</span><br><span class="line"></span><br><span class="line"> 在文件第 n 行前插入内容 xxx，不带 -i 参数直接输出内容，不会更改文件 </span><br><span class="line"> 注意 Linux 中以换行符 LF 作为一行的判断条件，所以指定不存在的行号时无法插入数据 </span><br><span class="line">sed -i &quot;ni\xxx&quot; your_file</span><br><span class="line"></span><br><span class="line"> 输出文件的第 n 到 m 行内容 </span><br><span class="line">sed -n &quot;n,mp&quot; your_file</span><br></pre></td></tr></table></figure><h3 id="查看 -Linux- 系统的发行版本"><a href="# 查看 -Linux- 系统的发行版本" class="headerlink" title="查看 Linux 系统的发行版本"></a>查看 Linux 系统的发行版本 </h3><p><code>lsb_release -a</code>，该命令适用于所有 <code>Linux</code> 系统，会显示出完整的版本信息，包括 <code>Linux</code> 系统的名称，如：<code>Debian</code>、<code>Ubuntu</code>、<code>CentOS</code> 等，和对应的版本号，以及该版本的代号，例如在 <code>Debian 8</code> 中将会显示代号 <code>jessie</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> 输出示例：</span><br><span class="line">No LSB modules are available.</span><br><span class="line">Distributor ID:Ubuntu</span><br><span class="line">Description:Ubuntu 14.04.6 LTS</span><br><span class="line">Release:14.04</span><br><span class="line">Codename:trusty</span><br></pre></td></tr></table></figure><p><code>cat /etc/issue</code>，该命令适用于所有的 <code>Linux</code> 系统，显示的版本信息较为简略，只有系统名称和对应的版本号。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> 输出示例：</span><br><span class="line">Ubuntu 14.04.6 LTS \n \l</span><br></pre></td></tr></table></figure><p><code>cat /etc/redhat-release</code>，该命令仅适用于 <code>Redhat</code> 系列的 <code>Linux</code> 系统，显示的版本信息也较为简略。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> 输出示例：</span><br><span class="line">CentOS release 6.10 (Final)</span><br></pre></td></tr></table></figure><p><code>uname -a</code>、<code>cat /proc/version</code>，可以查看 <code>Linux</code> 的内核版本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> 输出示例 1：</span><br><span class="line">Linux wufazhuce 3.13.0-32-generic #57-Ubuntu SMP Tue Jul 15 03:51:08 UTC 2014 x86_64 x86_64 x86_64 GNU/Linux</span><br><span class="line"></span><br><span class="line"> 输出示例 2：</span><br><span class="line">Linux version 3.13.0-32-generic (buildd@kissel) (gcc version 4.8.2 (Ubuntu 4.8.2-19ubuntu1) ) #57-Ubuntu SMP Tue Jul 15 03:51:08 UTC 2014</span><br></pre></td></tr></table></figure><h3 id="查看 -Linux- 系统的核数"><a href="# 查看 -Linux- 系统的核数" class="headerlink" title="查看 Linux 系统的核数"></a> 查看 Linux 系统的核数 </h3><p> 继续阅读之前先搞清楚几个概念：物理 <code>CPU</code> 个数、单颗 <code>CPU</code> 核数、单核 <code>CPU</code> 线程数。</p><ul><li>一台物理机的 <code>CPU</code> 个数 </li><li> 一个 <code>CPU</code> 的核数 </li><li> 一个 <code>CPU</code> 核上面支持的线程数 </li></ul><p> 当一台机器有多个物理 <code>CPU</code>，那么各个 <code>CPU</code> 之间就要通过总线进行通信，效率比较低。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200412175009.jpg" alt="多个 CPU 通过总线通信" title="多个 CPU 通过总线通信"></p><p>此时可以利用多核的特性，在多核 <code>CPU</code> 上，不同的核之间通过 <code>L2 cache</code> 进行通信，存储和外设通过总线与 <code>CPU</code> 通信，效率提高。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200412175024.jpg" alt="多核之间通信" title="多核之间通信"></p><p>当然，如果利用多核超线程，即每个 <code>CPU</code> 核有两个逻辑的处理单元，两个线程共同分享一个 <code>CPU</code> 核的资源，计算起来更快。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200412175032.jpg" alt="多核超线程" title="多核超线程"></p><p>因此，一台机器的总核数为：物理机的 <code>CPU</code> 个数 X 一个 <code>CPU</code> 的核数，总逻辑 <code>CPU</code> 个数为：物理机的 <code>CPU</code> 个数 X 一个 <code>CPU</code> 的核数 X 一个 <code>CPU</code> 核上面支持的线程数。</p><p>查看一台机器的 <code>CPU</code> 信息，主要就是 <code>/proc/cpuinfo</code> 文件，里面包含了关于 <code>CPU</code> 的信息，以 <code>CentOS</code> 系统为例，输出内容如下【只是截取了一小部分】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">processor: 23</span><br><span class="line">vendor_id: GenuineIntel</span><br><span class="line">cpu family: 6</span><br><span class="line">model: 62</span><br><span class="line">model name: Intel (R) Xeon (R) CPU E5-2620 v2 @ 2.10GHz</span><br><span class="line">stepping: 4</span><br><span class="line">microcode: 1060</span><br><span class="line">cpu MHz: 1200.000</span><br><span class="line">cache size: 15360 KB</span><br><span class="line">physical id: 1</span><br><span class="line">siblings: 12</span><br><span class="line">core id: 5</span><br><span class="line">cpu cores: 6</span><br><span class="line">apicid: 43</span><br><span class="line">initial apicid: 43</span><br><span class="line">fpu: yes</span><br><span class="line">fpu_exception: yes</span><br><span class="line">cpuid level: 13</span><br><span class="line">wp: yes</span><br><span class="line">flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon pebs bts rep_good xtopology nonstop_tsc aperfmperf pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 cx16 xtpr pdcm pcid dca sse4_1 sse4_2 x2apic popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm ida arat epb xsaveopt pln pts dtherm tpr_shadow vnmi flexpriority ept vpid fsgsbase smep erms</span><br><span class="line">bogomips: 4189.67</span><br><span class="line">clflush size: 64</span><br><span class="line">cache_alignment: 64</span><br><span class="line">address sizes: 46 bits physical, 48 bits virtual</span><br><span class="line">power management:</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>其中，<code>model name</code> 表示 <code>CPU</code> 的型号；<code>physical id</code> 表示物理 <code>CPU</code> 个数，编号从 0 开始；<code>cpu cores</code> 表示单颗 <code>CPU</code> 核数；<code>processor</code> 表示所有 <code>CPU</code> 线程数，编号从 0 开始。</p><p>因此，查看 <code>CPU</code> 信息时可以使用管道、过滤、查找来输出需要的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> 查看 CPU 型号信息 </span><br><span class="line">cat /proc/cpuinfo | grep name | cut -f2 -d: | uniq -c</span><br><span class="line"></span><br><span class="line"> 查看物理 CPU 个数 </span><br><span class="line">cat /proc/cpuinfo | grep &quot;physical id&quot; | sort | uniq | wc -l</span><br><span class="line"></span><br><span class="line"> 查看单颗 CPU 核数 </span><br><span class="line">cat /proc/cpuinfo | grep &quot;cpu cores&quot; | uniq</span><br><span class="line"></span><br><span class="line"> 查看所有 CPU 线程数 </span><br><span class="line">cat /proc/cpuinfo | grep &quot;processor&quot; | wc -l</span><br></pre></td></tr></table></figure><p>顺带提一下查看内存的信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/meminfo</span><br></pre></td></tr></table></figure><h3 id="split- 拆分文件"><a href="#split- 拆分文件" class="headerlink" title="split 拆分文件"></a>split 拆分文件 </h3><p> 把文件按照 60 行一个文件进行拆分，拆分后的文件以 <code>data_</code> 作为前缀，以 2 位数字作为后缀：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">split -l 60 your_file -d -a 2 data_</span><br><span class="line"></span><br><span class="line"> 拆分后的文件为 data_00、data_01、data_02 等等 </span><br></pre></td></tr></table></figure><h3 id="curl- 请求网络"><a href="#curl- 请求网络" class="headerlink" title="curl 请求网络"></a>curl 请求网络 </h3><p> 可以模拟请求，传输参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST 请求，参数放在 seeds 中 </span><br><span class="line">curl --location --request POST &apos;url&apos; \</span><br><span class="line">--header &apos;content-type: application/x-www-form-urlencoded; charset=UTF-8&apos; \</span><br><span class="line">--header &apos;Content-Type: application/x-www-form-urlencoded&apos; \</span><br><span class="line">--data-urlencode &apos;seeds=[&#123;&quot;keyword&quot;:&quot; 王者荣耀 & quot;, &quot;TOP_PAGE&quot;: &quot;100&quot;&#125;,&#123;&quot;keyword&quot;:&quot;LOL&quot;, &quot;TOP_N&quot;: &quot;100&quot;&#125;]&apos;</span><br><span class="line"></span><br><span class="line">GET 请求，获取信息 </span><br><span class="line">curl --location --request GET url&apos;</span><br><span class="line"></span><br><span class="line">POST 请求，参数放在文件中 </span><br><span class="line">curl -X POST --data-binary @your_file url</span><br></pre></td></tr></table></figure><p>查询 <code>Elasticsearch</code> 的数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">-- 查询指定索引、指定类型的数据，返回 1 条数据，格式化结果 </span><br><span class="line">curl -X POST &apos;dev:9202/your_index/your_type/_search?pretty&apos; -d &apos;</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123; &quot;match_all&quot;: &#123;&#125; &#125;,&quot;size&quot;:1</span><br><span class="line">&#125;&apos;</span><br></pre></td></tr></table></figure><h3 id="文本文件行数"><a href="# 文本文件行数" class="headerlink" title="文本文件行数"></a>文本文件行数 </h3><p> 获取文本文件的内容行数，返回数值：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wc -l 输入文件名称 </span><br></pre></td></tr></table></figure><h3 id="文件去重"><a href="# 文件去重" class="headerlink" title="文件去重"></a>文件去重 </h3><p> 对单个文本文件的内容进行去重，返回去重后的文本：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sort 输入文件 | uniq &gt; 输出文件 </span><br></pre></td></tr></table></figure><p>说明：<code>|</code> 竖线表示管道，把前一个命令的输出作为后一个命令的输入，<code>&gt;</code> 右尖括号表示重定向，把前一个命令的输出内容导出到文件中，<code>sort</code> 表示排序，但是并不会去除重复内容，所以会有重复的行，<code>uniq</code> 表示去除重复的内容，则输出结果就是去重后的内容。</p><h3 id="随机获取文本内容"><a href="# 随机获取文本内容" class="headerlink" title="随机获取文本内容"></a>随机获取文本内容 </h3><p> 随机抽取单个文件中的内容行数，例如随机输出 100 行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shuf -n 行数 输入文件 &gt; 输出文件 </span><br></pre></td></tr></table></figure><p>说明：<code>-n</code> 后面紧跟着数字，表示行数，不需要空格。</p><h3 id="计算交集差集并集"><a href="# 计算交集差集并集" class="headerlink" title="计算交集差集并集"></a>计算交集差集并集 </h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat 输入文件 1 输入文件 2 | sort | uniq -d &gt; 输出文件 </span><br></pre></td></tr></table></figure><p> 说明：<code>cat</code> 表示查看文件内容，<code>uniq</code> 的 <code>-d</code> 参数表示抽取重复的内容，即出现 2 次及以上的内容，在这里得到的结果也就是交集了。</p><p>此外，还有 <code>-u</code> 参数表示抽取唯一的内容，则可以计算差集。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat 输入文件 1 输入文件 2 输入文件 2 | sort | uniq -u &gt; 输出文件（注意输入文件 2 故意重复 2 次）</span><br></pre></td></tr></table></figure><p>说明：<code>uniq</code> 的 <code>-u</code> 参数表示抽取唯一的内容，即只出现一次的内容，这样得到的输出结果必然是只在输入文件 1 中，而不在输入文件 2 中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat 输入文件 1 输入文件 2 | sort | uniq &gt; 输出文件 </span><br></pre></td></tr></table></figure><p>说明：这个很容易理解了，合并文件后简单去重，也就是并集了。</p><h2 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h2><p>查看 <code>topic</code> 信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --list --zookeeper dev:2181</span><br></pre></td></tr></table></figure><p>生产数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-producer.sh --topic your_topic --broker-list dev:9092</span><br></pre></td></tr></table></figure><p>消费数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-console-consumer.sh --zookeeper dev:2181 --topic your_topic</span><br></pre></td></tr></table></figure><p>查看 <code>topic</code> 状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-topics.sh --zookeeper dev:2181 --describe --topic your_topic</span><br></pre></td></tr></table></figure><p>查看消费情况【消费组消费量、积压量、生产量】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">kafka-consumer-offset-checker.sh --zookeeper dev:2181 --group your_group --topic your_topic</span><br><span class="line"></span><br><span class="line"> 如果上述脚本不可用，可以直接引用类 </span><br><span class="line">kafka-run-class.sh kafka.tools.ConsumerOffsetChecker --group your_group --topic your_topic --zkconnect dev:2181</span><br></pre></td></tr></table></figure><h2 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h2><p>操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> 登录 </span><br><span class="line">zkCli.sh -server ip:port</span><br><span class="line"></span><br><span class="line"> 退出 </span><br><span class="line">quit</span><br></pre></td></tr></table></figure><h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><p>操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> 登录，n 表示桶，默认 0</span><br><span class="line">redis-cli -h host -p port -n 0 --raw</span><br><span class="line"></span><br><span class="line"> 查看配置信息，例如数据量、占用内存 </span><br><span class="line">info</span><br><span class="line"></span><br><span class="line"> 查看单个配置项，例如最大内存 </span><br><span class="line">config get maxmemory</span><br><span class="line"></span><br><span class="line"> 退出 </span><br><span class="line">quit</span><br></pre></td></tr></table></figure><h2 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h2><p><code>HBase</code> 操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"> 登录 </span><br><span class="line">hbase shell</span><br><span class="line"></span><br><span class="line"> 帮助文档 </span><br><span class="line">!help</span><br><span class="line"></span><br><span class="line"> 退格 </span><br><span class="line">ctrl + Backspace</span><br><span class="line"></span><br><span class="line"> 退出 </span><br><span class="line">!quit</span><br><span class="line"></span><br><span class="line"> 查看表信息 </span><br><span class="line">!list</span><br><span class="line"></span><br><span class="line"> 获取数据 </span><br><span class="line">get &apos; 表名称 & apos;,&apos;pk 值 & apos;</span><br><span class="line"></span><br><span class="line"> 扫描数据 </span><br><span class="line">scan &apos; 表名称 & apos;,&#123;LIMIT =&gt; 5&#125;</span><br><span class="line"></span><br><span class="line"> 检查 Region 状态 </span><br><span class="line">hbase hbck</span><br><span class="line"></span><br><span class="line"> 此命令用于修复未分配，错误分配或者多次分配 Region 的问题，注意用户权限问题 </span><br><span class="line">hbase hbck -fixAssignments</span><br><span class="line">sudo -u hbase hbase hbck -fixAssignments</span><br><span class="line"></span><br><span class="line"> 检查 Region 的元数据 </span><br><span class="line">get &apos;hbase:meta&apos;,&apos;your_region_name&apos;</span><br><span class="line"></span><br><span class="line"> 检测数据表的状态 </span><br><span class="line">hbase hbck&quot;YOUR_TABLE&quot;</span><br><span class="line"></span><br><span class="line"> 修复 Region 空洞问题【Region hole】，注意用户权限问题 </span><br><span class="line">sudo -u hbase hbase hbck -fix&quot;YOUR_TABLE&quot;</span><br><span class="line"></span><br><span class="line"> 手动做 major_compact</span><br><span class="line">major_compact &quot;your_table_name&quot;</span><br></pre></td></tr></table></figure><p><code>phoenix</code> 操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"> 登录 </span><br><span class="line">phoenix-client/bin/sqlline.py dev:2181:/hbase-unsecure</span><br><span class="line"></span><br><span class="line"> 帮助文档 </span><br><span class="line">!help</span><br><span class="line"></span><br><span class="line"> 退出 </span><br><span class="line">!quit</span><br><span class="line"></span><br><span class="line"> 查看表信息 </span><br><span class="line">!tables</span><br><span class="line"></span><br><span class="line"> 查询数据 </span><br><span class="line">select * from &quot; 表名称 & quot; where &quot;pk&quot;=&apos;pk 值 & apos;;</span><br><span class="line">select * from &quot; 表名称 & quot; limit 5;</span><br><span class="line"></span><br><span class="line"> 导出数据 </span><br><span class="line">-- 格式化输出 </span><br><span class="line">!outputformat csv</span><br><span class="line">-- 设置导出文件到 data.csv</span><br><span class="line">!record data.csv</span><br><span class="line">-- 查询需要导出的数据 </span><br><span class="line">select * from &quot; 表名称 & quot; limit 1000;</span><br><span class="line">-- 完成导数操作 </span><br><span class="line">!record</span><br><span class="line"></span><br><span class="line"> 格式化显示 </span><br><span class="line">!outputformat vertical</span><br><span class="line">!outputformat csv</span><br><span class="line">!outputformat table</span><br></pre></td></tr></table></figure><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><p>启动 <code>Spark</code> 命令行，使用 <code>spark-shell --master yarn-client</code>，然后就可以在交互式命令行中写 <code>scala</code>、<code>python</code> 代码了。</p><p>举例：读取 2 个文件，做并集：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">var list1=sc.textFile (&quot;/tmp/pengfei/file1.txt&quot;).flatMap (x =&gt; x.split (&quot;\\t&quot;))</span><br><span class="line"> 文件在 HDFS 中，一行数据有被 Tab 分隔的多条内容 </span><br><span class="line"></span><br><span class="line">var list2=sc.textFile (&quot;/tmp/pengfei/file2.txt&quot;).flatMap (x =&gt; x.split (&quot;\\t&quot;))</span><br><span class="line"></span><br><span class="line">list1.union (list2).saveAsTextFile (&quot;/tmp/pengfei/union.txt&quot;)</span><br></pre></td></tr></table></figure><p>触发任务后在 <code>yarn</code> 中就可以看到任务的运行情况，注意，启动 <code>Spark-shell</code> 的时候也可以直接在本地运行，但是处理数据量大的时候内存会不够用，不建议。启动 <code>Spark-Shell</code> 的时候，可以指定使用 <code>yarn</code> 模式：<code>spark-shell --master yarn-client</code>，如果不指定，默认在本地启动，内存与处理速度会慢。</p><p><code>spark-shell --help</code>，查看启动参数详情，例如以下参数，<code>--executor-memory 4G</code>、<code>--total-executor-cores 4</code>、<code>--executor-cores 2</code> 可以加大内存，增加 <code>core</code> 数。</p><p>常用方法举例：<code>dinstinct</code>【去重】、<code>intersection</code>【交集】、<code>subtract</code>【差集】、<code>union</code>【并集】、<code>saveAsTextFile</code>【导出】、<code>textFile</code>【读取】。</p><p>分割去重过滤：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">.flatMap (x=&gt;x.split (&quot;\\t&quot;)).distinct ().filter (x=&gt;11&gt;x.length ())</span><br></pre></td></tr></table></figure><p>单词计数写回文件，按照 <code>value</code> 降序排列：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">val wordcount = sc.textFile (&quot;&quot;).flatMap (line =&gt; line.split (&quot;\\t&quot;)).map (word =&gt; (word, 1)).reduceByKey ((a, b) =&gt; a + b)</span><br><span class="line"></span><br><span class="line">val finalresult=wordcount.filter (count =&gt; count._2 &gt; 100)</span><br><span class="line"></span><br><span class="line">finalresult.persist ()</span><br><span class="line"></span><br><span class="line">finalresult.foreach (println)</span><br><span class="line"></span><br><span class="line">finalresult.count ()</span><br><span class="line"></span><br><span class="line">finalresult=finalresult.sortBy (_._2,false)</span><br><span class="line"></span><br><span class="line">val textresult=finalresult.map (count =&gt; count._1 + &quot;,&quot; + count._2)</span><br><span class="line"></span><br><span class="line">textresult.foreach (println)</span><br><span class="line"></span><br><span class="line">textresult.saveAsTextFile (&quot;&quot;)</span><br></pre></td></tr></table></figure><h1 id="大数据任务调度系统"><a href="# 大数据任务调度系统" class="headerlink" title="大数据任务调度系统"></a>大数据任务调度系统 </h1><h2 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h2><h3 id="移动队列"><a href="# 移动队列" class="headerlink" title="移动队列"></a> 移动队列 </h3><p> 在资源紧张的情况下，可以手动移动 <code>Spark</code> 任务的队列，合理利用空闲的资源。</p><p>如果 <code>yarn</code> 队列没有资源了，而自己又有 <code>Spark</code> 任务需要跑，在提交 <code>Spark</code> 任务后，可以看到任务在等待分配资源，此时可以把 <code>Spark</code> 任务移动到别的空闲队列去【也可以在提交 <code>Spark</code> 任务的时候指定队列】。</p><p>或者某些运行耗时比较长的任务，也可以先移动到别的空闲队列跑，释放自己业务的队列资源给其它任务使用。前提是确保不影响别的队列的正常使用，不能长久占用别人的队列。</p><p>使用 <code>yarn</code> 自带的命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-- 指定 Spark 任务的 applicationId，以及目标队列名称 </span><br><span class="line">-- 注意用户权限 </span><br><span class="line">yarn application -movetoqueue applicationId -queue queueName</span><br></pre></td></tr></table></figure><p>如果不熟悉这些命令，可以使用 <code>yarn -help</code>、<code>yarn application -help</code> 等命令查看帮助说明文档。</p><h3 id="取消任务"><a href="# 取消任务" class="headerlink" title="取消任务"></a>取消任务 </h3><p> 如果想 <code>kill</code> 掉一个正在 <code>yarn</code> 环境上运行的 <code>Spark</code> 任务，可以直接使用 <code>kill</code> 命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- 指定 Spark 任务的 applicationId</span><br><span class="line">yarn application -kill applicationId</span><br></pre></td></tr></table></figure><p>当然，如果使用的 <code>yarn-client</code> 模式提交任务，并且有权限看到 <code>Driver</code> 端的进程，也可以直接使用 <code>Linux</code> 的命令 <code>kill</code> 掉进程【不够优雅】。</p><h3 id="下载日志"><a href="# 下载日志" class="headerlink" title="下载日志"></a>下载日志 </h3><p> 下载已经完成的 <code>Spark</code> 任务的文本日志，在 <code>Saprk</code> 任务的 <code>UI</code> 界面，可以看到 <code>Excutor</code> 的日志，但是当日志多的时候，看起来不方便，搜索就更不用说了。</p><p>当任务完成之后，可以使用 <code>yarn</code> 命令下载文本日志进行查看、分析：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-- 指定 Spark 任务的 applicationId，所属用户 </span><br><span class="line">yarn logs -applicationId applicationId -appOwner userName</span><br></pre></td></tr></table></figure><p>注意，由于用户权限的问题，在下载日志之前需要切换用户，否则获取不到日志或者抛出权限相关错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">-- 切换用户，根据实际情况而定 </span><br><span class="line">-- 决定着去哪个目录获取日志文件 </span><br><span class="line">export HADOOP_USER_NAME=xxx</span><br></pre></td></tr></table></figure><h1 id="操作系统"><a href="# 操作系统" class="headerlink" title="操作系统"></a>操作系统 </h1><h2 id="回车换行"><a href="# 回车换行" class="headerlink" title="回车换行"></a> 回车换行 </h2><p> 在文本数据处理中，<code>CR</code>、<code>LF</code>、<code>CR/LF</code> 是不同操作系统上使用的换行符，表现为文本另起一行输出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">CR：Carriage Return</span><br><span class="line">LF：Line Feed</span><br></pre></td></tr></table></figure><p>下面列出常见的操作系统标准：</p><ul><li><code>Dos</code> 和 <code>Windows</code> 采用回车符 + 换行符，<code>CR/LF</code> 表示下一行 </li><li><code>UNIX/Linux</code> 采用换行符，<code>LF</code> 表示下一行</li><li> 苹果 <code>MAC OS</code> 系统则采用回车符，<code>CR</code> 表示下一行 </li></ul><p> 还需要注意，<code>CR</code> 用符号 <code>\\r</code> 表示，十进制 <code>ASCII</code> 编码是 13，十六进制编码为 <code>0x0d</code>，<code>LF</code> 使用 <code>\\n</code> 符号表示，十进制 <code>ASCII</code> 编码是 10，十六制编码为 <code>0x0a</code> 。</p><p>所以在 <code>Windows</code> 平台上的换行效果，在文本文件中是使用 <code>0d</code>、<code>0a</code> 这两个字节来实现的，而 <code>UNIX</code> 和苹果平台上的换行效果则是使用 <code>0a</code> 或 <code>0d</code> 一个字符来实现的，节约了存储空间。</p><p>在一个操作系统平台上使用另一种换行标准的文本文件，可能会带来意想不到的问题，特别是在程序代码中，有时候代码在编辑器中显示正常，但是在编译打包时却会因为换行符问题而出错【备注：使用 <code>Git</code> 版本控制系统时，<code>Git</code> 客户端已经根据当前的操作系统环境，自动帮助我们实时替换了换行符号】。</p><p>例如常见的一个现象，<code>Unix/Mac</code> 系统下的文件在 <code>Windows</code> 里打开的时候，会发现所有的文字变成了一行，而同理，<code>Windows</code> 里的文件在 <code>Unix/Mac</code> 下打开的话，在每行的结尾可能会多出一个 <code>^M</code> 符号。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;本文记录一些工作当中常用的小工具、小技巧，有时候自己可以查看备忘，同时也给读者参考。当然，内容是在不断补充的，涉及的维度也会扩展。&lt;/p&gt;
    
    </summary>
    
      <category term="基础技术知识" scheme="https://www.playpi.org/categories/basic-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="Hadoop" scheme="https://www.playpi.org/tags/Hadoop/"/>
    
      <category term="Spark" scheme="https://www.playpi.org/tags/Spark/"/>
    
      <category term="Yarn" scheme="https://www.playpi.org/tags/Yarn/"/>
    
      <category term="Python" scheme="https://www.playpi.org/tags/Python/"/>
    
      <category term="Java" scheme="https://www.playpi.org/tags/Java/"/>
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="Shell" scheme="https://www.playpi.org/tags/Shell/"/>
    
      <category term="Phoenix" scheme="https://www.playpi.org/tags/Phoenix/"/>
    
      <category term="Zookeeper" scheme="https://www.playpi.org/tags/Zookeeper/"/>
    
      <category term="Excel" scheme="https://www.playpi.org/tags/Excel/"/>
    
      <category term="Linux" scheme="https://www.playpi.org/tags/Linux/"/>
    
      <category term="Kakfa" scheme="https://www.playpi.org/tags/Kakfa/"/>
    
  </entry>
  
  <entry>
    <title>Spark 警告：Not enough space to cache rdd in memory</title>
    <link href="https://www.playpi.org/2020012201.html"/>
    <id>https://www.playpi.org/2020012201.html</id>
    <published>2020-01-21T17:58:33.000Z</published>
    <updated>2020-01-21T17:58:33.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>在常规的 <code>Spark</code> 任务中，出现警告：<code>Not enough space to cache rdd_0_255 in memory! (computed 8.3 MB so far)</code>，接着任务就卡住，等了很久最终 <code>Spark</code> 任务失败。排查到原因是 <code>RDD</code> 缓存的时候内存不够，无法继续处理数据，等待资源释放，最终导致假死现象。本文中的开发环境基于 <code>Spark v1.6.2</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 在服务器上面执行一个简单的 <code>Spark</code> 任务，代码逻辑里面有 <code>rdd.cache ()</code> 操作，结果在日志中出现类似如下的警告：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">01:35:42.207 [Executor task launch worker-4] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_28 as it would require dropping another block from the same RDD</span><br><span class="line">01:35:42.211 [Executor task launch worker-4] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_28 in memory! (computed 340.2 MB so far)</span><br><span class="line">01:35:42.213 [Executor task launch worker-4] INFO  org.apache.spark.storage.MemoryStore - Memory use = 8.3 KB (blocks) + 4.9 GB (scratch space shared across 106 tasks (s)) = 4.9 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:35:49.104 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_15 as it would require dropping another block from the same RDD</span><br><span class="line">01:35:49.105 [Executor task launch worker-0] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_15 in memory! (computed 341.4 MB so far)</span><br><span class="line">01:35:49.105 [Executor task launch worker-0] INFO  org.apache.spark.storage.MemoryStore - Memory use = 8.3 KB (blocks) + 4.9 GB (scratch space shared across 106 tasks (s)) = 4.9 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:35:51.375 [Executor task launch worker-11] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_33 as it would require dropping another block from the same RDD</span><br><span class="line">01:35:51.375 [Executor task launch worker-11] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_33 in memory! (computed 341.4 MB so far)</span><br><span class="line">01:35:51.376 [Executor task launch worker-11] INFO  org.apache.spark.storage.MemoryStore - Memory use = 8.3 KB (blocks) + 4.9 GB (scratch space shared across 106 tasks (s)) = 4.9 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:35:52.188 [Executor task launch worker-12] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_48 as it would require dropping another block from the same RDD</span><br><span class="line">01:35:52.188 [Executor task launch worker-12] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_48 in memory! (computed 341.4 MB so far)</span><br><span class="line">01:35:52.189 [Executor task launch worker-12] INFO  org.apache.spark.storage.MemoryStore - Memory use = 8.3 KB (blocks) + 4.9 GB (scratch space shared across 106 tasks (s)) = 4.9 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:35:52.213 [Executor task launch worker-6] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_58 as it would require dropping another block from the same RDD</span><br><span class="line">01:35:52.213 [Executor task launch worker-6] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_58 in memory! (computed 342.6 MB so far)</span><br><span class="line">01:35:52.214 [Executor task launch worker-6] INFO  org.apache.spark.storage.MemoryStore - Memory use = 8.3 KB (blocks) + 4.9 GB (scratch space shared across 106 tasks (s)) = 4.9 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:35:56.619 [Executor task launch worker-2] INFO  org.apache.spark.storage.MemoryStore - Block rdd_0_41 stored as values in memory (estimated size 378.7 MB, free 378.7 MB)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200122021258.png" alt="Storage 内存不足警告" title="Storage 内存不足警告"></p><p>看起来这只是一个警告，显示 <code>Storage</code> 内存不足，无法进行 <code>rdd.cache ()</code>，等待一段时间之后，<code>Spark</code> 任务的部分 <code>Task</code> 可以接着运行。</p><p>但是后续还是会发生同样的事情：内存不足，导致 <code>Task</code> 一直在等待，最后假死【或者说 <code>Spark</code> 任务基本卡住不动】。</p><p>里面有一个明显的提示：<code>Storage limit = 5.0 GB.</code>，也就是 <code>Storage</code> 的上限是 <code>5GB</code>。</p><h1 id="问题分析解决"><a href="# 问题分析解决" class="headerlink" title="问题分析解决"></a>问题分析解决 </h1><p> 查看业务代码，里面有一个：<code>rdd.cache ();</code> 操作，显然会占用大量的内存。</p><p>查看官方文档的配置：<a href="https://spark.apache.org/docs/1.6.2/configuration.html" target="_blank" rel="noopener">1.6.2-configuration</a> ，里面有一个重要的参数：<code>spark.storage.memoryFraction</code>，它是一个系数，决定着缓存上限的大小【基数是 <code>spark.excutor.memory</code>】。</p><blockquote><p>(deprecated) This is read only if spark.memory.useLegacyMode is enabled. Fraction of Java heap to use for Spark’s memory cache. This should not be larger than the “old” generation of objects in the JVM, which by default is given 0.6 of the heap, but you can increase it if you configure your own old generation size.</p></blockquote><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200122023358.png" alt="memoryFraction 参数" title="memoryFraction 参数"></p><p>另外还有 2 个相关的参数，读者也可以了解一下。</p><p>读者可以注意到，官方是不建议使用这个参数的，也就是不建议变更。当然如果你非要使用也是可以的，可以提高系数的值，这样的话缓存的空间就会变多。显然这样做不合理。</p><p>那有没有别的方法了呢？有！当然有。</p><p>主要是从缓存的方式入手，不要直接使用 <code>rdd.cache ()</code>，而是通过序列化 <code>RDD</code> 数据：<code>rdd.persist (StorageLevel.MEMORY_ONLY_SER)</code>，减少空间的占用，或者直接缓存一部分数据到磁盘：<code>rdd.persist (StorageLevel.MEMORY_AND_DISK)</code>，避免内存不足。</p><p>我下面演示使用后者，即直接缓存一部分数据到磁盘，当然，使用这种方式，<code>Spark</code> 任务执行速度肯定是慢了不少。</p><p>我这里测试后，得到的结果：耗时是以前的 3 倍【可以接受】。</p><p>再接着执行 <code>Spark</code> 任务，日志中还是会出现上述警告：<code>Not enough space to cache rdd in memory!</code>，但是接着会提示数据被缓存到磁盘了：<code>Persisting partition rdd_0_342 to disk instead.</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">01:55:24.414 [Executor task launch worker-3] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_342 as it would require dropping another block from the same RDD</span><br><span class="line">01:55:24.414 [Executor task launch worker-3] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_342 in memory! (computed 96.8 MB so far)</span><br><span class="line">01:55:24.414 [Executor task launch worker-3] INFO  org.apache.spark.storage.MemoryStore - Memory use = 3.0 GB (blocks) + 2.0 GB (scratch space shared across 339 tasks (s)) = 5.0 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:55:24.414 [Executor task launch worker-3] WARN  org.apache.spark.CacheManager - Persisting partition rdd_0_342 to disk instead.</span><br><span class="line">01:55:33.262 [Executor task launch worker-12] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_229 as it would require dropping another block from the same RDD</span><br><span class="line">01:55:33.262 [Executor task launch worker-12] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_229 in memory! (computed 342.6 MB so far)</span><br><span class="line">01:55:33.262 [Executor task launch worker-12] INFO  org.apache.spark.storage.MemoryStore - Memory use = 3.0 GB (blocks) + 2.0 GB (scratch space shared across 339 tasks (s)) = 5.0 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:55:33.262 [Executor task launch worker-12] WARN  org.apache.spark.CacheManager - Persisting partition rdd_0_229 to disk instead.</span><br><span class="line">01:55:40.247 [Executor task launch worker-13] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_254 as it would require dropping another block from the same RDD</span><br><span class="line">01:55:40.248 [Executor task launch worker-13] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_254 in memory! (computed 18.0 MB so far)</span><br><span class="line">01:55:40.248 [Executor task launch worker-13] INFO  org.apache.spark.storage.MemoryStore - Memory use = 3.0 GB (blocks) + 2.0 GB (scratch space shared across 339 tasks (s)) = 5.0 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:55:40.248 [Executor task launch worker-13] WARN  org.apache.spark.CacheManager - Persisting partition rdd_0_254 to disk instead.</span><br><span class="line">01:56:28.062 [dispatcher-event-loop-9] INFO  o.a.spark.storage.BlockManagerInfo - Added rdd_0_255 on disk on localhost:55066 (size: 146.4 MB)</span><br><span class="line">01:56:28.194 [Executor task launch worker-1] INFO  org.apache.spark.storage.MemoryStore - Will not store rdd_0_255 as it would require dropping another block from the same RDD</span><br><span class="line">01:56:28.194 [Executor task launch worker-1] WARN  org.apache.spark.storage.MemoryStore - Not enough space to cache rdd_0_255 in memory! (computed 8.3 MB so far)</span><br><span class="line">01:56:28.194 [Executor task launch worker-1] INFO  org.apache.spark.storage.MemoryStore - Memory use = 3.0 GB (blocks) + 2.0 GB (scratch space shared across 339 tasks (s)) = 5.0 GB. Storage limit = 5.0 GB.</span><br><span class="line">01:56:28.194 [Executor task launch worker-1] INFO  o.apache.spark.storage.BlockManager - Found block rdd_0_255 locally</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200122024437.png" alt="一部分数据被缓存到磁盘" title="一部分数据被缓存到磁盘"></p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 综上所述，有三种方式可以解决这个问题：</p><ul><li>提高缓存空间系数：<code>spark.storage.memoryFraction</code>【或者增大 <code>spark.excutor.memory</code>，不建议】</li><li>使用序列化 <code>RDD</code> 数据的方式：<code>rdd.persist (StorageLevel.MEMORY_ONLY_SER)</code></li><li>使用磁盘缓存的方式：<code>rdd.persist (StorageLevel.MEMORY_AND_DISK)</code></li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在常规的 &lt;code&gt;Spark&lt;/code&gt; 任务中，出现警告：&lt;code&gt;Not enough space to cache rdd_0_255 in memory! (computed 8.3 MB so far)&lt;/code&gt;，接着任务就卡住，等了很久最终 &lt;code&gt;Spark&lt;/code&gt; 任务失败。排查到原因是 &lt;code&gt;RDD&lt;/code&gt; 缓存的时候内存不够，无法继续处理数据，等待资源释放，最终导致假死现象。本文中的开发环境基于 &lt;code&gt;Spark v1.6.2&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Spark" scheme="https://www.playpi.org/tags/Spark/"/>
    
      <category term="cache" scheme="https://www.playpi.org/tags/cache/"/>
    
  </entry>
  
  <entry>
    <title>Elasticsearch 的 Reindex API 详解</title>
    <link href="https://www.playpi.org/2020011601.html"/>
    <id>https://www.playpi.org/2020011601.html</id>
    <published>2020-01-16T12:15:12.000Z</published>
    <updated>2020-01-16T12:15:12.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>在业务中只要有使用到 <code>Elasticsearch</code> 的场景，那么有时候会遇到需要重构索引的情况，例如 <code>mapping</code> 被污染了、某个字段需要变更类型等。如果对 <code>reindex API</code> 不熟悉，那么在遇到重构的时候，必然事倍功半，效率低下。但是如果熟悉了，就可以方便地进行索引重构，省时省力。</p><p>本文演示内容基于 <code>Elasticsearch v5.6.8</code>，在以后会不断补充完善。</p><a id="more"></a><h1 id="常用方式"><a href="# 常用方式" class="headerlink" title="常用方式"></a>常用方式 </h1><p> 提前声明，在开始演示具体的 <code>API</code> 的时候，有一点读者必须知道，<code>reindex</code> 不会尝试自动设置目标索引，它也不会复制源索引的设置。读者应该在运行 <code>reindex</code> 操作之前设置好目标索引的参数，包括映射、分片数、副本数等等。目标索引如果不设置 <code>mapping</code>，则会使用默认的配置，默认配置不会自动处理一些有特殊要求的字段【例如分词字段、数值类型字段】，则会引发字段类型错误的结果。</p><p>当然，关于设置索引，最常见的做法不是手动设置索引信息，而是使用索引模版【使用动态模版：<code>dynamic_templates</code>】，只要索引模版的匹配形式可以匹配上源索引和目标索引，我们不需要去考虑索引配置的问题，模版会为我们解决对应的问题。当然，关于的索引的分片数、副本数，还是需要考虑的。</p><p>最后，关于版本的说明，以下内容只针对 <code>v5.x</code> 以及之后的版本，更多版本的使用方式读者可以参考文末的备注信息。</p><h2 id="迁移数据简单示例"><a href="# 迁移数据简单示例" class="headerlink" title="迁移数据简单示例"></a>迁移数据简单示例 </h2><p> 涉及到 <code>_reindex</code> 关键字，简单示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行返回结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;took&quot;: 9991,</span><br><span class="line">  &quot;timed_out&quot;: false,</span><br><span class="line">  &quot;total&quot;: 12505,</span><br><span class="line">  &quot;updated&quot;: 12505,</span><br><span class="line">  &quot;created&quot;: 0,</span><br><span class="line">  &quot;deleted&quot;: 0,</span><br><span class="line">  &quot;batches&quot;: 13,</span><br><span class="line">  &quot;version_conflicts&quot;: 0,</span><br><span class="line">  &quot;noops&quot;: 0,</span><br><span class="line">  &quot;retries&quot;: &#123;</span><br><span class="line">    &quot;bulk&quot;: 0,</span><br><span class="line">    &quot;search&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;throttled_millis&quot;: 0,</span><br><span class="line">  &quot;requests_per_second&quot;: -1,</span><br><span class="line">  &quot;throttled_until_millis&quot;: 0,</span><br><span class="line">  &quot;failures&quot;: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200203234351.png" alt="演示结果" title="演示结果"></p><p>演示结果迁移了 12505 条数据，耗时 9991 毫秒。</p><p>以上只是一个非常基础的示例，还有一些可以优化的参数没有指定，全部使用的是默认值，例如看到 <code>batches</code>、<code>total</code> 对应的数值，就可以算出一批的数据大小默认为 1000，12505 条数据需要 13 批次才能迁移完成。再看到 <code>updated</code> 的数值，就可以猜测是更新了目标索引的数据，而不是创建数据，说明目标索引本来就有数据，被重新覆盖了。而这些内容在后续的小节中都会逐一解释，并再次演示。</p><p>注意一点，如果 <code>Elasticsearch</code> 是 <code>v6.x</code> 以及以下的版本，会涉及到索引的 <code>type</code>，如果源索引只有一个 <code>type</code>，则可以省略 <code>type</code> 这个参数，即不需要指定【数据会迁移到目标索引的同名 <code>type</code> 里面】。但是，如果涉及到多个 <code>type</code> 的数据迁移，肯定是要指定的【例如把多个 <code>type</code> 的数据迁移到同一个 <code>type</code> 中，或者仅仅把某个 <code>type</code> 的数据迁移到另外一个 <code>type</code> 中】。因此，为了准确无误，最好还是指定 <code>type</code>【当然再高的版本就没有 <code>type</code> 的概念了，无需指定】。</p><p>如果根据上面的示例，再添加 <code>type</code> 参数：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;type&quot;: &quot;user&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="version-type- 参数"><a href="#version-type- 参数" class="headerlink" title="version_type 参数"></a>version_type 参数 </h2><p> 就像 <code>_update_by_query</code> 里面的逻辑一样，<code>_reindex</code> 会生成源索引的快照【<code>snapshot</code>】，但是它的目标索引必须是一个不同的索引【另外创建一个】，以便避免版本冲突问题。同时，针对 <code>dest index</code> 可以像 <code>index API</code>【索引数据】 一样进行配置，以乐观锁控制并发写入【并发写入相同的数据，通过 <code>version</code> 来控制，有冲突时会写入失败，可以重试】。</p><p>像上面那样最简单的方式，不设置 <code>version_type</code> 参数【默认为 <code>internal</code>】或显式设置它为 <code>internal</code>，效果都一样。此时，<code>Elasticsearch</code> 将会直接将文档转储到 <code>dest index</code> 中，直接覆盖任何具有相同类型和 <code>id</code> 的 <code>document</code>，不会产生冲突问题。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;version_type&quot;: &quot;internal&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果把 <code>version_type</code> 设置为 <code>external</code>，则 <code>Elasticsearch</code> 会从 <code>source</code> 读取 <code>version</code> 字段，当遇到具有相同类型和 <code>id</code> 的 <code>document</code> 时，只会保留 <code>newer verion</code>，即最新的 <code>version</code> 对应的数据。此时可能会有冲突产生【例如把 <code>op_type</code> 设置为 <code>create</code>】，对于产生的冲突现象，返回体中的 <code>failures</code> 会携带冲突的数据信息【类似详细的日志可以查看】。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;version_type&quot;: &quot;external&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上面的说法看起来似乎有点不好理解，再简单直观点来说，就是在 <code>redinex</code> 的时候，我们的 <code>dest index</code> 可以不是一个新创建的不包含数据的 <code>index</code>，而是已经包含有数据的。如果我们的 <code>source index</code> 和 <code>dest index</code> 里面有相同类型和 <code>id</code> 的 <code>document</code>【一模一样的数据】，对于使用 <code>internal</code> 来说，就是直接覆盖，而使用 <code>external</code> 的话，只有当 <code>source index</code> 的数据的 <code>version</code> 比 <code>dest index</code> 数据的 <code>version</code> 更加新的时候，才会去更新【即保留最新的 <code>version</code> 对应的数据】。</p><p>再说明一下，<code>internal</code> 可以理解为使用内部版本号，即 <code>Elasticsearch</code> 不会单独比较版本号，对于 <code>dest index</code> 来说，无论是索引数据还是更新数据，写入时都按部就班把版本号累加，所以也就不会有冲突问题【从 <code>source index</code> 出来的数据是不携带版本信息的】，但是有可能会出现版本号不合法的问题，参考后面的 <strong>使用脚本配置 </strong>小节【使用脚本时人为变更版本号】。</p><p>另一方面，<code>external</code> 表示外部版本号，即 <code>Elasticsearch</code> 会单独比较版本号再决定写入的流程，对于 <code>dest index</code> 来说，无论是索引数据还是更新数据，写入时会先比较版本号，只保留版本号最大的数据【如果是来自不同索引的数据，版本号会不一致；如果是来自不同集群的数据，版本号规则可能也不一致】。</p><h2 id="op-type- 参数"><a href="#op-type- 参数" class="headerlink" title="op_type 参数"></a>op_type 参数 </h2><p><code>op_type</code> 参数控制着写入数据的冲突处理方式，如果把 <code>op_type</code> 设置为 <code>create</code>【默认值】，在 <code>_reindex API</code> 中，表示写入时只在 <code>dest index</code> 中添加不存在的 <code>doucment</code>，如果相同的 <code>document</code> 已经存在，则会报 <code>version confilct</code> 的错误，那么索引操作就会失败。【这种方式与使用 <code>_create API</code> 时效果一致】</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;op_type&quot;: &quot;create&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p> 如果这样设置了，也就不存在更新数据的场景了【冲突数据无法写入】，则 <code>version_type</code> 参数的设置也就无所谓了【但是返回体的 <code>failures</code> 中还是会携带冲突信息】。</p><p>另外也可以把 <code>op_type</code> 设置为 <code>index</code>，表示所有的数据全部重新索引创建。</p><p>再总结一下，如果把 <code>version_type</code> 设置为 <code>external</code>，无论 <code>op_type</code> 怎么设置，都有可能产生冲突【会比较版本】；如果把 <code>version_type</code> 设置为 <code>internal</code>，则在 <code>op_type</code> 为 <code>index</code> 的时候不会产生冲突，在 <code>op_type</code> 为 <code>create</code> 的时候可能有冲突。</p><h2 id="conflicts- 配置"><a href="#conflicts- 配置" class="headerlink" title="conflicts 配置"></a>conflicts 配置 </h2><p> 默认情况下，当发生 <code>version conflict</code> 的时候，<code>_reindex</code> 会被 <code>abort</code>，任务终止【此时数据还没有 <code>reindex</code> 完成】，在返回体中的 <code>failures</code> 指标中会包含冲突的数据【有时候数据会非常多】，除非把 <code>conflicts</code> 设置为 <code>proceed</code>。</p><p>关于 <code>abort</code> 的说明，如果产生了 <code>abort</code>，已经执行的数据【例如更新写入的】仍然存在于目标索引，此时任务终止，还会有数据没有被执行，也就是漏数了。换句话说，该执行过程不会回滚，只会终止。如果设置了 <code>proceed</code>，任务在检测到数据冲突的情况下，不会终止，会跳过冲突数据继续执行，直到所有数据执行完成，此时不会漏掉正常的数据，只会漏掉有冲突的数据。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;op_type&quot;: &quot;create&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;conflicts&quot;: &quot;proceed&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>故意把 <code>op_type</code> 设置为 <code>create</code>，人为制造数据冲突的场景，测试时更容易观察到冲突现象。</p><p>如果把 <code>conflicts</code> 设置为 <code>proceed</code>，在返回体结果中不会再出现 <code>failures</code> 的信息，但是通过 <code>version_conflicts</code> 指标可以看到具体的数量。</p><h2 id="query- 配置"><a href="#query- 配置" class="headerlink" title="query 配置"></a>query 配置 </h2><p> 迁移数据的时候，肯定有只是复制源索引中部分数据的场景，此时就需要配置查询条件【使用 <code>query</code> 参数】，只复制命中条件的部分数据，而不是全部，这和查询 <code>Elasticsearch</code> 数据的逻辑一致。</p><p>如下示例，通过 <code>query</code> 参数，把需要 <code>_reindex</code> 的 <code>document</code> 限定在一定的范围，我这里是限定更新时间 <code>update_timestamp</code> 在 1546272000000【20190101000000】之后。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">      &quot;bool&quot;: &#123;</span><br><span class="line">        &quot;must&quot;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;range&quot;: &#123;</span><br><span class="line">              &quot;update_timestamp&quot;: &#123;</span><br><span class="line">                &quot;gte&quot;: 1546272000000</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="批次大小配置"><a href="# 批次大小配置" class="headerlink" title="批次大小配置"></a>批次大小配置 </h2><p> 如果在 <code>query</code> 参数的同一层次【即 <code>source</code> 参数中】再添加 <code>size</code> 参数，并不是表示随机获取部分数据，而是表示 <code>scroll size</code> 的大小【会影响批次的次数，进而影响整体的速度】，这个有点迷惑人。另外，直接在 <code>query</code> 中添加 <code>size</code> 参数是不被允许的。</p><p>如果不显式设置，默认是一批 1000 条数据，在一开始的简单示例中也看到了。</p><p>如下，设置 <code>scroll size</code> 为 100：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;query&quot;: &#123;</span><br><span class="line">      &quot;bool&quot;: &#123;</span><br><span class="line">        &quot;must&quot;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;range&quot;: &#123;</span><br><span class="line">              &quot;update_timestamp&quot;: &#123;</span><br><span class="line">                &quot;gte&quot;: 1546272000000</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;size&quot;: 100</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200205001736.png" alt="返回结果" title="返回结果"></p><p>根据返回结果可以看到，实际迁移 12505 条数据，<code>batches</code> 为 126【可以算出每次 <code>batch</code> 为 100 条数据】，耗时为 31868 毫秒，是前面简单示例耗时的 3 倍【前面简单示例的耗时是 10 秒左右】。</p><p>注意，千万不要用错 <code>size</code> 参数的位置，可以继续参考下面的 <strong>随机 size 配置 </strong>小节。</p><h2 id="多对一迁移"><a href="# 多对一迁移" class="headerlink" title="多对一迁移"></a>多对一迁移 </h2><p> 如果需要把多个索引的数据或者多个 <code>type</code> 的数据共同迁移到同一个目标索引中，则需要在 <code>source</code> 参数中指定多个索引。</p><p>把同一个索引中的不同 <code>type</code> 的数据共同迁移到同一个索引中，例如把 <code>my-index-user</code> 下的 <code>user</code>、<code>user2</code> 数据共同迁移到 <code>my-index-user-v2</code> 中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: [</span><br><span class="line">      &quot;my-index-user&quot;,</span><br><span class="line">      &quot;my-index-user&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;type&quot;: [</span><br><span class="line">      &quot;user&quot;,</span><br><span class="line">      &quot;user2&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>把不同索引中的数据共同迁移到同一个索引中，例如把 <code>my-index-user</code> 下的 <code>user</code>、<code>my-index-user-v2</code> 下的 <code>user2</code> 数据共同迁移到 <code>my-index-user-v3</code> 中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: [</span><br><span class="line">      &quot;my-index-user&quot;,</span><br><span class="line">      &quot;my-index-user-v2&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;type&quot;: [</span><br><span class="line">      &quot;user&quot;,</span><br><span class="line">      &quot;user2&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v3&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里要注意的是，对于上面的第二个示例，如果 <code>my-index-user</code> 和 <code>my-index-user-v2</code> 中有 <code>document</code> 的 <code>id</code> 是一样的，则无法保证最终出现在 <code>my-index-user-v3</code> 里面的 <code>document</code> 是哪个，因为迭代是随机的。按照默认的配置【即前面的 <code>version_type</code>、<code>opt_type</code> 等参数】，最后一条数据会覆盖前一条数据。</p><p>当然，对于第一个示例也会有这个问题。</p><p>另外，不要想着一对多迁移、多对多迁移等操作，不支持，因为写入时必须指定唯一确定的索引，否则 <code>Elasticsearch</code> 不知道数据要往哪个索引里面写入。</p><h2 id="随机 -size- 配置"><a href="# 随机 -size- 配置" class="headerlink" title="随机 size 配置"></a>随机 size 配置 </h2><p> 有时候想提前测试一下迁移结果是否准确，或者使用少量数据做一下性能测试，则需要随机取数的配置，可以使用 <code>size</code> 参数。注意，<code>size</code> 参数的位置是与 <code>source</code>、<code>dest</code> 在同一层级的，即在最外层。</p><p>例如随机取数 100 条，示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200205004107.png" alt="随机 100 条数据结果" title="随机 100 条数据结果"></p><p>可以看到，随机迁移了 100 条数据，耗时 223 毫秒。</p><p>注意这里，千万不要用错 <code>size</code> 参数的位置，一个是表示 <code>scroll size</code> 的大小【如上面的 <strong>query 配置 </strong>中，配置在与 <code>query</code> 同一个层级，在 <code>source</code> 里面】，一个是表示随机抽取多少条【本小节示例，配置在最外层】。</p><p>我曾经就把 <code>size</code> 设置错误，放在与 <code>query</code> 同一层级，也就是误把 <code>scroll size</code> 设置为 10 了【本来是想先迁移 10 条数据看看结果】，导致 <code>reindex</code> 速度非常慢，30 分钟才 20 万数据量【我还在疑惑为什么设置的随机 10 条不生效，把全部数据都迁移了】。</p><p>后来发现了，把任务取消，重新提交 <code>reindex</code> 任务，准确地把随机 <code>size</code> 设置为 10 万，把 <code>scroll size</code> 设置为 2000。测试后才正式开始迁移数据，速度达到了 30 分钟 500 万，和前面的误操作比较明显提升了很多。</p><h2 id="排序配置"><a href="# 排序配置" class="headerlink" title="排序配置"></a>排序配置 </h2><p> 好，上面有了随机数据条数的设置，但是如果我们想根据某个字段进行排序，获取 <code>top 100</code>，有没有办法呢？有，当然有，可以使用排序的功能，关键字为 <code>sort</code>，使用方式和查询 <code>Elasticsearch</code> 时一致。</p><p>例如按照更新时间 <code>update_timestamp</code> 的降序排列，获取前 100 条数据：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;sort&quot;: &#123;</span><br><span class="line">      &quot;update_timestamp&quot;: &quot;desc&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200205005139.png" alt="返回结果" title="返回结果"></p><h2 id="指定字段配置"><a href="# 指定字段配置" class="headerlink" title="指定字段配置"></a>指定字段配置 </h2><p> 如果迁移数据时只需要特定的字段，可以使用 <code>_source</code> 参数指定字段，字段少了迁移速度也可以提升。下面的示例指定了 3 个字段：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;_source&quot;: [</span><br><span class="line">      &quot;id&quot;,</span><br><span class="line">      &quot;city_level&quot;,</span><br><span class="line">      &quot;task_ids&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200205235937.png" alt="返回结果" title="返回结果"></p><p>可以看到，迁移 12505 条数据，耗时 3611 毫秒，比前面的简单示例快了不少。</p><p>查看目标数据，可以看到只有 3 个字段【注意，<code>_id</code> 字段是 <code>document</code> 的主键，会自动携带】。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206001202.png" alt="有 3 个字段的数据示例" title="有 3 个字段的数据示例"></p><p>但是有一点需要注意，如果 <code>Elasticsearch</code> 的索引设置中，使用 <code>_source、excludes</code> 排除了部分字段的存储【为了节省磁盘空间】，实际上没有存储字段，只是做了索引，则不可以直接迁移，会丢失这些字段。</p><h2 id="使用脚本配置"><a href="# 使用脚本配置" class="headerlink" title="使用脚本配置"></a>使用脚本配置 </h2><p> 如果集群开启了允许使用 <code>script</code> 的功能【在配置文件 <code>elasticsearch.yml</code> 中使用 <code>script.inline: true</code> 开启】，就可以使用 <code>script</code> 做一些简单的数据转换。</p><p>例如把满足条件的数据做一个 <code>_version</code> 增加，并且移除指定的字段，在写入目标索引时，利用 <code>version_type</code> 参数保留最新版本的数据。</p><p>以下示例为了方便查看结果，只获取 3 个字段，脚本逻辑：对于 <code>city_level</code> 等于 1 的数据【<code>city</code> 为北京、上海、广州、深圳】，做一个版本自增，并且把 <code>city_level</code> 字段移除。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;_source&quot;: [</span><br><span class="line">      &quot;id&quot;,</span><br><span class="line">      &quot;city&quot;,</span><br><span class="line">      &quot;city_level&quot;</span><br><span class="line">    ]</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;version_type&quot;: &quot;external&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;script&quot;: &#123;</span><br><span class="line">    &quot;source&quot;: &quot;if (ctx._source.city_level == &apos;1&apos;) &#123;ctx._version++; ctx._source.remove (&apos;city_level&apos;)&#125;&quot;,</span><br><span class="line">    &quot;lang&quot;: &quot;painless&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>数据结果查看：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206012210.png" alt="数据结果查看" title="数据结果查看"></p><p>数据结果中可以看到数据的 <code>city_level</code> 字段已经消失了，只剩下 2 个字段。</p><p>再执行一次，如果数据相同，会有冲突问题，因为设置了 <code>version_type</code> 为 <code>external</code>，会比较版本【数据的版本不够新从而无法写入】。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">         &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">         &quot;type&quot;: &quot;user&quot;,</span><br><span class="line">         &quot;id&quot;: &quot;AW3zlTvZa9C6UomAXwqT&quot;,</span><br><span class="line">         &quot;cause&quot;: &#123;</span><br><span class="line">            &quot;type&quot;: &quot;version_conflict_engine_exception&quot;,</span><br><span class="line">            &quot;reason&quot;: &quot;[user][AW3zlTvZa9C6UomAXwqT]: version conflict, current version [3] is higher or equal to the one provided [1]&quot;,</span><br><span class="line">            &quot;index_uuid&quot;: &quot;tJienWj1T_udvoJQTcDzyg&quot;,</span><br><span class="line">            &quot;shard&quot;: &quot;1&quot;,</span><br><span class="line">            &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">         &#125;,</span><br><span class="line">         &quot;status&quot;: 409</span><br><span class="line">      &#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206012415.png" alt="冲突现象" title="冲突现象"></p><p>如果把 <code>version_type</code> 设置为 <code>internal</code>，同时指定 <code>op_type</code> 为 <code>index</code>【默认是 <code>create</code>】，则会出现版本号不合法的异常。因为在脚本中手动自增了版本号，不符合按照 <code>index</code> 方式索引数据的要求。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">action_request_validation_exception</span><br><span class="line">Validation Failed: 1: illegal version value [0] for version type [INTERNAL];</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206012755.png" alt="版本号不合法" title="版本号不合法"></p><p>在此引申一下脚本的内容，<code>Elasticsearch</code> 提供了脚本的支持，可以通过 <code>Groovy</code> 外置脚本【已经过时，<code>v6.x</code> 以及之后的版本，不建议使用】、内置 <code>painless</code> 脚本实现各种复杂的操作【类似于写逻辑代码，对数据进行 <code>ETL</code> 操作】，如上面的示例。</p><p><code>painless</code> 有轻便之意，使用时直接在语法中调用即可，无需外置，也就是不支持通过外部文件存储 <code>painless</code> 脚本来调用。</p><blockquote><p>默认的脚本语言是 Groovy，一种快速表达的脚本语言，在语法上与 JavaScript 类似。它在 Elasticsearch v1.3.0 版本首次引入并运行在沙盒中，然而 Groovy 脚本引擎存在漏洞，允许攻击者通过构建 Groovy 脚本，在 Elasticsearch Java VM 运行时脱离沙盒并执行 shell 命令。<br>因此，在版本 v1.3.8、v1.4.3 和 v1.5.0 及更高的版本中，它已经被默认禁用。此外，您可以通过设置集群中的所有节点的 config/elasticsearch.yml 文件来禁用动态 Groovy 脚本：script.groovy.sandbox.enabled: false，这将关闭 Groovy 沙盒，从而防止动态 Groovy 脚本作为请求的一部分被接受。</p></blockquote><p>当然，对于常用的脚本，可以通过 <code>_scripts/calculate-score</code> 接口创建后缓存起来【也需要集群的配置：<code>script.store: true</code>】，会生成一个唯一 <code>id</code>，下次可以直接使用【就像声明了一个方法】，还支持参数传递。</p><h2 id="使用 -Ingest-Node- 配置"><a href="# 使用 -Ingest-Node- 配置" class="headerlink" title="使用 Ingest Node 配置"></a>使用 Ingest Node 配置 </h2><p><code>Ingest</code> 其实就是定义了一些预处理的规则，可以预处理数据，提升性能，主要依靠 <code>Pipeline</code>、<code>Processors</code>。当然前提还是需要集群支持，定义一些 <code>Ingest</code> 节点、预处理流程，可以通过配置 <code>elasticsearch.xml</code> 文件中的 <code>node.ingest: true</code> 来开启 <code>Ingest</code> 节点。</p><p> 这个功能应该说是最好用的了，当你的 <code>source</code> 因为不合理的结构，需要重新结构化所有的数据时，通过 <code>Ingest Node</code>，可以很方便地在新的 <code>index</code> 中获得不一样的 <code>mapping</code> 和 <code>value</code>【例如把数值类型转为字符串类型，或者把值替换掉】。</p><p>使用方式也很简单【需要提前创建 <code>pipeline</code>】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;,</span><br><span class="line">    &quot;pipeline&quot;: &quot;some_ingest_pipeline&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 100</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如果没有创建 <code>pipeline</code>，会报错，在返回体的 <code>failures</code> 中：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;type&quot;: &quot;illegal_argument_exception&quot;,</span><br><span class="line">&quot;reason&quot;: &quot;pipeline with id [some_ingest_pipeline] does not exist&quot;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206014248.png" alt="报错信息" title="报错信息"></p><h2 id="迁移远程集群数据到当前环境"><a href="# 迁移远程集群数据到当前环境" class="headerlink" title="迁移远程集群数据到当前环境"></a>迁移远程集群数据到当前环境 </h2><p> 有时候需要跨集群迁移数据，例如把 <code>A</code> 集群的数据复制到 <code>B</code> 集群中，只要 <code>A</code> 集群的节点开放了 <code>ip</code>、端口，就可以使用 <code>remote</code> 参数。</p><p>在 <code>B</code> 集群中也需要设置白名单，在 <code>elasticsearch.xml</code> 文件中配置 <code>reindex.remote.whitelist: otherhost:9200</code> 参数即可，多个使用英文逗号隔开。</p><p>使用示例【如果有认证机制则还需要带上用户名、密码信息】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;remote&quot;: &#123;</span><br><span class="line">      &quot;host&quot;: &quot;http://otherhost:9200&quot;,</span><br><span class="line">      &quot;username&quot;: &quot;username&quot;,</span><br><span class="line">      &quot;password&quot;: &quot;password&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里需要注意，对于复制在其他集群上的 <code>index</code> 数据来说，就不存在直接从本地镜像复制的便利【速度快】，需要从网络上下载数据再写到本地。默认的设置，<code>buffer</code> 的 <code>size</code> 是 <code>100Mb</code>，在 <code>scroll size</code> 是 1000 的情况下【默认值】，如果单个 <code>document</code> 的平均大小超过 <code>100Kb</code>，则会报错，数据过大。</p><p>因此在遇到非常大的 <code>document</code> 时，需要减小 <code>batch</code> 的 <code>size</code>【尽管会导致 <code>batch</code> 变多，迁移速度变慢，但是安全】，使用 <code>size</code> 参数【参考前面的 <strong>批次大小配置 </strong>小节】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">POST _reindex</span><br><span class="line">&#123;</span><br><span class="line">  &quot;source&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user&quot;,</span><br><span class="line">    &quot;remote&quot;: &#123;</span><br><span class="line">      &quot;host&quot;: &quot;http://otherhost:9200&quot;,</span><br><span class="line">      &quot;username&quot;: &quot;username&quot;,</span><br><span class="line">      &quot;password&quot;: &quot;password&quot;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;size&quot;: 100</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;dest&quot;: &#123;</span><br><span class="line">    &quot;index&quot;: &quot;my-index-user-v2&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="返回体"><a href="# 返回体" class="headerlink" title="返回体"></a>返回体 </h2><p> 在提交迁移数据任务后，如果耐心等待，会有执行的结果返回，正常情况下，返回的结果格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;took&quot;: 9675,</span><br><span class="line">  &quot;timed_out&quot;: false,</span><br><span class="line">  &quot;total&quot;: 12505,</span><br><span class="line">  &quot;updated&quot;: 12505,</span><br><span class="line">  &quot;created&quot;: 0,</span><br><span class="line">  &quot;deleted&quot;: 0,</span><br><span class="line">  &quot;batches&quot;: 13,</span><br><span class="line">  &quot;version_conflicts&quot;: 0,</span><br><span class="line">  &quot;noops&quot;: 0,</span><br><span class="line">  &quot;retries&quot;: &#123;</span><br><span class="line">    &quot;bulk&quot;: 0,</span><br><span class="line">    &quot;search&quot;: 0</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;throttled_millis&quot;: 0,</span><br><span class="line">  &quot;requests_per_second&quot;: -1,</span><br><span class="line">  &quot;throttled_until_millis&quot;: 0,</span><br><span class="line">  &quot;failures&quot;: []</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206015432.png" alt="返回信息" title="返回信息"></p><p>下面挑选几个指标解释说明一下：</p><ul><li><code>took</code>，整个操作从开始到结束的毫秒数 </li><li><code>total</code>，已成功处理的文档数</li><li><code>updated</code>，已成功更新的文档数</li><li><code>deleted</code>，已成功删除的文档数</li><li><code>batches</code>，由查询更新拉回的滚动响应数【结合 <code>total</code> 可以算出 <code>scroll size</code>】，与 <code>scroll size</code> 有关</li><li><code>version_conflicts</code>，按查询更新的版本冲突数【涉及到版本的比较判断】</li><li><code>retries</code>，逐个更新尝试的重试次数，<code>bulk</code> 是批量操作的重试次数，<code>search</code> 是搜索操作的重试次数</li><li><code>failures</code>，如果在此过程中存在任何不可恢复的错误，发生 <code>abort</code>，则会返回故障信息数组【内容可能会比较多】</li></ul><p> 这里需要注意的是 <code>failures</code> 信息，如果里面的信息不为空，则表示本次 <code>_reindex</code> 是失败的，是被中途 <code>abort</code>，一般都是因为发生了 <code>conflicts</code>。前面已经描述过如何合理设置【业务场景可接受的方式，例如把 <code>conflicts</code> 设置为 <code>proceed</code>】，可以确保在发生 <code>conflict</code> 的时候还能继续运行。但是，这样设置后任务不会被 <code>abort</code>，可以正常执行完成，则最终也就不会返回 <code>failures</code> 信息了，但是通过 <code>version_conflicts</code> 指标可以看到具体的数量。</p><h2 id="查看任务进度以及取消任务"><a href="# 查看任务进度以及取消任务" class="headerlink" title="查看任务进度以及取消任务"></a>查看任务进度以及取消任务 </h2><p> 一般来说，如果我们的 <code>source index</code> 很大【比如几百万数据量】，则可能需要比较长的时间来完成 <code>_reindex</code> 的工作，可能需要几十分钟。而在此期间不可能一直等待结果返回，可以去做其它事情，如果中途需要查看进度，可以通过 <code>_tasks API</code> 进行查看。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">GET _tasks?detailed=true&amp;actions=*reindex</span><br></pre></td></tr></table></figure><p>返回结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;nodes&quot;: &#123;</span><br><span class="line">    &quot;ietwyYpJRo-gz_C1tCbpgQ&quot;: &#123;</span><br><span class="line">      &quot;name&quot;: &quot;dev4_xx&quot;,</span><br><span class="line">      &quot;transport_address&quot;: &quot;xx.xx.xx.204:9308&quot;,</span><br><span class="line">      &quot;host&quot;: &quot;xx.xx.xx.204&quot;,</span><br><span class="line">      &quot;ip&quot;: &quot;xx.xx.xx.204:9308&quot;,</span><br><span class="line">      &quot;roles&quot;: [</span><br><span class="line">        &quot;master&quot;,</span><br><span class="line">        &quot;data&quot;,</span><br><span class="line">        &quot;ingest&quot;</span><br><span class="line">      ],</span><br><span class="line">      &quot;tasks&quot;: &#123;</span><br><span class="line">        &quot;ietwyYpJRo-gz_C1tCbpgQ:420711&quot;: &#123;</span><br><span class="line">          &quot;node&quot;: &quot;ietwyYpJRo-gz_C1tCbpgQ&quot;,</span><br><span class="line">          &quot;id&quot;: 420711,</span><br><span class="line">          &quot;type&quot;: &quot;transport&quot;,</span><br><span class="line">          &quot;action&quot;: &quot;indices:data/write/reindex&quot;,</span><br><span class="line">          &quot;status&quot;: &#123;</span><br><span class="line">            &quot;total&quot;: 12505,</span><br><span class="line">            &quot;updated&quot;: 0,</span><br><span class="line">            &quot;created&quot;: 0,</span><br><span class="line">            &quot;deleted&quot;: 0,</span><br><span class="line">            &quot;batches&quot;: 5,</span><br><span class="line">            &quot;version_conflicts&quot;: 4000,</span><br><span class="line">            &quot;noops&quot;: 0,</span><br><span class="line">            &quot;retries&quot;: &#123;</span><br><span class="line">              &quot;bulk&quot;: 0,</span><br><span class="line">              &quot;search&quot;: 0</span><br><span class="line">            &#125;,</span><br><span class="line">            &quot;throttled_millis&quot;: 0,</span><br><span class="line">            &quot;requests_per_second&quot;: -1,</span><br><span class="line">            &quot;throttled_until_millis&quot;: 0</span><br><span class="line">          &#125;,</span><br><span class="line">          &quot;description&quot;: &quot;reindex from [my-index-user] to [my-index-user-v2]&quot;,</span><br><span class="line">          &quot;start_time_in_millis&quot;: 1580958992770,</span><br><span class="line">          &quot;running_time_in_nanos&quot;: 1441736539,</span><br><span class="line">          &quot;cancellable&quot;: true</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200206113250.png" alt="查看任务状态" title="查看任务状态"></p><p>根据任务的各项指标，就可以预估完成进度，从而预估完成时间，做到心中有数。</p><p>注意观察里面的几个重要指标，例如从 <code>description</code> 中可以看到任务描述，从 <code>tasks</code> 中可以找到任务的 <code>id</code>【例如 <code>ietwyYpJRo-gz_C1tCbpgQ:420711</code>】，从 <code>cancellable</code> 可以判断任务是否支持取消操作。</p><p>这个 <code>API</code> 其实就是模糊匹配，同理也可以查询其它类型的任务信息，例如使用 <code>GET _tasks?detailed=true&amp;actions=*byquery</code> 查看查询请求的状态。</p><p>如果知道了 <code>task_id</code>，也可以使用 <code>GET /_tasks/task_id</code> 更加准确地查询指定的任务状态，避免集群的任务过多，不方便查看。</p><p>如果遇到操作失误的场景，想取消任务，有没有办法呢？有，泼出去的水还是可以收回的，通过 <code>_tasks API</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">POST _tasks/task_id/_cancel</span><br></pre></td></tr></table></figure><p>这里的 <code>task_id</code> 就是通过上面的查询任务接口获取的，另外还需要任务支持取消操作【<code>cancellable</code> 为 <code>true</code>】。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 参考官网：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/docs-reindex.html" target="_blank" rel="noopener">docs-reindex</a> 。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在业务中只要有使用到 &lt;code&gt;Elasticsearch&lt;/code&gt; 的场景，那么有时候会遇到需要重构索引的情况，例如 &lt;code&gt;mapping&lt;/code&gt; 被污染了、某个字段需要变更类型等。如果对 &lt;code&gt;reindex API&lt;/code&gt; 不熟悉，那么在遇到重构的时候，必然事倍功半，效率低下。但是如果熟悉了，就可以方便地进行索引重构，省时省力。&lt;/p&gt;&lt;p&gt;本文演示内容基于 &lt;code&gt;Elasticsearch v5.6.8&lt;/code&gt;，在以后会不断补充完善。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="HTTP" scheme="https://www.playpi.org/tags/HTTP/"/>
    
  </entry>
  
  <entry>
    <title>Spark 异常之 Failed to create local dir</title>
    <link href="https://www.playpi.org/2020010201.html"/>
    <id>https://www.playpi.org/2020010201.html</id>
    <published>2020-01-01T16:32:44.000Z</published>
    <updated>2020-02-10T16:32:44.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>今天的 <code>Spark Streaming</code> 程序莫名出现异常【对于一个 <code>task</code> 来说，<code>Spark Streaming</code> 会重试 4 次，全部重试都失败后整个 <code>Stage</code> 才会失败】，紧接着 <code>task</code> 中的 <code>batch</code> 就会卡住不动【后来查到卡住是其它原因造成的】，使得整个 <code>Spark Streaming</code> 任务进程进入到等待状态，所有的 <code>batch</code> 都处于 <code>queued</code> 状态，数据流无法继续执行。本文内容基于 <code>Spark v1.6.2</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 线上一个一直很稳定的 <code>Spark Streaming</code> 程序，突然出现异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">Job aborted due to stage failure: Task 0 in stage 2174.0 failed 4 times, most recent failure: Lost task 0.3 in stage 2174.0 (TID 32656, host): java.io.IOException: Failed to create local dir in /cloud/data2/spark/local/spark-4fccb5c2-29f5-45f9-926e-1c6e33636884/executor-30fdf8f9-6459-43c0-bba5-3a406db7e700/blockmgr-7edadea3-1fa3-4f32-bef2-1cf81230042a/07.</span><br><span class="line">at org.apache.spark.storage.DiskBlockManager.getFile (DiskBlockManager.scala:73)</span><br><span class="line">at org.apache.spark.storage.DiskStore.contains (DiskStore.scala:161)</span><br><span class="line">at org.apache.spark.storage.BlockManager.org$apache$spark$storage$BlockManager$$getCurrentBlockStatus (BlockManager.scala:398)</span><br><span class="line">at org.apache.spark.storage.BlockManager.doPut (BlockManager.scala:827)</span><br><span class="line">at org.apache.spark.storage.BlockManager.putBytes (BlockManager.scala:700)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$anonfun$$getRemote$1$1.apply (TorrentBroadcast.scala:130)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$anonfun$$getRemote$1$1.apply (TorrentBroadcast.scala:127)</span><br><span class="line">at scala.Option.map (Option.scala:145)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.org$apache$spark$broadcast$TorrentBroadcast$$anonfun$$getRemote$1 (TorrentBroadcast.scala:127)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply (TorrentBroadcast.scala:137)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1$$anonfun$1.apply (TorrentBroadcast.scala:137)</span><br><span class="line">at scala.Option.orElse (Option.scala:257)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply$mcVI$sp (TorrentBroadcast.scala:137)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply (TorrentBroadcast.scala:120)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$org$apache$spark$broadcast$TorrentBroadcast$$readBlocks$1.apply (TorrentBroadcast.scala:120)</span><br><span class="line">at scala.collection.immutable.List.foreach (List.scala:318)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast.org$apache$spark$broadcast$TorrentBroadcast$$readBlocks (TorrentBroadcast.scala:120)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply (TorrentBroadcast.scala:175)</span><br><span class="line">at org.apache.spark.util.Utils$.tryOrIOException (Utils.scala:1205)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock (TorrentBroadcast.scala:165)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute (TorrentBroadcast.scala:64)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast._value (TorrentBroadcast.scala:64)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast.getValue (TorrentBroadcast.scala:88)</span><br><span class="line">at org.apache.spark.broadcast.Broadcast.value (Broadcast.scala:70)</span><br><span class="line">at org.apache.spark.scheduler.ResultTask.runTask (ResultTask.scala:62)</span><br><span class="line">at org.apache.spark.scheduler.Task.run (Task.scala:89)</span><br><span class="line">at org.apache.spark.executor.Executor$TaskRunner.run (Executor.scala:227)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">Driver stacktrace:</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200211004947.png" alt="异常信息" title="异常信息"></p><p>重点内容在于 <code>java.io.IOException: Failed to create local dir</code>。</p><p>为了不影响业务逻辑，首先尝试重启，重启后短暂恢复正常，大概运行 20-40 分钟后，继续出现上述异常，非常诡异【重启 5 次左右仍旧出现】。</p><p>同时，伴随着部分 <code>Stage</code> 失败，<code>Spark Streaming</code> 任务出现了 <code>batch</code> 卡住的现象，有 2 个 <code>btach</code> 一直处于 <code>processing</code> 状态，极不正常。导致后面所有的 <code>batch</code> 都处于 <code>queued</code> 状态，数据流无法继续执行，最终整个 <code>Spark Streaming</code> 任务会卡住不动，类似于假死。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200211010335.png" alt="batch 卡住" title="batch 卡住"></p><h1 id="问题分析解决"><a href="# 问题分析解决" class="headerlink" title="问题分析解决"></a>问题分析解决 </h1><p> 查了一下文档，发现有两种情况会造成这个异常，一是没有权限写入磁盘，二是磁盘空间不足。</p><p>找运维人员协助查看了一下，服务器的磁盘没有问题，根据进程的用户判断权限也没有问题，而且有好几个其它类似的 <code>Spark Straming</code> 任务可以正常运行，一点问题没有。</p><p>于是回顾一下最近的代码变动，发现有问题的 <code>Spark Streaming</code> 任务都变更过一个第三方接口的调用，于是联系对应的开发人员。</p><p>经过沟通测试，发现了问题，第三方接口有一个异常没有捕捉，导致上述异常产生。同时，由此会导致资源不释放的 <code>bug</code>，进而影响了 <code>Spark Streaming</code> 任务的 <code>batch</code> 卡住。</p><p>以上这些问题都与 <code>Spark</code> 无关，后面紧急升级第三方接口，任务得以正常运行，后续又观察了三天，都没有问题。</p><h1 id="深入探究"><a href="# 深入探究" class="headerlink" title="深入探究"></a>深入探究 </h1><p> 问题虽然解决了，但还是要关注一下这个异常的场景，通过查看源码，发现这个异常是在创建目录的时候产生的，如下图：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2020/20200211011126.png" alt="查看源码" title="查看源码"></p><p>那这个场景就很简单了，如果进程没有写入磁盘的权限或者磁盘空间不足，都会产生这个异常。</p><p>进一步思考，为什么会创建这个目录，作用是什么呢？</p><p>原来，<code>Spark</code> 在 <code>shuffle</code> 时需要通过 <code>diskBlockManage</code> 将 <code>map</code> 结果写入本地，优先写入 <code>memory store</code>，在 <code>memore store</code> 空间不足时会创建临时文件。这是一个二级目录，如异常中的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">/cloud/data2/spark/local/spark-4fccb5c2-29f5-45f9-926e-1c6e33636884/executor-30fdf8f9-6459-43c0-bba5-3a406db7e700/blockmgr-7edadea3-1fa3-4f32-bef2-1cf81230042a/07</span><br></pre></td></tr></table></figure><p>使用完成后会立即删除。</p><p>那 <code>shuffle</code> 又是咋回事呢？<code>Spark</code> 作为并行计算框架，同一个作业会被划分为多个任务在多个节点上执行，<code>reduce</code> 的输入可能存在于多个节点，因此需要 <code>shuffle</code> 将所有 <code>reduce</code> 的输入汇总起来。这一步比较消耗内存或者说是磁盘，视选择的缓存方式而定。</p><p>那上面的 <code>memory store</code> 的大小是多少，什么情况下会超出上限从而选择使用 <code>disk store</code>？其实，<code>memory store</code> 的大小取决于 <code>spark.excutor.memory</code> 的大小，默认为 <code>spark.excutor.memory * 0.6</code>。此外，官方是不建议更改 0.6 这个系数的【参数：<code>spark.storage.memoryFraction</code>】，参考：<a href="https://spark.apache.org/docs/1.6.2/configuration.html" target="_blank" rel="noopener">configuration-1.6.2</a> 。</p><p>那临时文件的目录，可以更改吗，例如磁盘空间不足后，新挂载了一块磁盘。是可以的，在 <code>spark.env</code> 中添加 <code>SPARK_LOCAL_DIRS</code> 变量即可【通过 <code>spark-env.sh</code> 脚本可以添加】，或者直接在程序中配置【<code>spark conf</code>，参数名是 <code>spark.local.dir</code>】，可配置多个路径，使用英文逗号分隔，这样可以增强 <code>IO</code> 效率。这个参数的官方说明如下，默认目录是 <code>/tmp</code>。</p><blockquote><p>Directory to use for “scratch” space in Spark, including map output files and RDDs that get stored on disk. This should be on a fast, local disk in your system. It can also be a comma-separated list of multiple directories on different disks. NOTE: In Spark 1.0 and later this will be overriden by SPARK_LOCAL_DIRS (Standalone, Mesos) or LOCAL_DIRS (YARN) environment variables set by the cluster manager.</p></blockquote><p>综上所述，在生产环境中一定要确保磁盘空间充足和磁盘写权限，切记磁盘空间按需配置，不可乱用，运维侧也要加上磁盘相关的监控，有问题可以及时预警。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 异常参考：<a href="https://stackoverflow.com/questions/41238121/spark-java-ioexception-failed-to-create-local-dir-in-tmp-blockmgr" target="_blank" rel="noopener">stackoverflow</a> 。</p><p>官方文档参考：<a href="https://spark.apache.org/docs/1.6.2/configuration.html" target="_blank" rel="noopener">configuration-1.6.2</a> 。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;今天的 &lt;code&gt;Spark Streaming&lt;/code&gt; 程序莫名出现异常【对于一个 &lt;code&gt;task&lt;/code&gt; 来说，&lt;code&gt;Spark Streaming&lt;/code&gt; 会重试 4 次，全部重试都失败后整个 &lt;code&gt;Stage&lt;/code&gt; 才会失败】，紧接着 &lt;code&gt;task&lt;/code&gt; 中的 &lt;code&gt;batch&lt;/code&gt; 就会卡住不动【后来查到卡住是其它原因造成的】，使得整个 &lt;code&gt;Spark Streaming&lt;/code&gt; 任务进程进入到等待状态，所有的 &lt;code&gt;batch&lt;/code&gt; 都处于 &lt;code&gt;queued&lt;/code&gt; 状态，数据流无法继续执行。本文内容基于 &lt;code&gt;Spark v1.6.2&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Spark" scheme="https://www.playpi.org/tags/Spark/"/>
    
      <category term="Streaming" scheme="https://www.playpi.org/tags/Streaming/"/>
    
      <category term="IOException" scheme="https://www.playpi.org/tags/IOException/"/>
    
  </entry>
  
  <entry>
    <title>写入 Elasticsearch 异常：413 Request Entity Too Large</title>
    <link href="https://www.playpi.org/2019122901.html"/>
    <id>https://www.playpi.org/2019122901.html</id>
    <published>2019-12-29T15:53:41.000Z</published>
    <updated>2019-12-29T15:53:41.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:17 GMT+0800 (China Standard Time) --><p>在一个非常简单的业务场景中，偶尔出现异常：<code>413 Request Entity Too Large</code>，业务场景是写入数据到 <code>Elasticsearch</code> 中，异常日志中还有 <code>Nginx</code> 字样。</p><p>本文记录排查过程，本文环境基于 <code>Elasticsearch v5.6.8</code>，使用的写入客户端是 <code>elasticsearch-rest-high-level-client-5.6.8.jar</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 在后台日志中，发现异常信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">19/12/29 23:06:41 ERROR ESBulkProcessor: bulk [2307 : 1577632001467] 1000 request - 0 response - Unable to parse response body</span><br><span class="line">ElasticsearchStatusException [Unable to parse response body]; nested: ResponseException [POST http://your_ip_address:9200/_bulk?timeout=1m: HTTP/1.1 413 Request Entity Too Large</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;&lt;title&gt;413 Request Entity Too Large&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;center&gt;&lt;h1&gt;413 Request Entity Too Large&lt;/h1&gt;&lt;/center&gt;</span><br><span class="line">&lt;hr&gt;&lt;center&gt;nginx/1.16.1&lt;/center&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br><span class="line">];</span><br><span class="line">at org.elasticsearch.client.RestHighLevelClient.parseResponseException (RestHighLevelClient.java:506)</span><br><span class="line">at org.elasticsearch.client.RestHighLevelClient$1.onFailure (RestHighLevelClient.java:477)</span><br><span class="line">at org.elasticsearch.client.RestClient$FailureTrackingResponseListener.onDefinitiveFailure (RestClient.java:605)</span><br><span class="line">at org.elasticsearch.client.RestClient$1.completed (RestClient.java:362)</span><br><span class="line">at org.elasticsearch.client.RestClient$1.completed (RestClient.java:343)</span><br><span class="line">at org.apache.http.concurrent.BasicFuture.completed (BasicFuture.java:115)</span><br><span class="line">at org.apache.http.impl.nio.client.DefaultClientExchangeHandlerImpl.responseCompleted (DefaultClientExchangeHandlerImpl.java:173)</span><br><span class="line">at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.processResponse (HttpAsyncRequestExecutor.java:355)</span><br><span class="line">at org.apache.http.nio.protocol.HttpAsyncRequestExecutor.inputReady (HttpAsyncRequestExecutor.java:242)</span><br><span class="line">at org.apache.http.impl.nio.client.LoggingAsyncRequestExecutor.inputReady (LoggingAsyncRequestExecutor.java:87)</span><br><span class="line">at org.apache.http.impl.nio.DefaultNHttpClientConnection.consumeInput (DefaultNHttpClientConnection.java:264)</span><br><span class="line">at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady (InternalIODispatch.java:73)</span><br><span class="line">at org.apache.http.impl.nio.client.InternalIODispatch.onInputReady (InternalIODispatch.java:37)</span><br><span class="line">at org.apache.http.impl.nio.reactor.AbstractIODispatch.inputReady (AbstractIODispatch.java:113)</span><br><span class="line">at org.apache.http.impl.nio.reactor.BaseIOReactor.readable (BaseIOReactor.java:159)</span><br><span class="line">at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvent (AbstractIOReactor.java:338)</span><br><span class="line">at org.apache.http.impl.nio.reactor.AbstractIOReactor.processEvents (AbstractIOReactor.java:316)</span><br><span class="line">at org.apache.http.impl.nio.reactor.AbstractIOReactor.execute (AbstractIOReactor.java:277)</span><br><span class="line">at org.apache.http.impl.nio.reactor.BaseIOReactor.execute (BaseIOReactor.java:105)</span><br><span class="line">at org.apache.http.impl.nio.reactor.AbstractMultiworkerIOReactor$Worker.run (AbstractMultiworkerIOReactor.java:584)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">Suppressed: java.lang.IllegalStateException: Unsupported Content-Type: text/html</span><br><span class="line">at org.elasticsearch.client.RestHighLevelClient.parseEntity (RestHighLevelClient.java:523)</span><br><span class="line">at org.elasticsearch.client.RestHighLevelClient.parseResponseException (RestHighLevelClient.java:502)</span><br><span class="line">... 20 more</span><br><span class="line">Caused by: org.elasticsearch.client.ResponseException: POST http://your_ip_address:9200/_bulk?timeout=1m: HTTP/1.1 413 Request Entity Too Large</span><br><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;&lt;title&gt;413 Request Entity Too Large&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;center&gt;&lt;h1&gt;413 Request Entity Too Large&lt;/h1&gt;&lt;/center&gt;</span><br><span class="line">&lt;hr&gt;&lt;center&gt;nginx/1.16.1&lt;/center&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br><span class="line"></span><br><span class="line">at org.elasticsearch.client.RestClient$1.completed (RestClient.java:354)</span><br><span class="line">... 17 more</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191230002704.jpg" alt="异常信息" title="异常信息"></p><p>留意重点内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;html&gt;</span><br><span class="line">&lt;head&gt;&lt;title&gt;413 Request Entity Too Large&lt;/title&gt;&lt;/head&gt;</span><br><span class="line">&lt;body&gt;</span><br><span class="line">&lt;center&gt;&lt;h1&gt;413 Request Entity Too Large&lt;/h1&gt;&lt;/center&gt;</span><br><span class="line">&lt;hr&gt;&lt;center&gt;nginx/1.16.1&lt;/center&gt;</span><br><span class="line">&lt;/body&gt;</span><br><span class="line">&lt;/html&gt;</span><br></pre></td></tr></table></figure><p>看起来是发送的 <code>HTTP</code> 请求的请求体【<code>body</code>】过大，超过了服务端 <code>Nginx</code> 的配置，导致返回异常。</p><p>这个请求体过大，本质上还是将要写入 <code>Elasticsearch</code> 的文档过大，可见是某个字段的取值过大【这种情况一般都是异常数据导致的，例如采集系统把整个网页的内容全部抓回来作为正文，或者把网站反扒的干扰长文本全部抓回来作为正文】。</p><p>但是我又不禁想，这个配置参数名是什么呢？限制的最大字节数是多少呢？</p><h1 id="问题排查解决"><a href="# 问题排查解决" class="headerlink" title="问题排查解决"></a>问题排查解决 </h1><p> 在 <code>Elasticsearch</code> 官网查看相关配置项，发现有一个参数：<code>http.max_content_length</code>，表示一个 <code>HTTP</code> 请求的内容大小上限，默认为 <code>100MB</code>【对于 <code>v5.6</code> 来说】。</p><p>官网地址：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/5.6/modules-http.html" target="_blank" rel="noopener">elasticsearch 关于 HTTP 的配置 </a> ，以下为参数说明：</p><blockquote><p>The max content of an HTTP request. Defaults to 100mb. If set to greater than Integer.MAX_VALUE, it will be reset to 100mb.</p></blockquote><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191230005435.jpg" alt="HTTP 相关配置" title="HTTP 相关配置"></p><p> 这个参数是配置在 <code>elasticsearch.yml</code> 配置文件中的，我查看了我使用的 <code>Elasticsearch</code> 集群中对应的配置，没有发现参数的设置，说明使用了默认配置。</p><p>其实，<code>100MB</code> 对于文本来说很大了，一般正常的文本也不过只有几 <code>KB</code> 大小，对于长一点的文本来说，例如几万个字符，也就是几百 <code>KB</code>。由于写入 <code>Elasticsearch</code> 是批量的，1000 条数据一批，如果一批里面包含的全部是长文本，还是有可能超过 <code>100MB</code> 的，可见调整 <code>HTTP</code> 请求大小的上限是有必要的，或者是降低批次的数据量【会影响写入性能】。</p><p>此外，关于 <code>HTTP</code> 的另外两个参数也值得关注：<code>http.max_initial_line_length</code>、<code>http.max_header_size</code>。</p><p>前者表示 <code>HTTP</code> 请求链接的长度，默认为 <code>4KB</code>：</p><blockquote><p>The max length of an HTTP URL. Defaults to 4kB</p></blockquote><p>后者表示 <code>HTTP</code> 请求头的大小上限，默认为 <code>8KB</code>：</p><blockquote><p>The max size of allowed headers. Defaults to 8kB</p></blockquote><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:17 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在一个非常简单的业务场景中，偶尔出现异常：&lt;code&gt;413 Request Entity Too Large&lt;/code&gt;，业务场景是写入数据到 &lt;code&gt;Elasticsearch&lt;/code&gt; 中，异常日志中还有 &lt;code&gt;Nginx&lt;/code&gt; 字样。&lt;/p&gt;&lt;p&gt;本文记录排查过程，本文环境基于 &lt;code&gt;Elasticsearch v5.6.8&lt;/code&gt;，使用的写入客户端是 &lt;code&gt;elasticsearch-rest-high-level-client-5.6.8.jar&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="HTTP" scheme="https://www.playpi.org/tags/HTTP/"/>
    
      <category term="RestHighLevelClient" scheme="https://www.playpi.org/tags/RestHighLevelClient/"/>
    
  </entry>
  
  <entry>
    <title>大数据平台框架常用参数优化</title>
    <link href="https://www.playpi.org/2019121901.html"/>
    <id>https://www.playpi.org/2019121901.html</id>
    <published>2019-12-19T14:54:12.000Z</published>
    <updated>2019-12-19T14:54:12.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>本文记录大数据平台框架的一些常用参数，这些参数基本是我见过的或者实际使用过的，我会列出参数的含义以及使用效果，具有一定的参考意义。当然，根据实际的场景不同，参数值并不能随便设置为一样，必须要考虑到实际的情况，否则可能没有效果，或者具有反作用。</p><p>会保持更新。</p><a id="more"></a><h1 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h1><p>选择 <code>HBase</code>、<code>Hadoop</code> 时注意版本适配的问题，<code>Hadoop</code> 选择 <code>v2.7.1</code> 还是很好的，能适配 <code>HBase v1.2.x</code> 以及以上的版本【<code>Hbase</code> 兼容的 <code>Hadoop</code> 版本参见：<a href="http://hbase.apache.org/book.html#configuration" target="_blank" rel="noopener">hbase-configuration</a> 】，也能适配 <code>Hive v0.10.0</code> 以及以上的版本【<code>Hive</code> 兼容的 <code>Hadoop</code> 版本参见：<a href="http://hive.apache.org/downloads.html" target="_blank" rel="noopener">hive-downloads</a> 】。</p><h2 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h2><ul><li><code>fs.hdfs.impl.disable.cache</code>，如果设置为 <code>true</code>，表示关闭文件系统的缓存，这样多线程手动处理 <code>HDFS</code> 文件时，不会 <code>IOException: Filesystem closed</code></li></ul><h2 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h2><p>配置文件：<code>mapred-site.xml</code>。</p><p><code>Yarn</code> 资源模型：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Node Manager -&gt; yarn.nodemanager.resource.memory-mb</span><br><span class="line">YARN container -&gt; yarn.scheduler.minimum-allocation-mb、yarn.scheduler.maximum-allocation-mb</span><br><span class="line">Mapper/Reducer -&gt; mapreduce.map.memory.mb、mapreduce.reduce.memory.mb</span><br><span class="line">JVM -&gt; mapred.map.child.java.opts、mapred.reduce.child.java.opts</span><br></pre></td></tr></table></figure><p>对于内存参数的配置，注意它们之间的受限关系，取值不能乱设置，总体来说越具体的参数取值越小，例如常见的一般把 <code>mapreduce.map.java.opts</code> 的值配置成 <code>mapreduce.map.memory.mb * 0.9</code> 。</p><ul><li><code>map</code> 并发大小：<code>mapreduce.job.running.map.limit</code>，可以设置大点，50、100 随便 </li><li><code>map</code> 内存大小：<code>mapreduce.map.memory.mb</code>，单位为 <code>MB</code>，一般 <code>4GB</code> 够用</li><li><code>reduce</code> 启动延迟：<code>mapred.reduce.slowstart.completed.maps</code>，表示 <code>reduce</code> 在 <code>map</code> 执行到什么程度可以启动，例如设置为 <code>1.0</code> 表示等待 <code>map</code> 全部完成后才能执行 <code>reduce</code></li><li><code>reduce</code> 内存大小：<code>mapreduce.reduce.memory.mb</code>，单位为 <code>MB</code>，要根据实际情况设置，一般 <code>4GB</code> 够用</li><li><code>reduce</code> 虚拟内存：<code>yarn.nodemanager.vmem-pmem-ratio</code>，一般 2-5 即可</li><li><code>reduce</code> 并发大小：<code>mapreduce.job.running.reduce.limit</code>，一般 5-10 个够用【根据业务场景、机器资源而定】</li><li><code>mapred.map.child.java.opts</code>，<code>Map</code> 的 <code>JVM</code> 参数，例如：<code>-Xmx200m</code></li><li><code>mapred.reduce.child.java.opts</code>，<code>Reduce</code> 的 <code>JVM</code> 参数，例如：<code>-Xmx200m</code></li><li><code>mapreduce.admin.map.child.java.opts</code>，作用同 <code>mapred.map.child.java.opts</code>，优先级最高，会覆盖掉用户设置的</li><li><code>mapreduce.admin.reduce.child.java.opts</code>，作用同 <code>mapred.reduce.child.java.opts</code>，优先级最高，会覆盖掉用户设置的</li></ul><h2 id="es-hadoop"><a href="#es-hadoop" class="headerlink" title="es-hadoop"></a>es-hadoop</h2><p> 使用 <code>es-hadoop</code> 框架处理 <code>Elasticsearch</code> 数据，可以专注于数据 <code>ETL</code> 处理逻辑，其它与集群交互的读写操作交给 <code>es-hadoop</code> 框架处理，这里面有一些常用的参数。</p><p>参考官方文档：<a href="https://www.elastic.co/guide/en/elasticsearch/hadoop/current/configuration.html" target="_blank" rel="noopener">configuration</a> 。</p><ul><li>读取，只读取指定的字段：<code>es.read.field.include</code>，默认为空，读取全部字段，注意，在 <code>query</code> 中设置 <code>_source</code> 是无效的 </li><li> 读取，排除指定的字段：<code>es.read.field.exclude</code>，默认为空，则不排除任何字段 </li><li> 读取，关闭日期的处理：<code>es.mapping.date.rich</code>，默认为 <code>true</code>，关闭后，读取 <code>Elasticsearch</code> 的 <code>date</code> 类型的字段，会自动转换为 <code>long</code> 类型，不再是 <code>date</code> 类型 </li><li> 读取，解析指定字段为数组类型：<code>es.read.field.as.array.include</code>，默认为空，则不解析任何字段【字段类型保持原样】</li><li>读取，排除解析指定字段为数组字段：<code>es.read.field.as.array.exclude</code>，默认为空，则不排除任何字段【字段该是数组的还是数组，不是数组的仍旧保持原样】</li></ul><h2 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h2><p>配置文件：<code>mapred-site.xml</code> 。</p><ul><li><code>yarn.nodemanager.local-dirs</code>，临时目录 </li></ul><h1 id="HBase"><a href="#HBase" class="headerlink" title="HBase"></a>HBase</h1><h2 id="架构图"><a href="# 架构图" class="headerlink" title="架构图"></a> 架构图 </h2><p> 如下 </p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200407021026.png" alt="架构图" title="架构图"></p><h2 id="部分知识点"><a href="# 部分知识点" class="headerlink" title="部分知识点"></a> 部分知识点 </h2><p> 以下内容是关于 <code>major compaction</code>【大合并】、<code>minor compaction</code>【小合并】 的说明。</p><p><code>minor compaction</code> 操作只用来做部分文件【触发时相关的几个 <code>StoreFile</code> 文件】的合并操作，不做任何清除数据、多版本数据清理工作。</p><p><code>major compaction</code> 操作是对一个 <code>Region</code> 下的 <code>HStore</code> 下的所有 <code>StoreFile</code> 执行合并操作，最终的结果是整理合并出一个文件。此过程会真正删除标记为需要清理的数据，而且会消耗大量的磁盘 <code>IO</code>、网络 <code>IO</code>，甚至导致部分节点无法响应，严重影响读写性能，读请求会变慢，写请求会被阻塞。</p><p><code>major compaction</code> 的操作目的：</p><ul><li>合并文件 </li><li> 真正清除标记为删除、过期、多余版本的数据【<code>minor compaction</code> 并不会真正清除数据】</li><li>提高读写数据的效率，当然，由于磁盘 <code>IO</code>、网络 <code>IO</code> 的消耗，此操作过程会严重影响读写性能，读请求会变慢，写请求会被阻塞【有时候甚至会降低 10 倍】</li></ul><p>一般情况下，<code>HBase</code> 集群的 <code>major compact</code> 都是关闭的，如果开启默认是 7 天执行一次，因此离线的 <code>major compact</code> 是必要的，可以定期手动触发，可以使用 <code>major_compact 表名称 </code> 对某个表进行操作。如果手动触发，操作命令很快就返回结果，但是后台操作其实一直在运行，可以通过 <code>grafana</code> 监控查看压缩队列的长度，当压缩队列长度超过 100 的时候，应该延迟操作。由于 <code>major compact</code> 是很重的后台操作，因此操作之前需要有仔细的观察和分析，例如通过 <code>grafana</code> -&gt; <code>HBase</code> 监控可以获得，关于触发时期的选择建议：</p><ul><li> 业务低峰时段运行，即读写请求不大的时候，可以避免影响正常的业务 </li><li> 分表执行【或者分 <code>Region</code> 执行】，不要整个集群集体执行，并且优先考虑含有 <code>TTL</code> 的表 </li><li><code>StoreFile</code> 短期内增加比较多的时候</li><li> 表中 <code>StoreFile</code> 平均大小比较小的时候 </li></ul><h2 id="相关配置项"><a href="# 相关配置项" class="headerlink" title="相关配置项"></a> 相关配置项 </h2><p><code>HBase</code> 相关配置说明：</p><ul><li><code>hbase.hregion.majorcompaction</code>，<code>HBase</code> 自动做 <code>major compaction</code> 的周期，会严重影响写入性能，建议定期手动做</li><li><code>hbase.hregion.majorcompaction=0</code>，关闭定期的 <code>major compaction</code> 操作，必要时只能手动执行</li><li><code>hbase.client.retries.number</code>，客户端连接重试次数，建议设置大一点，例如 24</li><li><code>hbase.rootdir</code>，<code>HBase</code> 在 <code>HDFS</code> 中的根目录</li><li><code>zookeeper.znode.parent</code>，<code>HBase</code> 在 <code>Zookeeper</code> 的根目录，例如使用 <code>Phoenix</code> 登录时需要</li><li><code>hbase.hregion.max.filesize</code>，设置 <code>HBase</code> 分区大小，超过此值时自动分裂，避免一个分区过大，默认值 10GB【10737418240B】，在创建表时合理预估数据大小，预设置合理的分区规则【利用 <code>rowkey</code>】，可以避免频繁分裂，也使数据分布更加均匀</li></ul><h1 id="Elasticsearch"><a href="#Elasticsearch" class="headerlink" title="Elasticsearch"></a>Elasticsearch</h1><p> 总述，在设置 <code>Elasticsearch</code> 堆大小时需要通过 <code>$ES_HEAP_SIZE</code> 环境变量，遵循两个规则：</p><ul><li>不要超过可用 <code>RAM</code> 的 50%，<code>Lucene</code> 能很好利用文件系统的缓存，它是通过系统内核管理的，如果没有足够的文件系统缓存空间，性能会受到影响。 此外，专门用于堆的内存越多意味着其它可用的内存越少，例如 <code>fielddata</code></li><li>不要超过 <code>32GB</code>，如果堆大小小于 <code>32GB</code>，<code>JVM</code> 可以利用指针压缩，这可以大大降低内存的使用，每个指针是 4 字节而不是 8 字节 </li></ul><p> 分片的分配：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/shards-allocation.html" target="_blank" rel="noopener">shards-allocation</a> 。<br>脚本的使用：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-scripting-using.html" target="_blank" rel="noopener">modules-scripting-using</a> 。<br>熔断器相关：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/current/circuit-breaker.html" target="_blank" rel="noopener">circuit-breaker</a> 。<br>节点选举、故障检测：<a href="https://www.elastic.co/guide/en/elasticsearch/reference/6.8/modules-discovery-zen.html" target="_blank" rel="noopener">modules-discovery-zen</a> 。</p><p><code>fielddata</code>，对字段进行 <code>agg</code> 时，会把数据加载到内存中【索引数据时不会】，记录的是占用内存空间情况，超过指定的值，开始回收内存，防止 <code>OOM</code>。</p><p><code>allocate</code> 表示分片的分配【第一次分配、负载均衡过程中的再次分配】；<code>relocate</code> 【负载均衡过程中的再次分配】表示分片再次进行 <code>allocate</code>。<code>Recovery</code> 表示将一个索引的未分配 <code>shard</code> <code>allocate</code> 到一个结点的过程，在快照恢复、更改索引副本数量、结点故障、结点启动时发生。</p><p><code>Elasticsearch</code> 慢查询日志、慢索引日志等一些配置信息只在 <code>master</code> 节点配置即可，不需要每个节点都配置。</p><p>如果设置索引副本数为 1，同一个索引的主分片、副本分片不会被分配在同一个节点上面，这才能保证数据高可用，挂了一个节点也没关系【如果一台物理节点开启了两个 <code>Elasticsearch</code> 节点，需要注意使用 <code>cluster.routing.allocation.same_shard.host</code> 参数】。</p><ul><li>磁盘空间使用占比上限：<code>cluster.routing.allocation.disk.watermark.high</code>，默认为 90%，表示如果当前节点的磁盘使用占比超过这个值，则分片【针对所有类型的分片：主分片、副本分片】会被自动 <code>relocate</code> 到其它节点，并且任何分片都不会 <code>allocate</code> 到当前节点【此外，对于新创建的 <code>primary</code> 分片也是如此，除非整个 <code>Elasticsearch</code> 集群只有一个节点了】</li><li>磁盘空间使用占比下限：<code>cluster.routing.allocation.disk.watermark.low</code>，默认为 85%，表示如果当前节点的磁盘使用占比超过这个值，则分片【新创建的 <code>primary</code> 分片、从来没有进行过 <code>allocate</code> 的分片除外】不会被 <code>allocate</code> 到当前节点 </li><li> 索引的分片副本数：<code>number_of_replicas</code>，一般设置为 1，表示总共有 2 份数据 </li><li><code>index.auto_expand_replicas</code>：副本数自动扩展，会根据可用 <code>Elasticsearch</code> 节点数来设置副本数，默认为 <code>false</code>，可以设置为 <code>0-all</code>、<code>0-5</code> 等等</li><li> 每个节点分配的分片个数：<code>total_shards_per_node</code>，一般设置为 2，一个节点只分配 2 个分片，分别为主分片、副本分片 </li><li> 索引的分片个数：<code>number_of_shards</code>，当索引数据很大时，一般设置为节点个数【例如 <code>索引数据大小 / 50GB</code> 大于节点个数，例如 10 个节点，索引大小 <code>800GB</code>，此时按照官方建议应该设置 16 个分片，但是分片过多也不好，就可以设置 10 个分片，每个分片大小 <code>80GB</code>】，再配合 <strong>分片副本数为 1</strong>、<strong> 每个节点分配的分片个数为 2</strong>，就可以确保分片分配在所有的节点上面，并且每个节点上有 2 个分片，分别为主分片、副本分片 </li><li> 数据刷新时间：<code>refresh_interval</code>，表示数据写入后等待多久可以被搜索到，默认值 <code>1s</code>，每次索引的 <code>refresh</code> 会产生一个新的 <code>lucene</code> 段，这会导致频繁的合并行为，如果业务需求对实时性要求没那么高，可以将此参数调大，例如调整为 <code>60s</code>，会大大降低 <code>cpu</code> 的使用率 </li><li> 索引的分片大小，官方建议是每个分片大小在 <code>30GB</code> 到 <code>50GB</code> 不要超过 <code>50GB</code>，所以当索引的数据很大时，就要考虑增加分片的数量 </li><li> 设置 <code>terms</code> 最大个数：<code>index.max_terms_count</code>，默认最大个数为 65535，可以根据集群情况降低，例如设置为 10000，为了集群稳定，一般不需要设置那么大 </li><li> 设置 <code>Boolean Query</code> 的子语句数量：<code>indices.query.bool.max_clause_count</code>，默认为 1024，不建议增大这个值，也可以根据集群情况适当减小 </li><li> 查看热点线程：<code>http://your_ip:your_port/_nodes/your_node_name/hot_threads</code>，可以判断热点线程是 <code>search</code>，<code>bulk</code>，还是 <code>merge</code>，从而进一步分析是查询还是写入导致负载过高 </li><li> 数据目录：<code>path.data: /path/to/data</code>，多个目录使用逗号分隔，里面存放数据文件 </li><li> 日志目录：<code>path.logs: /path/to/logs</code>，里面存放的是节点的日志、慢查询日志、慢索引日志 </li><li> 家目录：<code>path.home: /path/to/home</code>，<code>elasticsearch</code> 的家目录，里面有插件、<code>lib</code>、配置文件等 </li><li> 插件目录：<code>path.plugins: /path/to/plugins</code>，插件目录，里面存放的是插件，例如：分词器 </li><li> 设置慢获取时间边界：<code>index.search.slowlog.threshold.fetch.warn: 30s</code>，超过这个时间的信息会被记录在日志文件中，<code>path.logs</code> 参数指定的目录中 <code>cluster-name_index_fetch_slowlog.log</code> 文件 </li><li> 设置慢查询时间边界：<code>index.search.slowlog.threshold.query.warn: 60s</code>，超过这个时间的信息会被记录在日志文件中，<code>path.logs</code> 参数指定的目录中 <code>cluster-name_index_search_slowlog.log</code> 文件 </li><li> 设置慢索引时间边界：<code>index.search.slowlog.threshold.index.warn: 60s</code>，超过这个时间的信息会被记录在日志文件中，<code>path.logs</code> 参数指定的目录中 <code>cluster-name_index_indexing_slowlog.log</code> 文件 </li><li> 禁止集群重分配：<code>cluster.routing.allocation.enable=none</code>，手动操作分片前需要关闭，否则会引起分片的移动，造成不必要的 <code>IO</code></li><li>开启集群重分配：<code>cluster.routing.allocation.enable=all</code>，集群的分片管理权限交由集群，保持数据均衡 </li><li> 设置集群均衡分片时可以同时 <code>rebalance</code> 分片的个数，<code>cluster.routing.allocation.cluster_concurrent_rebalance:2</code>，不宜设置过大，一般 2-4 个为好，当然如果集群资源足够或者需要快速均衡分片，可以设置大一点 </li><li> 允许分片分配，<code>cluster.routing.allocation.enable=all</code>，开启后分片的分配交由集群管理，如果偶尔需要手动管理分片或者集群停机重启，可以临时关闭，取值设置为 <code>none</code> 即可 </li><li> 推迟索引的分片分配时间，在分片节点出故障或者重启时，可以避免分片数据的移动，前提是及时把节点恢复：<code>index.unassigned.node_left.delayed_timeout=5m</code>，通俗点说，就是趁分片不注意，节点已经恢复了，此时数据分片保持不变，避免了不必要的 <code>IO</code></li><li><code>index.max_slices_per_scroll</code>，除了传统的 <code>scroll</code> 读取数据的方式，<code>v5.x</code> 之后 <code>Elasticsearch</code> 又增加了对每个分片读取数据的功能，称之为切片处理【<code>sliced scroll</code>】，这种读取方式可以对多个分片并行读取数据，大大提高了取数效率，<code>elasticsearch-hadoop</code> 就是采用这种方式读取数据的。但是，这里面有一个限制，<code>Elasticsearch</code> 默认一个 <code>scroll</code> 最大的切片数量为 1024【一般小于等于分片数，也可以通过指定切片字段来创建大于分片数的切片】，可以通过 <code>index.max_slices_per_scroll</code> 参数来变更【不建议更改】</li><li><code>cluster.routing.allocation.same_shard.host</code>，在单台物理节点配置多个 <code>Elasticsearch</code> 实例时，这个参数才生效，用来检查同一个分片的多个实例【主分片、副本分片】是否能分配在同一台主机上面，默认值为 <code>false</code>。如果设置为 <code>true</code>，表示开启检查机制，一台物理机上面启动 2 个 <code>Elasticsearch</code> 节点，则分配相同编号的分片时，不会都在这台机器上面，尽管可以满足主分片、副本分片不在同一个 <code>Elasticsearch</code> 节点上 </li><li><code>script.groovy.sandbox.enabled: false</code>，禁用 <code>Grovvy</code> 脚本，默认是关闭的</li><li><code>script.inline: false</code>，允许使用内置 <code>painless</code> 脚本</li><li><code>script.stored: false</code>，允许使用保存在 <code>config/scripts</code> 中的脚本，调用时使用 <code>id</code> 即可，类似方法名</li><li><code>script.file: false</code>，允许使用外部脚本文件</li><li><code>http.port: 9200</code>，集群的 <code>HTTP</code> 端口号</li><li><code>transport.tcp.port: 9300</code>，集群的 <code>TCP</code> 端口号</li><li><code>thread_pool.bulk.queue_size: 1500</code>，<code>bulk</code> 队列的大小</li><li><code>indices.breaker.total.use_real_memory: true</code>，熔断器回收内存，防止 <code>OOM</code>，决定父熔断器是考虑实际内存使用情况，还是仅考虑子熔断器内存使用情况</li><li><code>indices.breaker.total.limit: 70%</code>，熔断器回收内存，防止 <code>OOM</code>，当 <code>use_real_memory</code> 为 <code>true</code> 时，设置为 70%，否则默认为 70%</li><li><code>indices.breaker.fielddata.limit: 40%</code>，熔断器回收内存，防止 <code>OOM</code>，默认 40%</li><li><code>indices.breaker.request.limit: 60%</code>，熔断器回收内存，防止 <code>OOM</code>，默认 60%</li><li><code>indices.fielddata.cache.size</code>，可以设置 <code>20%</code>，要低于 <code>fielddata.limit</code>，默认无界限</li><li><code>discovery.zen.fd.ping_timeout: 60s</code>，集群故障检测</li><li><code>discovery.zen.fd.ping_interval: 10s</code>，集群故障检测</li><li><code>discovery.zen.fd.ping_retries: 10</code>，集群故障检测</li><li><ul><li><code>discovery.zen.master_election.ignore_non_master_pings: true</code>，选举主节点，设置为 <code>true</code> 时非 <code>node.master</code> 节点不能参与选举，投票也无效</li></ul></li><li><code>discovery.zen.minimum_master_nodes: 2</code>，选举主节点，最少有多少个备选主节点参加选举，防止脑裂现象</li><li><code>discovery.zen.ping_timeout: 10s</code>，选举主节点</li><li><code>discovery.zen.ping.unicast.hosts: [&quot;ip1:port&quot;,&quot;ip2:port&quot;]</code>，选举主节点，主机列表</li><li><code>node.data: true</code>，数据节点</li><li><code>node.master: true</code>，有资格被选举为主节点</li><li><code>action.destructive_requires_name=true</code>，设置严格校验，对于删除数据、删除索引的破坏性行为进行严格校验，不支持通配符，防止类似于 <code>rm -rf /*</code> 的悲剧</li><li><code>network.host</code>，绑定主机名或者 <code>ip</code> 地址，用于向集群广播自己</li><li><code>cluster.routing.allocation.exclude._ip</code>，临时下线节点，类似于黑名单，分片不会往指定的主机移动，同时会把分片从指定的节点全部移除，最终可以下线该节点，可通过 <code>put transient</code> 设置临时生效</li></ul><h1 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h1><p><a href="https://spark.apache.org/docs/1.6.2/running-on-yarn.html" target="_blank" rel="noopener">v1.6.2 官方说明文档</a> 。</p><ul><li> 序列化方式：<code>spark.serializer</code>，可以选择：<code>org.apache.spark.serializer.KryoSerializer</code></li><li><code>executor</code> 附加参数：<code>spark.executor.extraJavaOptions</code>，例如可以添加：<code>-Dxx=yy</code>【如果仅仅在 <code>driver</code> 端设置，<code>executor</code> 是不会有的】</li><li><code>driver</code> 附加参数：<code>spark.driver.extraJavaOptions</code>，例如可以添加：<code>-Dxx=yy</code></li><li>日志配置文件设置，<code>Spark</code> 使用的是 <code>log4j</code>，默认在 <code>Spark</code> 安装目录的 <code>conf</code> 下面，如果想要增加 <code>log4j</code> 相关配置，更改 <code>driver</code> 机器上面的 <code>log4j.properties</code> 配置文件是无效的，必须把所有的 <code>executor</code> 节点上的配置文件全部更新。如果没有权限，也可以自己上传配置文件，然后需要在 <code>executor</code> 附加参数中指定：<code>-Dlog4j.configuration=file:/path/to/file</code>，启动 <code>Spark</code> 任务时还需要使用 <code>--files</code> 指定配置文件名称，多个用逗号分隔，用来上传配置文件到 <code>Spark</code> 节点 </li><li> 开启允许多 <code>SparkContext</code> 存在：<code>spark.driver.allowMultipleContexts</code>，设置为 <code>true</code> 即可，在使用多个 <code>SparkContext</code> 时，需要先停用当前活跃的，使用 <code>stop</code> 方法【在 <code>Spark v2.0</code> 以及以上版本，已经取消了这个限制】</li><li><code>spark.executor.cores</code>，每个执行器上面的占用核数，会消耗 <code>CPU</code>，一般设置为 2-3</li><li><code>spark.executor.memory</code>，执行器上面的堆内存大小，一般设置为 <code>2048M</code>、<code>4096M</code></li><li><code>spark.port.maxRetries</code>，提交任务的 <code>Spark UI</code> 重试次数 </li><li><code>spark.default.parallelism</code>，默认并行度</li><li><code>spark.cores.max</code>，最大核心数</li><li><code>spark.executor.logs.rolling.strategy</code></li><li><code>spark.executor.logs.rolling.maxRetainedFiles</code></li><li><code>spark.executor.logs.rolling.size.maxBytes</code></li><li><code>spark.ui.showConsoleProgress</code></li><li><code>spark.yarn.max.executor.failures</code>，<code>task</code> 失败重试次数，默认为 <code>spark.executor.cores</code> 的 2 倍，最小值为 3，如果重试最大次数后 <code>task</code> 仍旧失败，则整个 <code>Application</code> 执行失败【容错性】</li><li><code>spark.yarn.maxAppAttempts</code>，提交申请的最大尝试次数，小于等于 <code>yarn</code> 配置中的全局最大尝试次数，尝试最大次数后仍旧无法提交，则 <code>Application</code> 提交失败【<code>yarn</code> 配置为 <code>yarn.resourcemanager.am.max-attempts</code>，默认为 2，即有 2 次提交机会】</li></ul><h2 id="Kafka- 输入数据源"><a href="#Kafka- 输入数据源" class="headerlink" title="Kafka 输入数据源"></a>Kafka 输入数据源</h2><p><code>Spark Streaming</code> 配置：</p><ul><li><code>spark.streaming.backpressure.enabled</code>，开启反压机制</li><li><code>spark.streaming.backpressure.pid.minRate</code>，</li><li><code>spark.streaming.kafka.maxRatePerPartition</code>，每个 <code>partition</code> 的最大读取速度，单位秒，一般设置 500-100 即可</li><li><code>spark.streaming.receiver.maxRate</code>，<code>receiver</code> 最大处理数据量，单位秒，与 <code>maxRatePerPartition</code>、<code>Durations</code> 有关，实际运行时由于反压机制，数据处理速度会低于这个值</li><li><code>spark.streaming.receiver.writeAheadLog.enable</code>，</li><li><code>spark.streaming.stopGracefullyOnShutdown</code>，优雅地退出</li><li><code>spark.streaming.gracefulStopTimeout</code>，</li><li><code>xx</code>，</li></ul><p><code>Kakfa</code> 配置：</p><ul><li><code>metadata.broker.list</code>，</li><li><code>offsets.storage</code>，设置为 <code>kafka</code></li><li><code>zookeeper.connect</code>，</li><li><code>zookeeper.connection.timeout.ms</code></li><li><code>group.id</code></li><li><code>fetch.message.max.bytes</code></li><li><code>auto.offset.reset</code>，消费的起始位置，这个参数高低版本之间的名称、值都会不同，需要注意</li><li><code>consumer.timeout.ms</code></li><li><code>rebalance.max.retries</code></li><li><code>rebalance.backoff.ms</code></li></ul><h2 id="集群参数"><a href="# 集群参数" class="headerlink" title="集群参数"></a> 集群参数 </h2><p><code>yarn</code> 集群：</p><ul><li><code>yarn.resourcemanager.am.max-attempts</code>，最大应用尝试次数，它是所有 <code>AM</code> 的全局设置，每个应用都可以通过 <code>API</code> 的参数指定其各自的最大应用尝试次数【参数 <code>spark.yarn.maxAppAttempts</code>】，但是单个数字不能超过这个全局上限，如果超过了，资源管理器将覆盖它。默认数量设置为 2，以允许至少 1 次重试，即有 2 次提交的机会</li></ul><p><code>standalone</code> 集群</p><ul><li> 临时目录：<code>SPARK_LOCAL_DIRS</code>，用来存放 <code>Spark</code> 任务运行过程中的临时数据，例如内存不足时把数据缓存到磁盘，就会有数据写入这个目录，当然，在启动 <code>Spark</code> 任务时也可以单独指定，但是最好还是设置在集群上面，可以在 <code>spark-env.sh</code> 脚本中设置，键值对的形式，例如：<code>SPARK_LOCAL_DIRS=/your_path/spark/local</code>。需要注意的是，启动 <code>Excutor</code> 的用户必须有这个目录的写权限，并且保证这个目录的磁盘空间足够使用，否则在 <code>Spark</code> 任务中会出现异常：<code>java.io.IOException: Failed to create local dir in xx</code>，进而导致 <code>Task</code> 失败 </li><li><code>Work</code> 目录：<code>SPARK_WORKER_DIR</code>，用来存放 <code>Work</code> 的信息，设置方式同上面的 <code>SPARK_LOCAL_DIRS</code>，如果 <code>Spark</code> 任务里面有 <code>System.out ()</code>，输出的内容在此目录下</li><li><code>SPARK_LOG_DIR</code>，<code>Spark</code> 集群自身的日志文件，例如 <code>Work</code> 接收 <code>Spark</code> 任务后通信的内容</li></ul><h1 id="Storm"><a href="#Storm" class="headerlink" title="Storm"></a>Storm</h1><ul><li><code>StormUI nimbus</code> 内容传输大小限制：<code>nimbus.thrift.max_buffer_size: 1048576</code>，取值的单位是字节，默认为 <code>1048576</code>，如果 <code>nimbus</code> 汇报的内容过多，超过这个值，则在 <code>StormUI</code> 上面无法查看 <code>Topology Summary</code> 信息，会报错：<code>Internal Server Error org.apache.thrift7.transport.TTransportException: Frame size (3052134) larger than max length (1048576)</code></li><li> 执行实例 <code>worker</code> 对应的端口号：<code>supervisor.slots.ports:</code>，可以设置多个，和 <code>CPU</code> 的核数一致，或者稍小，提高机器资源的使用率 </li><li><code>worker</code> 的 <code>JVM</code> 参数：<code>WORKER_GC_OPTS</code>，取值参考：<code>-Xms1G -Xmx5G -XX:+UseG1GC</code>，根据集群机器的资源多少而定，<code>G1</code> 是一种垃圾回收器</li><li><code>supervisor</code> 的 <code>JVM</code> 参数：<code>SUPERVISOR_GC_OPTS</code>，取值参考：<code>-Xms1G -Xmx5G -XX:+UseG1GC</code>，根据集群机器的资源多少而定，<code>G1</code> 是一种垃圾回收器</li><li><code>Storm UI</code> 的服务端口：<code>ui.port</code>，可以使用浏览器打开网页查看 <code>Topology</code> 详细信息</li><li><code>ZooKeeper</code> 服务器列表：<code>storm.zookeeper.servers</code></li><li><code>ZooKeeper</code> 连接端口：<code>storm.zookeeper.port</code></li><li><code>ZooKeeper</code> 中 <code>Storm</code> 的根目录位置：<code>storm.zookeeper.root</code>，用来存放 <code>Storm</code> 集群元信息</li><li> 客户端连接 <code>ZooKeeper</code> 超时时间：<code>storm.zookeeper.session.timeout</code></li><li><code>Storm</code> 使用的本地文件系统目录：<code>storm.local.dir</code>，注意此目录必须存在并且 <code>Storm</code> 进程有权限可读写 </li><li><code>Storm</code> 集群运行模式：<code>storm.cluster.mode</code>，取值可选：<code>distributed</code>、<code>local</code></li></ul><h1 id="Kafka"><a href="#Kafka" class="headerlink" title="Kafka"></a>Kafka</h1><p> 注意，<code>Kafka</code> 的不同版本参数名、参数值会有变化，特别是 <code>v0.9.x</code> 之后，与之前的低版本差异很大，例如数据游标的参数可以参考我的另外一篇博文：<a href="https://www.playpi.org/2017060101.html">记录一个 Kafka 错误：OffsetOutOfRangeException</a> ，<code>Kafka</code> 官网参见：<a href="https://kafka.apache.org/090/documentation.html#consumerconfigs" target="_blank" rel="noopener">Kafka-v0.9.0.x-configuration</a> 。</p><p>配置优化都是修改 <code>server.properties</code> 文件中参数值。</p><ul><li><code>JVM</code> 参数：<code>KAFKA_HEAP_OPTS</code>，取值参考：<code>-Xmx2G</code></li><li>文件存放位置：<code>log.dirs</code>，多个使用逗号分隔，注意所有的 <code>log</code> 级别需要设置为 <code>INFO</code></li><li>单个 <code>Topic</code> 的文件保留策略：<code>log.retention.hours=72</code>【数据保留 72 小时，超过时旧数据被删除】，<code>log.retention.bytes=1073741824</code>【数据保留 1GB，超过时旧数据被删除】</li><li>数据文件刷盘策略：<code>log.flush.interval.messages=10000</code>【每当 <code>producer</code> 写入 10000 条消息时，刷数据到磁盘】，<code>log.flush.interval.ms=1000</code>【每间隔 1 秒钟时间，刷数据到磁盘】</li><li><code>Topic</code> 的分区数量：<code>num.partitions=8</code></li><li>启动 <code>Fetch</code> 线程给副本同步数据传输大小限制：<code>replica.fetch.max.bytes=10485760</code>，要比 <code>message.max.bytes</code> 大 </li><li><code>message.max.bytes=10485700</code>，这个参数决定了 <code>broker</code> 能够接收到的最大消息的大小，要比 <code>max.request.size</code> 大</li><li><code>max.request.size=10480000</code>，这个参数决定了 <code>producer</code> 生产消息的大小</li><li><code>fetch.max.bytes=10485760</code>，这个参数决定了 <code>consumer</code> 消费消息的大小，要比 <code>message.max.bytes</code> 大</li><li><code>broker</code> 处理消息的最大线程数：<code>num.network.threads=17</code>，一般 <code>num.network.threads</code> 主要处理网络 <code>IO</code>，读写缓冲区数据，基本没有 <code>IO</code> 等待，配置线程数量为 <code>CPU</code> 核数加 1</li><li><code>broker</code> 处理磁盘 <code>IO</code> 的线程数：<code>num.io.threads=32</code>，<code>num.io.threads</code> 主要进行磁盘 <code>IO</code> 操作，高峰期可能有些 <code>IO</code> 等待，因此配置需要大些，配置线程数量为 <code>CPU</code> 核数 2 倍，最大不超过 3 倍</li><li> 强制新建一个 <code>segment</code> 的时间：<code>log.roll.hour=72</code></li><li>是否允许自动创建 <code>Topic</code>：<code>auto.create.topics.enable=true</code>，如果设置为 <code>false</code>，则代码无法创建，需要通过 <code>kafka</code> 的命令创建 <code>Topic</code></li><li><code>auto.offset.reset</code>，关于数据游标的配置【<code>earliest</code> 与 <code>latest</code>、<code>smallest</code>、<code>largest</code>】，由于不同版本之间的差异，可以参考：<a href="https://www.playpi.org/2017060101.html">记录一个 Kafka 错误：OffsetOutOfRangeException</a></li><li><code>advertised.host.name</code>、<code>advertised.port</code>，关于外网集群可以访问的配置，跨网络生产、消费数据，<code>v082</code> 以及之前的版本【之后的版本有保留这两个参数，但是不建议使用】</li><li><code>advertised.listeners</code>、<code>listeners</code>，关于外网集群可以访问的配置，跨网络生产、消费数据，<code>v090</code> 以及之后的版本 </li></ul><p> 留意参数取值大小的限制：<code>fetch.max.bytes</code> 大于 <code>message.max.bytes</code> 大于 <code>max.request.size</code>，<code>replica.fetch.max.bytes</code> 大于 <code>message.max.bytes</code> 大于 <code>max.request.size</code>。</p><h1 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h1><p>配置 <code>zoo.cfg</code> 文件：</p><ul><li><code>dataDir</code>，表示快照日志目录</li><li><code>dataLogDir</code>，表示事务日志目录，不配置的时候事务日志目录同 <code>dataDir</code></li><li><code>clientPort=2181</code>，服务的监听端口</li><li><code>tickTime=2000</code>，<code>Zookeeper</code> 的时间单元，<code>Zookeeper</code> 中所有时间都是以这个时间单元的整数倍去配置的，例如，<code>session</code> 的最小超时时间是 <code>2*tickTime</code>【单位：毫秒】</li><li><code>syncLimit=5</code>，表示 <code>Follower</code> 和 <code>Observer</code> 与 <code>Leader</code> 交互时的最大等待时间，只不过是在与 <code>leader</code> 同步完毕之后，进入正常请求转发或 <code>ping</code> 等消息交互时的超时时间</li><li><code>initLimit=10</code>，<code>Observer</code> 和 <code>Follower</code> 启动时，从 <code>Leader</code> 同步最新数据时，<code>Leader</code> 允许 <code>initLimit * tickTime</code> 的时间内完成，如果同步的数据量很大，可以相应地把这个值设置大一些</li><li><code>maxClientCnxns=384</code>，最大并发客户端数，用于防止 <code>Ddos</code> 的，默认值是 10，设置为 0 是不加限制</li><li><code>maxSessionTimeout=120000</code>，<code>Session</code> 超时时间限制，如果客户端设置的超时时间不在这个范围，那么会被强制设置一个最大时间，默认的 <code>Session</code> 超时时间是在 <code>2 * tickTime ~ 20 * tickTime</code> 这个范围</li><li><code>minSessionTimeout=4000</code>，同 <code>maxSessionTimeout</code></li><li><code>server.x=hostname:2888:3888</code>，<code>x</code> 是一个数字，与每个服务器的 <code>myid</code> 文件中的 <code>id</code> 是一样的，<code>hostname</code> 是服务器的 <code>hostname</code>，右边配置两个端口，第一个端口用于 <code>Follower</code> 和 <code>Leader</code> 之间的数据同步和其它通信，第二个端口用于 <code>Leader</code> 选举过程中投票通信</li><li><code>autopurge.purgeInterval=24</code>，在 <code>v3.4.0</code> 及之后的版本，<code>Zookeeper</code> 提供了自动清理事务日志文件和快照日志文件的功能，这个参数指定了清理频率，单位是小时，需要配置一个 1 或更大的整数。默认是 0，表示不开启自动清理功能</li><li><code>autopurge.snapRetainCount=30</code>，参数指定了需要保留的事务日志文件和快照日志文件的数目，默认是保留 3 个，和 <code>autopurge.purgeInterval</code> 搭配使用</li></ul><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;本文记录大数据平台框架的一些常用参数，这些参数基本是我见过的或者实际使用过的，我会列出参数的含义以及使用效果，具有一定的参考意义。当然，根据实际的场景不同，参数值并不能随便设置为一样，必须要考虑到实际的情况，否则可能没有效果，或者具有反作用。&lt;/p&gt;&lt;p&gt;会保持更新。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Elasticsearch" scheme="https://www.playpi.org/tags/Elasticsearch/"/>
    
      <category term="Hadoop" scheme="https://www.playpi.org/tags/Hadoop/"/>
    
      <category term="Spark" scheme="https://www.playpi.org/tags/Spark/"/>
    
      <category term="HDFS" scheme="https://www.playpi.org/tags/HDFS/"/>
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="Zookeeper" scheme="https://www.playpi.org/tags/Zookeeper/"/>
    
      <category term="MapReduce" scheme="https://www.playpi.org/tags/MapReduce/"/>
    
      <category term="Kafka" scheme="https://www.playpi.org/tags/Kafka/"/>
    
      <category term="Storm" scheme="https://www.playpi.org/tags/Storm/"/>
    
  </entry>
  
  <entry>
    <title>解决 jar 包冲突的神器：maven-shade-plugin</title>
    <link href="https://www.playpi.org/2019120101.html"/>
    <id>https://www.playpi.org/2019120101.html</id>
    <published>2019-11-30T16:54:21.000Z</published>
    <updated>2019-11-30T16:54:21.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>最近因为协助升级相关业务的 <code>sdk</code>，遇到过多次 <code>jar</code> 包冲突的问题，此外自己在业务中升级算法接口的 <code>sdk</code> 时，也遇到过 <code>jar</code> 冲突问题。而且，这种冲突是灾难性的，不要指望通过排除特定包、升级版本、降级版本解决，根本无济于事，还会越来越混乱。</p><p>那么，最高效的方法是使用 <code>maven-shade-plugin</code> 插件，只要加上冲突相关的 <code>relocation</code> 配置，变更包名，即可迅速化解冲突的问题。</p><a id="more"></a><p>在此提前说明，下文中涉及的代码已经被我上传至 <code>GtiHub</code>：<a href="https://github.com/iplaypi/iplaypistudy-shade" target="_blank" rel="noopener">iplaypistudy-shade</a> ，特别独立创建了一个 <code>Maven</code> 小项目，专供演示使用，读者可以提前下载使用。</p><h1 id="前提场景"><a href="# 前提场景" class="headerlink" title="前提场景"></a>前提场景 </h1><p> 在 <code>Maven</code> 项目中，当功能越来越丰富，需要的第三方依赖也就越来越多，此时很容易发生 <code>jar</code> 包冲突。而通常是因为，<code>Mavne</code> 项目中依赖了同一个 <code>jar</code> 包的多个版本，即坐标版本号不同。</p><p>一般的思路是只保留一个版本，删除掉不需要的版本，但是在复杂情况下，版本之间不兼容，不可能就这么删掉某一个【因为多个 <code>jar</code> 分别被引用了不同的方法】，所以这种思路行不通。</p><p>例如我最近遇到了一个下图这样的例子【本文开头指定的 <code>GitHub</code> 源代码可以直接下载】：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208195230.png" alt="依赖冲突的项目结构" title="依赖冲突的项目结构"></p><p>其中，<code>module-a</code>、<code>module-b</code>、<code>module-c</code> 是我项目中的三个模块，<code>module-a</code> 同时依赖了子模块 <code>module-b</code> 和 <code>module-c</code>，这个很容易理解。</p><p>但是，在 <code>module-b</code>、<code>module-c</code> 中分别依赖了不同版本的 <code>guava</code>，并且在代码中有实际调用不兼容的方法，高版本的方法在低版本中不存在，低版本的方法在高版本中不存在【这属于 <code>guava</code> 没有做到向前兼容的问题】。</p><p>代码具体内容在后面的演示中会详细描述，这里先探讨一下这种情况该怎么办。</p><p>如果排除掉 <code>guava v19.0</code> 的话【使用 <code>exclude</code> 特性】，<code>module-b</code> 会报错，如果排除掉 <code>guava v26.0-jre</code> 的话，<code>module-c</code> 会报错，但是我又希望在项目中可以同时使用 <code>guava v19.0</code> 和 <code>guava v26.0-jre</code>，为了功能考虑也必须同时使用，不能排除任何一个。</p><p>好像陷入了僵局，反正我一开始是没有什么好办法的，直到有一位同事，在我旁边偶尔提了一句，你可以使用 <code>maven-shade-plugin</code> 插件，能完美解决你这个需求场景，方便快捷，毫无痛苦。</p><p>我自己先去了解了一下，后来又听他解释了一遍，才恍然大悟，感觉技术观念再一次被刷新了，居然还有这种操作。</p><p>下面就简单描述一下具体怎么使用 <code>maven-shade-plugin</code> 插件解决这个问题。</p><h1 id="解决方案演示"><a href="# 解决方案演示" class="headerlink" title="解决方案演示"></a>解决方案演示 </h1><h2 id="案例说明"><a href="# 案例说明" class="headerlink" title="案例说明"></a> 案例说明 </h2><p> 由于是演示 <code>maven-shade-plugin</code> 插件的使用，所以仅仅只有几行核心代码、几个核心依赖，但是完全可以表达出解决冲突的思路，源代码请读者从本文开头指定的 <code>GitHub</code> 链接下载。</p><p>如上图所示，<code>module-a</code>、<code>module-b</code>、<code>module-c</code> 是我项目中的三个模块，<code>module-a</code> 同时依赖了子模块 <code>module-b</code> 和 <code>module-c</code>。在子模块 <code>module-b</code> 中，依赖了 <code>guava v19.0</code>，在 子模块 <code>module-c</code> 中，依赖了 <code>guava v26.0-jre</code>。</p><p>好，接下来重点来了，在 <code>guava</code> 的两个版本中有下面两个不兼容的方法，特意挑选出来，用来测试：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 这个方法在 v19.0 中有，在 v26.0-jre 中没有 </span><br><span class="line">@CheckReturnValue</span><br><span class="line">  @Deprecated</span><br><span class="line">  public static ToStringHelper toStringHelper (Object self) &#123;</span><br><span class="line">return new ToStringHelper (self.getClass ().getSimpleName ());</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 这个方法在 v26.0-jre 中有，在 v19.0 中没有 </span><br><span class="line">public static String lenientFormat (@Nullable String template, @Nullable Object... args) &#123;</span><br><span class="line">...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当然，如果在 <code>module-b</code>、<code>module-c</code> 的依赖 <code>jar</code> 源码中有调用到，也是可以的，但是不直观，而且依赖 <code>jar</code> 的方法也不一定会执行，不好控制，所以我选择手动显式写代码调用的方式来演示。</p><h2 id="代码清单"><a href="# 代码清单" class="headerlink" title="代码清单"></a>代码清单 </h2><p> 演示代码主要内容如下。</p><p>在 <code>module-b</code> 中有一个类 <code>ModuleBRun</code>，调用了 <code>toStringHelper ()</code> 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">@Slf4j</span><br><span class="line">public class ModuleBRun &#123;</span><br><span class="line">public static void main (String [] args) &#123;</span><br><span class="line">log.info (&quot;====Hello World!&quot;);</span><br><span class="line">run ();</span><br><span class="line">&#125;</span><br><span class="line">public static void run () &#123;</span><br><span class="line">// 这个方法在 v19.0 中有，在 v26.0-jre 中没有 </span><br><span class="line">log.info (&quot;==== 开始执行 module-b 的代码 & quot;);</span><br><span class="line">Objects.ToStringHelper toStringHelper = Objects.toStringHelper (new Object ());</span><br><span class="line">toStringHelper.add (&quot;in&quot;, &quot;in&quot;);</span><br><span class="line">toStringHelper.add (&quot;out&quot;, &quot;out&quot;);</span><br><span class="line">log.info (&quot;====[&#123;&#125;]&quot;, toStringHelper.toString ());</span><br><span class="line">log.info (&quot;====module-b 的代码执行完成 & quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208015421.png" alt="ModuleBRun" title="ModuleBRun"></p><p>在 <code>module-c</code> 中有一个类 <code>ModuleCRun</code>，调用了 <code>lenientFormat ()</code> 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">@Slf4j</span><br><span class="line">public class ModuleCRun &#123;</span><br><span class="line">public static void main (String [] args) &#123;</span><br><span class="line">log.info (&quot;====Hello World!&quot;);</span><br><span class="line">run ();</span><br><span class="line">&#125;</span><br><span class="line">public static void run () &#123;</span><br><span class="line">log.info (&quot;==== 开始执行 module-c 的代码 & quot;);</span><br><span class="line">// 这个方法在 v26.0-jre 中有，在 v19.0 中没有 </span><br><span class="line">log.info (&quot;====[&#123;&#125;]&quot;, Strings.lenientFormat (&quot;&quot;, &quot;in&quot;, &quot;out&quot;));</span><br><span class="line">log.info (&quot;====module-c 的代码执行完成 & quot;);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208015600.png" alt="ModuleCRun" title="ModuleCRun"></p><p>在 <code>module-a</code> 中有一个类 <code>ModuleARun</code>，有一个 <code>run ()</code> 方法，分别调用了上面的 <code>ModuleBRun.run ()</code>、<code>ModuleCRun.run ()</code>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * 依赖 b/c 时，无法成功运行 </span><br><span class="line">     * &lt;p&gt;</span><br><span class="line">     * 依赖 b/c-shade 时，可以成功运行 </span><br><span class="line">     */</span><br><span class="line">public static void run () &#123;</span><br><span class="line">log.info (&quot;==== 开始执行 module-a 的代码 & quot;);</span><br><span class="line">ModuleBRun.run ();</span><br><span class="line">ModuleCRun.run ();</span><br><span class="line">log.info (&quot;====module-a 的代码执行完成 & quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208015629.png" alt="ModuleARun" title="ModuleARun"></p><h2 id="运行效果"><a href="# 运行效果" class="headerlink" title="运行效果"></a>运行效果 </h2><p> 此时，尝试本地调试运行 <code>ModuleARun.run ()</code>，或者使用 <code>Maven</code> 打 <code>jar</code> 包后运行：<code>java -jar iplaypistudy-shade-module-a-1.0-SNAPSHOT-jar-with-dependencies.jar</code>，需要提前使用 <code>maven-shade-plugin</code> 配置 <code>mainClass</code> 后打包。</p><p>可以发现如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleARun:13: ====Hello World!</span><br><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleARun:24: ==== 开始执行 module-a 的代码 </span><br><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleBRun:19: ==== 开始执行 module-b 的代码 </span><br><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleBRun:23: ====[Object&#123;in=in, out=out&#125;]</span><br><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleBRun:24: ====module-b 的代码执行完成 </span><br><span class="line">2020-02-08_01:27:25 [main] INFO study.ModuleCRun:18: ==== 开始执行 module-c 的代码 </span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.google.common.base.Strings.lenientFormat (Ljava/lang/String;[Ljava/lang/Object;) Ljava/lang/String;</span><br><span class="line">at org.playpi.study.ModuleCRun.run (ModuleCRun.java:20)</span><br><span class="line">at org.playpi.study.ModuleARun.run (ModuleARun.java:26)</span><br><span class="line">at org.playpi.study.ModuleARun.main (ModuleARun.java:14)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208014735.png" alt="调试运行结果" title="调试运行结果"></p><p>看到 <code>NoSuchMethodError</code> 就知道出现了严重的问题，如果试图使用搜索功能搜索 <code>Strings</code> 这个类，可以发现有 2 个一模一样的类，但是他们对应的 <code>guava jar</code> 的版本号不一致。这时候有经验的工程师就可以立马判断，编译运行 <code>JVM</code> 加载的 <code>jar</code> 对于 <code>ModuleCRun.run ()</code> 方法来说是有问题的，只加载了特定版本的 <code>guava jar</code>，确保了 <code>ModuleBRun.run ()</code> 方法可以顺利执行【和手动排除 <code>module-c</code> 中的 <code>guava v26.0-jre</code> 一个效果】。</p><p>如果是编译打包后使用 <code>java</code> 命令再运行，可以发现同样的错误，如果此时尝试解压 <code>jar</code> 包，反编译源码，查看具体的类，可以看到编译打包后有些类是不存在的【多版本的 <code>jar</code> 只会保留一个，就会导致另一个 <code>jar</code> 中的类全部丢失，如果此时恰好有不兼容的类，那就出问题】。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208013756.png" alt="搜索 Strings 类" title="搜索 Strings 类"></p><p>那有人会想到，能不能手动排除 <code>module-a</code> 中的 <code>guava v19.0</code> 呢，我来试试，在 <code>module-a</code> 的 <code>pom.xml</code> 中对 <code>module-b</code> 添加 <code>exclude</code> 属性：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">    &lt;groupId&gt;org.playpi.study&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;iplaypistudy-shade-module-b&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;parent.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;!-- 这里排除会导致调用 ModuleBRun.run () 时出现 NoSuchMethodError --&gt;</span><br><span class="line">    &lt;exclusions&gt;</span><br><span class="line">        &lt;exclusion&gt;</span><br><span class="line">            &lt;groupId&gt;com.google.guava&lt;/groupId&gt;</span><br><span class="line">            &lt;artifactId&gt;guava&lt;/artifactId&gt;</span><br><span class="line">        &lt;/exclusion&gt;</span><br><span class="line">    &lt;/exclusions&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure><p>调试运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">2020-02-08_01:40:33 [main] INFO study.ModuleARun:13: ====Hello World!</span><br><span class="line">2020-02-08_01:40:33 [main] INFO study.ModuleARun:24: ==== 开始执行 module-a 的代码 </span><br><span class="line">2020-02-08_01:40:33 [main] INFO study.ModuleBRun:19: ==== 开始执行 module-b 的代码 </span><br><span class="line">Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: com.google.common.base.Objects.toStringHelper (Ljava/lang/Object;) Lcom/google/common/base/Objects$ToStringHelper;</span><br><span class="line">at org.playpi.study.ModuleBRun.run (ModuleBRun.java:20)</span><br><span class="line">at org.playpi.study.ModuleARun.run (ModuleARun.java:25)</span><br><span class="line">at org.playpi.study.ModuleARun.main (ModuleARun.java:14)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208014710.png" alt="调试运行结果" title="调试运行结果"></p><p>可见还是有同样的问题，运行到 <code>ModuleBRun ()</code> 方法时已经出错了，根源就在于多版本的 <code>guava</code> 之间无法兼容。</p><p>这里需要注意的是，在 <code>module-a</code> 中并不能随意调用 <code>module-c</code> 中 <code>guava v26.0-jre</code> 的方法，如果方法不存在的话编译不会通过【<code>maven</code> 先加载了低版本的 <code>guava v19.0</code>】。而单独看 <code>module-c</code> 的话，它是一个独立的子模块，所以 <code>module-c</code> 中的方法不受编译的限制，只有在把 <code>module-a</code> 打包后，真正运行时才会抛出异常。</p><p>具体可以参考 <code>ModuleARun</code> 中的 <code>runGuava ()</code> 方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">     * 依赖 b/c 时或者依赖 b/c-shade 时:</span><br><span class="line">     * 在这里无法像 module-c 那样直接调用 26.0-jre 里面的方法，编译无法通过 </span><br><span class="line">     * 但是 module-c 里面的代码是单独处于模块里面，编译时无法检测，所以 ModuleCRun.run () 可以通过编译 (编译阶段不会检测 run 里面的代码)</span><br><span class="line">     * &lt;p&gt;</span><br><span class="line">     * 所以:</span><br><span class="line">     * 制作 shade 只是可以保证 ModuleCRun.run () 正常执行，并不能保证 Strings.lenientFormat 可用 (连编译都无法通过)</span><br><span class="line">     */</span><br><span class="line">public static void runGuava () &#123;</span><br><span class="line">log.info (&quot;==== 开始执行 module-a 的 guava v19.0 代码 & quot;);</span><br><span class="line">Objects.ToStringHelper toStringHelper = Objects.toStringHelper (new Object ());</span><br><span class="line">toStringHelper.add (&quot;in&quot;, &quot;in&quot;);</span><br><span class="line">toStringHelper.add (&quot;out&quot;, &quot;out&quot;);</span><br><span class="line">log.info (&quot;====[&#123;&#125;]&quot;, toStringHelper.toString ());</span><br><span class="line">log.info (&quot;====module-a 的 guava v19.0 代码执行完成 & quot;);</span><br><span class="line">log.info (&quot;&quot;);</span><br><span class="line">log.info (&quot;==== 开始执行 module-a 的 guava v26.0-jre 代码 & quot;);</span><br><span class="line">//        log.info (&quot;====[&#123;&#125;]&quot;, Strings.lenientFormat (&quot;&quot;, &quot;in&quot;, &quot;out&quot;));</span><br><span class="line">log.info (&quot;====module-a 的 guava v26.0-jre 代码执行完成 & quot;);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="插件登场"><a href="# 插件登场" class="headerlink" title="插件登场"></a>插件登场 </h2><p> 看似疑无路，其实还有柳暗花明，使用 <code>maven-shade-plugin</code> 插件可以完美解决上述的问题。</p><p>在 <code>module-c</code> 的 <code>pom.xml</code> 配置文件中，给插件 <code>maven-shade-plugin</code> 添加 <code>relocation</code> 配置，把 <code>com.google.common</code> 包路径变为 <code>iplaypi.com.google.common</code>，要确保独一无二，总体内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 非常好用的 shade 插件 --&gt;</span><br><span class="line">&lt;plugin&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;maven-shade-plugin.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;executions&gt;</span><br><span class="line">        &lt;execution&gt;</span><br><span class="line">            &lt;!-- Maven 的生命周期 --&gt;</span><br><span class="line">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">                &lt;!-- 插件目标 --&gt;</span><br><span class="line">                &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;!-- 配置多版本 jar 包中类路径的重命名 --&gt;</span><br><span class="line">                &lt;relocations&gt;</span><br><span class="line">                    &lt;relocation&gt;</span><br><span class="line">                        &lt;pattern&gt;com.google.common&lt;/pattern&gt;</span><br><span class="line">                        &lt;shadedPattern&gt;iplaypi.com.google.common&lt;/shadedPattern&gt;</span><br><span class="line">                    &lt;/relocation&gt;</span><br><span class="line">                &lt;/relocations&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/execution&gt;</span><br><span class="line">    &lt;/executions&gt;</span><br><span class="line">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208185321.png" alt="给 C 模块添加 relocation" title="给 C 模块添加 relocation"></p><p>此外，在 <code>module-a</code> 中也需要配置常规的打包参数，使用 <code>mainClass</code> 指定主类，使用 <code>shadedClassifierName</code> 指定 <code>jar</code> 包后缀【不会用到 <code>relocation</code> 的功能】，内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;plugin&gt;</span><br><span class="line">    &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt;</span><br><span class="line">    &lt;artifactId&gt;maven-shade-plugin&lt;/artifactId&gt;</span><br><span class="line">    &lt;version&gt;$&#123;maven-shade-plugin.version&#125;&lt;/version&gt;</span><br><span class="line">    &lt;executions&gt;</span><br><span class="line">        &lt;execution&gt;</span><br><span class="line">            &lt;!-- Maven 的生命周期 --&gt;</span><br><span class="line">            &lt;phase&gt;package&lt;/phase&gt;</span><br><span class="line">            &lt;goals&gt;</span><br><span class="line">                &lt;!-- 插件目标 --&gt;</span><br><span class="line">                &lt;goal&gt;shade&lt;/goal&gt;</span><br><span class="line">            &lt;/goals&gt;</span><br><span class="line">            &lt;configuration&gt;</span><br><span class="line">                &lt;transformers&gt;</span><br><span class="line">                    &lt;!-- 使用资源转换器 ManifestResourceTransformer, 可执行的 jar 包 --&gt;</span><br><span class="line">                    &lt;transformer</span><br><span class="line">                                        implementation=&quot;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer&quot;&gt;</span><br><span class="line">                        </span><br><span class="line">                     &lt;!-- 指定主类入口 --&gt;                       &lt;mainClass&gt;org.playpi.study.ModuleARun&lt;/mainClass&gt;</span><br><span class="line">                    &lt;/transformer&gt;</span><br><span class="line">                &lt;/transformers&gt;</span><br><span class="line">                &lt;!-- 指定 jar 包后缀 --&gt;</span><br><span class="line">                &lt;shadedClassifierName&gt;jar-with-dependencies&lt;/shadedClassifierName&gt;</span><br><span class="line">            &lt;/configuration&gt;</span><br><span class="line">        &lt;/execution&gt;</span><br><span class="line">    &lt;/executions&gt;</span><br><span class="line">&lt;/plugin&gt;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208185900.png" alt="给 A 模块添加打包参数" title="给 A 模块添加打包参数"></p><p>接着就可以编译打包了：<code>mvn clean package</code>，打包完成后，在 <code>target</code> 目录下找到最终的 <code>jar</code> 包，使用 <code>java</code> 命令执行：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java -jar iplaypistudy-shade-module-a-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleARun:12: ====Hello World!</span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleARun:23: ==== 开始执行 module-a 的代码 </span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleBRun:19: ==== 开始执行 module-b 的代码 </span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleBRun:23: ====[Object&#123;in=in, out=out</span><br><span class="line">]</span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleBRun:24: ====module-b 的代码执行完成 </span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleCRun:18: ==== 开始执行 module-c 的代码 </span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleCRun:20: ====[[in, out]]</span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleCRun:21: ====module-c 的代码执行完成 </span><br><span class="line">2020-02-08_19:08:04 [main] INFO study.ModuleARun:26: ====module-a 的代码执行完成 </span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208190909.png" alt="运行成功" title="运行成功"></p><p>可以看到运行结果，所有的方法都调用成功，说明不存在多版本的 <code>jar</code> 包冲突的问题了。</p><p>注意，此时不能使用 <strong>调试运行的方法 </strong>，读者会发现使用 <code>IDEA</code> 等工具直接调试运行，仍旧会出错，这是因为 <code>IDEA</code> 调试运行只是经过了 <code>compile</code> 阶段，而 <code>maven-shade-plugin</code> 插件中的 <code>shade relocation</code> 根本没有执行。由于我们配置的 <code>phase</code> 是 <code>package</code>【绑定到 <code>Maven</code> 的 <code>package</code> 生命周期】，因此，必须经过打包后，直接指定 <code>main</code> 主类运行 <code>jar</code> 包，才会看到效果。</p><p>为了知其然也知其所以然，我们肯定要看看 <code>jar</code> 包到底发生了什么变化，找到 <code>jar</code> 包，使用 <code>Java Decompiler</code> 工具反编译字节码文件，查看 <code>.java</code> 文件有什么变化，我们首先能想到的就是类路径变化了。</p><p>找到 <code>Strings</code> 类文件，可以看到它的类路径变化了，已经变为了 <code>iplaypi.com.google.common.base</code>，同时它所 <code>import</code> 的类路径也添加了 <code>iplaypi</code> 前缀。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208192045.png" alt="反编译查看源码" title="反编译查看源码"></p><p>也就是说，打包完成之后，在 <code>jar</code> 包里面可以看到原本 <code>com.google.common</code> 下面的类全部被保留，<code>guava v19.0</code> 的类路径没有变化，而 <code>guava v26.0-jre</code> 的所有类路径都被添加了前缀 <code>iplaypi.</code>，而这正是 <code>shade</code> 的功劳。如此一来，高、低版本的所有类都分离开了，调用方可以任意使用，不会再有冲突或者缺失的情况。</p><p>那我们再看看调用方的 <code>import</code> 是怎样的，分别找到 <code>ModuleBRun</code>、<code>ModuleCRun</code> 类。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208192409.png" alt="反编译后的 ModuleBRun" title="反编译后的 ModuleBRun"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208192525.png" alt="反编译后的 ModuleCRun" title="反编译后的 ModuleCRun"></p><p>从 <code>ModuleCRun</code> 中可以看到，调用方的代码类的 <code>import</code> 类路径也被同步替换。当然，由于 <code>ModuleBRun</code> 并没有参与 <code>shade relocation</code> 流程，所以 <code>import</code> 还是原来的样子。</p><p><strong>总结来说 </strong>，其实 <code>maven-shade-plugin</code> 插件并没有什么难以理解的地方，它只是帮助我们在构建 <code>jar</code> 包时，把特定的类路径转换为了我们指定的新路径，同时把所有调用方的 <code>import</code> 语句也改变了，这样就能确保这些类在加载到 <code>JVM</code> 中是独一无二的，也就不会冲突了【当然会造成最后的 <code>uber jar</code> 变大了，加载到 <code>JVM</code> 中的类也变多了】。</p><p>它的效果概念图如下：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208195142.png" alt="效果概念图" title="效果概念图"></p><p>当然，只看到现象还不够，下面我们来探讨一下它的实现方法，读者请看下一小节：<strong> 实现分析 </strong>。</p><h2 id="实现分析"><a href="# 实现分析" class="headerlink" title="实现分析"></a>实现分析 </h2><p> 想要分析 <code>maven-shade-plugin</code> 插件是如何实现这个功能的，源代码少不了，下面简单分析一下，可以直接打断点调试一下源代码，跟着源代码跑一遍打包的流程即可。</p><p>首先，需要下载源代码，在 <code>GitHub</code> 上面下载：<a href="https://github.com/apache/maven-shade-plugin/tree/maven-shade-plugin-3.2.1" target="_blank" rel="noopener">maven-shade-plugin</a> ，注意下载后切换到指定版本的，例如我使用的版本是 <code>v3.2.1</code>，则 <code>git clone</code> 后需要 <code>git checkout</code> 到指定的 <code>tag</code>【例如：<code>maven-shade-plugin-3.2.1</code>】。</p><p>源码下载成功后，它其实也是一个 <code>Maven</code> 项目【如果导入时 <code>IDEA</code> 识别不了，可以先 <code>Open</code> 看一下，需要一些初始化动作】，可以直接以 <code>Module</code> 的形式导入 <code>IDEA</code> 中，然后就可以直接被我们自己的项目依赖。</p><p>在 <code>IDEA</code> 中依次选择 <code>File</code>、<code>New</code>、<code>Module from existing Sources</code>【也可以在 <code>Project Structure</code> 中直接添加】，最终选择已经下载的项目源码，导入过程中还需要选择一些配置，例如项目为 <code>Maven</code> 类型、项目名称，直接使用默认值即可。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205501.png" alt="添加模块" title="添加模块"></p><p>由于有部分 <code>jar</code> 包需要从远程仓库拉取，如果网络不好的话【或者没配置国内的仓库、镜像】，速度有点慢，需要耐心等待。</p><p>添加成功后，需要确保 <code>maven-shade-plugin</code> 模块正常，通过 <code>File</code>、<code>Project Structure</code>、<code>Module</code> 查看。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205616.png" alt="检查模块" title="检查模块"></p><p>此时，我们 <code>module-a</code> 的 <code>pom.xml</code> 文件中配置的 <code>maven-shade-plugin</code> 插件，实际使用的就不是本地仓库的了，而是我们导入的 <code>Module</code>，这样就可以调试代码了。</p><p>找到 <code>maven-shade-plugin</code> 插件的入口，<code>Maven</code> 规定一般是 <code>@Mojo</code> 注解类的 <code>execute ()</code> 方法，我在这里找到类：<code>org.apache.maven.plugins.shade.mojo.ShadeMojo</code>，<code>execute ()</code> 方法在代码 381 行，在这个方法入口处 385 行：<code>setupHintedShader ();</code>，打上断点。</p><p>具体的生成 <code>jar</code> 包以及 <code>shade relocation</code> 功能的实现逻辑在 <code>org.apache.maven.plugins.shade.DefaultShader</code> 中，我们在 160 行的 <code>shadeJars ()</code> 方法中打上断点。</p><p>接着准备调试的步骤，可以增加一个 <code>Run/Debug Configuration</code>，把 <code>mvn clean package</code> 配置成为一个 <code>Application</code>，最后点击 <code>debug</code> 按钮就可以调试了。也可以直接选中项目右键，依次选择 <code>Debug Maven</code>、<code>debug: package</code>，直接进行调试，我使用的就是这种方式，如下图：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205712.png" alt="开始调试" title="开始调试"></p><p>首先进入到第一个断点：<code>execute ()</code> 方法，说明调试程序执行正常，直接进入到下一个断点：<code>shadeJars ()</code> 方法【注意，我这里截图执行的是 <code>module-c</code> 打包的流程，列出的 <code>jar</code> 包仅和 <code>module-c</code> 有关】：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205731.png" alt="execute 方法" title="execute 方法"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205746.png" alt="shadeJars 方法" title="shadeJars 方法"></p><p>可以从 <code>shadeRequest</code> 对象中看到 <code>jar</code> 包列表，以及 <code>relocators</code> 列表，<code>shade relocation</code> 的代码逻辑在 <code>org.apache.maven.plugins.shade.relocation.SimpleRelocator</code> 里面，里面有替换类路径、文件路径的操作实现。</p><p>接着进入到 <code>shadeSingleJar ()</code> 方法，可以看到对每一个文件进行处理，替换、合并等操作。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209205801.png" alt="shadeSingleJar 方法" title="shadeSingleJar 方法"></p><p>最后也可以测试一下，如果不对 <code>module-c</code> 做 <code>shade relocation</code>，最终项目打包收集的所有 <code>jar</code> 包中，是没有 <code>guava v26.0-jre</code> 的，只有 <code>guava v19.0</code>，这也可以解释为什么运行时会缺失。</p><h2 id="另一种情况"><a href="# 另一种情况" class="headerlink" title="另一种情况"></a>另一种情况 </h2><p> 假设 <code>module-c</code> 不是我们自己维护的模块，我们无权限变更，更不可能直接去更改它的 <code>pom.xml</code> 文件，此时应该怎么办。可以把 <code>module-c</code> 类比成一个独立的 <code>jar</code> 包，拥有自己的坐标，由开源组织发布【例如 <code>hive-client</code>、<code>hbase-client</code>】，被 <code>module-a</code> 依赖引用，此时我们不可能去更改它的配置文件或者代码。</p><p>也有办法，那就是为这类 <code>jar</code> 包单独创建一个独立的 <code>module</code>，在这个 <code>module</code> 中完成 <code>shade</code> 操作，然后才把这个 <code>module</code> 给我们的项目引用。</p><p>在本例中，就以 <code>module-c</code> 为例，假如我们没有权限更改 <code>module-c</code> 中的代码、配置文件，只能新创建一个 <code>module-c-shade</code>，它里面什么代码都没有，只是简单地依赖 <code>module-c</code>，然后在配置文件 <code>pom.xml</code> 中做一个 <code>shade relocation</code>，把可能冲突的类解决掉。</p><p>项目结构如下图：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200208200838.png" alt="复杂情况的传递依赖" title="复杂情况的传递依赖"></p><p>和上面的效果一致，编译打包后，依旧可以成功运行。</p><p>可以多思考一下，根据上面的情况，还有在什么场景下需要单独创建一个 <code>module</code>，里面没有任何代码，只是为了做影子依赖呢？</p><p>最先想到的肯定是类似上面那种，传递依赖导致的冲突，例如项目中依赖了 <code>es-hadoop</code>，而由此带来的 <code>guava</code>、<code>http</code> 等 <code>jar</code> 包冲突，我们不可能想着去改 <code>es-hadoop</code> 的 <code>pom.xml</code> 文件，因为我们不应当变更源码【太麻烦而且不利于管理】，当然也不一定能拿到源码。那么，只能单独创建一个 <code>module</code>，使用 <code>maven-shade-plugin</code> 插件做 <code>shade relocation</code>。</p><p>另外还有一种情况，如果传递依赖过多，例如 <code>es-hadoop</code> 中的 <code>guava</code>，<code>hbase</code> 中的 <code>commons-lang</code>，也没有必要为每一个 <code>jar</code> 包都单独创建一个 <code>module</code>，显得繁琐而且没必要。此时可以只创建一个 <code>module</code>，用来解决所有的依赖冲突，但是如果这些 <code>jar</code> 包之间的传递依赖本来就冲突，那还是得为每一个 <code>jar</code> 包都创建一个 <code>module</code>【此时这种 <code>Maven</code> 项目冲突过多，是不健康的，还是升级适配为好】。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p>1、新建 <code>module</code> 时如果卡住，可以设置参数 <code>archetypeCatalog=internal</code> 解决。</p><p>2、还要注意一点，低版本的 <code>maven-shade-plugin</code> 插件并不支持 <code>relocation</code> 参数来制作影子，编译时会报错，例如 <code>v2.4.3</code> 就不行，需要 <code>v3.0</code> 以上，例如：<code>v3.1.0</code>、<code>v3.2.1</code>。</p><p>3、引入新依赖后，要确保传递依赖不能污染了当前项目的依赖，而制作 <code>shade</code> 的目的在于这个新依赖不会有异常。</p><p> 当前项目中或者当前项目的依赖中，会有一些调用，如果被传递依赖污染，会导致异常。如果是当前项目的代码显式调用，编译不会通过，但是如果是在依赖 <code>jar</code> 中调用，编译阶段是检测不出来的，只会在运行调用时抛出异常。</p><p>使用上面的例子来说，如果在 <code>module-a</code> 中与 <code>module-b</code> 中的依赖有相同的，则在 <code>module-a</code> 中代码引用使用时【不是 <code>module-a</code> 中我们写的代码，而是 <code>module-a</code> 中 <code>jar</code> 的源代码】，确保使用的是 <code>module-a</code> 中的版本对应的类或者方法【即把 <code>module-b</code> 中的依赖给排除掉】，否则编译会通过，但是打包后还是会缺失。</p><p>因为 <code>jar</code> 包中的源代码在编译阶段不会被检测调用的是哪个依赖里面的类或者方法【编译时只会检测我们写的代码】，必须是打包运行后才明确【其实运行前就会把所有 <code>jar</code> 包的类加载到 <code>JVM</code> 中，由于冲突会丢弃一些】，但是运行前的加载 <code>JVM</code> 过程对于多版本的依赖无法确定具体是哪个依赖生效，编译完成后到运行的时候【执行到 <code>jar</code> 中相应的代码】，就会出问题。注意这里虽然在 <code>module-b</code> 中对部分依赖做了 <code>shade</code>，但是只是对 <code>module-b</code> 生效，而对 <code>module-a</code> 是无效的，所以可能会导致 <code>module-a</code> 中的 <code>jar</code> 中源代码引用时找不到类或者方法，于是编译打包正常，运行时就会出现 <code>NoClassDefFoundError</code> 异常。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;最近因为协助升级相关业务的 &lt;code&gt;sdk&lt;/code&gt;，遇到过多次 &lt;code&gt;jar&lt;/code&gt; 包冲突的问题，此外自己在业务中升级算法接口的 &lt;code&gt;sdk&lt;/code&gt; 时，也遇到过 &lt;code&gt;jar&lt;/code&gt; 冲突问题。而且，这种冲突是灾难性的，不要指望通过排除特定包、升级版本、降级版本解决，根本无济于事，还会越来越混乱。&lt;/p&gt;&lt;p&gt;那么，最高效的方法是使用 &lt;code&gt;maven-shade-plugin&lt;/code&gt; 插件，只要加上冲突相关的 &lt;code&gt;relocation&lt;/code&gt; 配置，变更包名，即可迅速化解冲突的问题。&lt;/p&gt;
    
    </summary>
    
      <category term="踩坑系列" scheme="https://www.playpi.org/categories/series-of-fixbug/"/>
    
    
      <category term="Maven" scheme="https://www.playpi.org/tags/Maven/"/>
    
      <category term="Java" scheme="https://www.playpi.org/tags/Java/"/>
    
      <category term="shade" scheme="https://www.playpi.org/tags/shade/"/>
    
  </entry>
  
  <entry>
    <title>Spark 项目依赖冲突问题总结</title>
    <link href="https://www.playpi.org/2019112901.html"/>
    <id>https://www.playpi.org/2019112901.html</id>
    <published>2019-11-29T12:05:46.000Z</published>
    <updated>2019-11-29T12:05:46.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>今天遇到一个常见的依赖冲突问题，在一个 <code>Spark</code> 项目中，引用了多个其它项目的公共包【例如公共 <code>elt</code> 模块、算法模块】，在提交运行 <code>Spark</code> 任务时，由于依赖冲突而失败，高低版本无法兼容。</p><p>本文记录问题解决过程以及经验总结，重要开发环境说明：<code>Spark v1.6</code>、<code>es-hadoop v5.6.8</code>、<code>kafka v0.9.x</code> 。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 在一个 <code>SparkStreaming</code> 项目中，由于业务需要而新增加了算法模块的依赖【公司开放的公共 <code>jar</code> 包】，结果无法正常运行，根本原因在于依赖包冲突，版本无法完全匹配。</p><p>下面简单描述一下各种现象，这当然是为了给读者参考才这么做的，在实际开发过程中如果也这么尝试是很浪费时间的【当然对于初学者还是很有必要的，实际踩坑才知道痛苦】。</p><p>在一开始，添加算法模块的依赖后，使用本地 <code>local</code> 模式试运行程序正常，相关算法接口可用，但是当提交任务到 <code>Spark</code> 集群后【<code>standalone</code> 模式】，提交任务失败，出现 <code>kryo</code> 序列化异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">2019-11-26_18:00:32 [task-result-getter-0] WARN scheduler.TaskSetManager:70: Lost task 0.0 in stage 0.0 (TID 0, dev4): java.io.EOFException</span><br><span class="line">at org.apache.spark.serializer.KryoDeserializationStream.readObject (KryoSerializer.scala:232)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject (TorrentBroadcast.scala:217)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply (TorrentBroadcast.scala:178)</span><br><span class="line">at org.apache.spark.util.Utils$.tryOrIOException (Utils.scala:1205)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock (TorrentBroadcast.scala:165)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast._value$lzycompute (TorrentBroadcast.scala:64)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast._value (TorrentBroadcast.scala:64)</span><br><span class="line">at org.apache.spark.broadcast.TorrentBroadcast.getValue (TorrentBroadcast.scala:88)</span><br><span class="line">at org.apache.spark.broadcast.Broadcast.value (Broadcast.scala:70)</span><br><span class="line">at org.apache.spark.scheduler.ResultTask.runTask (ResultTask.scala:62)</span><br><span class="line">at org.apache.spark.scheduler.Task.run (Task.scala:89)</span><br><span class="line">at org.apache.spark.executor.Executor$TaskRunner.run (Executor.scala:227)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2018/20191201000221.png" alt="启动 Spark 任务失败" title="启动 Spark 任务失败"></p><p>经过简单排查，上述错误的原因在于 <code>Spark</code> 需要依赖 <code>kryo v2.21</code>，而算法模块里面依赖了 <code>kryo v4.0.1</code>，在多版本同时存在的情况下，<code>Java</code> 类加载器加载到了高版本的 <code>kryo</code>【当然先加载到哪个类不确定，但是由前面的现象可以判定先加载了高版本的 <code>jar</code> 包】，导致 <code>Spark</code> 不兼容。</p><p>进一步想到可以将算法模块中的高版本 <code>kryo</code> 排除【当然此时没有考虑这样做对算法接口的影响】，我还就这么做了，又试了一次，结果出现以下异常【敏感包名使用 <code>xxx.yyy</code> 替换】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">java.lang.reflect.InvocationTargetException</span><br><span class="line">at org.apache.dubbo.common.bytecode.Wrapper0.invokeMethod (Wrapper0.java)</span><br><span class="line">at com.xxx.yyy.consumer.proxy.JavassistProxyFactory$1.doInvoke (JavassistProxyFactory.java:28)</span><br><span class="line">at com.xxx.yyy.consumer.proxy.AbstractProxyInvoker.doInvoke (AbstractProxyInvoker.java:57)</span><br><span class="line">at com.xxx.yyy.consumer.metric.ThanosConsumerMetric.makeMetric (ThanosConsumerMetric.java:90)</span><br><span class="line">at com.xxx.yyy.consumer.proxy.AbstractProxyInvoker.invoke (AbstractProxyInvoker.java:53)</span><br><span class="line">at com.xxx.yyy.consumer.proxy.InvokerInvocationHandler.invoke (InvokerInvocationHandler.java:36)</span><br><span class="line">at org.apache.dubbo.common.bytecode.proxy1.classify (proxy1.java)</span><br><span class="line">at com.xxx.zzz.analyz.rpc.ThanosRpcAlgorithmAnalyzer.classify (ThanosRpcAlgorithmAnalyzer.java:50)</span><br><span class="line">at com.xxx.zzz.analyz.rpc.ThanosRpcAlgorithmAnalyzer.classify (ThanosRpcAlgorithmAnalyzer.java:55)</span><br><span class="line">at com.xxx.zzz.analyz.rpc.ThanosRpcAlgorithmAnalyzer.main (ThanosRpcAlgorithmAnalyzer.java:70)</span><br><span class="line">Caused by: org.apache.dubbo.rpc.RpcException: Failed to invoke the method classify in the service com.xxx.yyy.service.Classifier. Tried 3 times of the providers [172.18.5.66:31142, 172.18.5.145:31142] (2/2) from the registry dev3:2181 on the consumer 172.18.7.203 using the dubbo version 2.7.3. Last error is: Failed to invoke remote method: classify, provider: dubbo://172.18.5.145:31142/com.xxx.yyy.service.Classifier?application=xxx-rpc-consumer&amp;check=false&amp;cluster=backpressure&amp;deprecated=false&amp;dubbo=2.0.2&amp;interface=com.xxx.yyy.service.Classifier&amp;lazy=false&amp;loadbalance=leastactive&amp;pid=10388&amp;qos.enable=false&amp;reference.filter=requestid,activelimit&amp;register.ip=172.18.7.203&amp;release=2.7.3&amp;remote.application=xxx-rpc-provider&amp;retries=2&amp;revision=0.1-20191122.093730-4&amp;serialization=kryo&amp;side=consumer&amp;sticky=false&amp;timeout=2147483647&amp;timestamp=1574327320883&amp;weight=16, cause: org.apache.dubbo.remoting.RemotingException: io.netty.handler.codec.EncoderException: java.lang.NoClassDefFoundError: com/esotericsoftware/kryo/pool/KryoFactory</span><br><span class="line">io.netty.handler.codec.EncoderException: java.lang.NoClassDefFoundError: com/esotericsoftware/kryo/pool/KryoFactory</span><br><span class="line">at io.netty.handler.codec.MessageToByteEncoder.write (MessageToByteEncoder.java:125)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:658)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:716)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:651)</span><br><span class="line">at io.netty.handler.timeout.IdleStateHandler.write (IdleStateHandler.java:266)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:658)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:716)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:651)</span><br><span class="line">at io.netty.channel.ChannelDuplexHandler.write (ChannelDuplexHandler.java:106)</span><br><span class="line">at org.apache.dubbo.remoting.transport.netty4.NettyClientHandler.write (NettyClientHandler.java:87)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:658)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.access$2000 (AbstractChannelHandlerContext.java:32)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write (AbstractChannelHandlerContext.java:939)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write (AbstractChannelHandlerContext.java:991)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run (AbstractChannelHandlerContext.java:924)</span><br><span class="line">at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks (SingleThreadEventExecutor.java:380)</span><br><span class="line">at io.netty.channel.nio.NioEventLoop.run (NioEventLoop.java:357)</span><br><span class="line">at io.netty.util.concurrent.SingleThreadEventExecutor$2.run (SingleThreadEventExecutor.java:116)</span><br><span class="line">at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run (DefaultThreadFactory.java:137)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">Caused by: java.lang.NoClassDefFoundError: com/esotericsoftware/kryo/pool/KryoFactory</span><br><span class="line">at java.lang.ClassLoader.defineClass1 (Native Method)</span><br><span class="line">at java.lang.ClassLoader.defineClass (ClassLoader.java:763)</span><br><span class="line">at java.security.SecureClassLoader.defineClass (SecureClassLoader.java:142)</span><br><span class="line">at java.net.URLClassLoader.defineClass (URLClassLoader.java:467)</span><br><span class="line">at java.net.URLClassLoader.access$100 (URLClassLoader.java:73)</span><br><span class="line">at java.net.URLClassLoader$1.run (URLClassLoader.java:368)</span><br><span class="line">at java.net.URLClassLoader$1.run (URLClassLoader.java:362)</span><br><span class="line">at java.security.AccessController.doPrivileged (Native Method)</span><br><span class="line">at java.net.URLClassLoader.findClass (URLClassLoader.java:361)</span><br><span class="line">at java.lang.ClassLoader.loadClass (ClassLoader.java:424)</span><br><span class="line">at sun.misc.Launcher$AppClassLoader.loadClass (Launcher.java:349)</span><br><span class="line">at java.lang.ClassLoader.loadClass (ClassLoader.java:357)</span><br><span class="line">at org.apache.dubbo.common.serialize.kryo.KryoObjectOutput.&lt;init&gt;(KryoObjectOutput.java:39)</span><br><span class="line">at org.apache.dubbo.common.serialize.kryo.KryoSerialization.serialize (KryoSerialization.java:51)</span><br><span class="line">at org.apache.dubbo.remoting.exchange.codec.ExchangeCodec.encodeRequest (ExchangeCodec.java:234)</span><br><span class="line">at org.apache.dubbo.remoting.exchange.codec.ExchangeCodec.encode (ExchangeCodec.java:69)</span><br><span class="line">at org.apache.dubbo.rpc.protocol.dubbo.DubboCountCodec.encode (DubboCountCodec.java:40)</span><br><span class="line">at org.apache.dubbo.remoting.transport.netty4.NettyCodecAdapter$InternalEncoder.encode (NettyCodecAdapter.java:70)</span><br><span class="line">at io.netty.handler.codec.MessageToByteEncoder.write (MessageToByteEncoder.java:107)</span><br><span class="line">... 19 more</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2018/20191201001332.png" alt="算法接口抛异常" title="算法接口抛异常"></p><p>这里可以明确得出的是，由于擅自排除了算法模块需要的高版本 <code>kryo</code>，现在算法接口无法提供服务了，缺失 <code>KryoFactory</code> 类。</p><p>没办法，只好对算法模块中的 <code>kryo</code> 做了影子复制，把包名 <code>com.esotericsoftware.kryo</code> 变更了一下，这样既不会影响到算法接口的使用，也不会影响到 <code>Spark</code> 任务提交。</p><p>好，<code>kryo</code> 的冲突问题解决了，但是紧接着又出现了 <code>netty</code> 冲突问题，现象类似，异常信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">2019-11-29_18:23:32 [appclient-register-master-threadpool-0] INFO client.AppClient$ClientEndpoint:58: Connecting to master spark://dev4:7077...</span><br><span class="line">2019-11-29_18:23:32 [shuffle-client-0] ERROR client.TransportClient:235: Failed to send RPC 8750922883607188033 to dev4/172.18.5.204:7077: java.lang.AbstractMethodError: org.apache.spark.network.protocol.MessageWithHeader.touch (Ljava/l</span><br><span class="line">ang/Object;) Lio/netty/util/ReferenceCounted;</span><br><span class="line">java.lang.AbstractMethodError: org.apache.spark.network.protocol.MessageWithHeader.touch (Ljava/lang/Object;) Lio/netty/util/ReferenceCounted;</span><br><span class="line">at io.netty.util.ReferenceCountUtil.touch (ReferenceCountUtil.java:77)</span><br><span class="line">at io.netty.channel.DefaultChannelPipeline.touch (DefaultChannelPipeline.java:116)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:785)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:701)</span><br><span class="line">at io.netty.handler.codec.MessageToMessageEncoder.write (MessageToMessageEncoder.java:112)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0 (AbstractChannelHandlerContext.java:716)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:708)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:791)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:701)</span><br><span class="line">at io.netty.handler.timeout.IdleStateHandler.write (IdleStateHandler.java:303)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0 (AbstractChannelHandlerContext.java:716)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:708)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.access$1700 (AbstractChannelHandlerContext.java:56)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write (AbstractChannelHandlerContext.java:1102)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$WriteAndFlushTask.write (AbstractChannelHandlerContext.java:1149)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.run (AbstractChannelHandlerContext.java:1073)</span><br><span class="line">at io.netty.util.concurrent.AbstractEventExecutor.safeExecute (AbstractEventExecutor.java:163)</span><br><span class="line">at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks (SingleThreadEventExecutor.java:510)</span><br><span class="line">at io.netty.channel.nio.NioEventLoop.run (NioEventLoop.java:518)</span><br><span class="line">at io.netty.util.concurrent.SingleThreadEventExecutor$6.run (SingleThreadEventExecutor.java:1044)</span><br><span class="line">at io.netty.util.internal.ThreadExecutorMap$2.run (ThreadExecutorMap.java:74)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">2019-11-29_18:23:32 [appclient-register-master-threadpool-0] WARN client.AppClient$ClientEndpoint:91: Failed to connect to master dev4:7077</span><br><span class="line">java.io.IOException: Failed to send RPC 8750922883607188033 to dev4/172.18.5.204:7077: java.lang.AbstractMethodError: org.apache.spark.network.protocol.MessageWithHeader.touch (Ljava/lang/Object;) Lio/netty/util/ReferenceCounted;</span><br><span class="line">at org.apache.spark.network.client.TransportClient$3.operationComplete (TransportClient.java:239)</span><br><span class="line">at org.apache.spark.network.client.TransportClient$3.operationComplete (TransportClient.java:226)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.notifyListener0 (DefaultPromise.java:577)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.notifyListenersNow (DefaultPromise.java:551)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.notifyListeners (DefaultPromise.java:490)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.setValue0 (DefaultPromise.java:615)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.setFailure0 (DefaultPromise.java:608)</span><br><span class="line">at io.netty.util.concurrent.DefaultPromise.tryFailure (DefaultPromise.java:117)</span><br><span class="line">at io.netty.util.internal.PromiseNotificationUtil.tryFailure (PromiseNotificationUtil.java:64)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.notifyOutboundHandlerException (AbstractChannelHandlerContext.java:818)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0 (AbstractChannelHandlerContext.java:718)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:708)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:791)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.write (AbstractChannelHandlerContext.java:701)</span><br><span class="line">at io.netty.handler.timeout.IdleStateHandler.write (IdleStateHandler.java:303)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite0 (AbstractChannelHandlerContext.java:716)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.invokeWrite (AbstractChannelHandlerContext.java:708)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext.access$1700 (AbstractChannelHandlerContext.java:56)</span><br><span class="line">at io.netty.channel.AbstractChannelHandlerContext$AbstractWriteTask.write (AbstractChannelHandlerContext.java:1102)</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2018/20191201001713.png" alt="netty 异常" title="netty 异常"></p><p>这个问题我见过很多次，通过简单排查发现 <code>Spark</code> 需要的是 <code>netty-all v4.0.29</code>，而算法模块需要的是 <code>v4.1.25</code>，我在本地看到实际加载的是 <code>v4.0.29</code>，这里 <code>Spark</code> 任务为什么提交失败我有疑惑【我只能怀疑服务器加载类的顺序和我本机的不一致，导致服务器上面实际加载的并不是 <code>Spark</code> 需要的版本】。</p><p>接着按照我的怀疑把高版本 <code>netty-all</code> 排除了，恢复正常【这里不需要复制影子，因为版本差别不大，算法模块可以兼容低版本 <code>netty-all</code> 依赖】。</p><p>但是，接着又出现 <code>org.apache.curator:curator-recipes</code> 依赖的问题，这是 <code>Spark</code> 任务读取 <code>kafka</code> 需要的依赖，而在算法模块中也需要。</p><p>异常信息如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Exception in thread &quot;main&quot; java.lang.NoSuchMethodError: org.apache.curator.framework.api.CreateBuilder.creatingParentsIfNeeded () Lorg/apache/curator/framework/api/ProtectACLCreateModePathAndBytesable;</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2018/20191201001855.png" alt="curator 异常" title="curator 异常"></p><p>其中，在 <code>Spark</code> 中需要的版本是 <code>v2.4.0</code>，而在算法模块中需要的是 <code>v4.0.1</code>，我看到实际加载的是 <code>v4.0.1</code>，所以 <code>Spark</code> 任务又失败了。</p><p>再按照这个节奏进行下去，读者是不是要疯掉了！好，我们到此为止，准备使用万能优雅的 <code>maven-shade-plugin</code> 插件解决这类让人抓狂的问题【只需要找到冲突的 <code>jar</code> 包替换包名，不需要排除】。</p><h1 id="问题分析解决"><a href="# 问题分析解决" class="headerlink" title="问题分析解决"></a>问题分析解决 </h1><h2 id="简单分析解决"><a href="# 简单分析解决" class="headerlink" title="简单分析解决"></a> 简单分析解决 </h2><p> 在上面的流程中，我会想到变更 <code>jar</code> 包依赖版本，或者移除多余的依赖，尝试让合适的版本出现，从而兼容代码中所有的调用。但是，遇到稍微复杂的情况这种做法显然是徒劳的。</p><p>诚然，这种方式针对单线程或者本地 <code>local</code> 模式运行的程序是可以生效的，但是对于集群模式的【<code>standalone</code>、<code>yarn</code> 等】<code>Spark</code> 任务，就无能为力了，很难恰好找到匹配的版本，毕竟公共包本身使用的依赖不是你能控制的，也不会为了你而做兼容【公共包面向大众发布，一般都会使用最新版本的依赖】。</p><p>接着详细来解释一下我这个典型场景，<code>Spark</code> 使用了一个低版本的 <code>kryo</code>，而算法模块使用了另外高版本的 <code>kyro</code>，但是诡异的是它们的依赖坐标不一致【算法模块是 <code>com.esotericsoftware:kryo</code>、<code>Spark</code> 是 <code>com.esotericsoftware.kryo:kryo</code>】，而实际类的包名却是一致的【都是 <code>com.esotericsoftware.kryo</code>】，这就导致类冲突无法兼容【在人们的经验中，<code>jar</code> 包坐标不同，类的包名也应该不同才对】。当然，<code>kryo</code> 高低版本之间的类不同也是无法兼容的原因之一。</p><p>如果选择移除算法模块的 <code>kryo</code>，调用算法接口时会报找不到类异常，如果移除 <code>Spark</code> 的 <code>kryo</code>，提交 <code>Spark</code> 任务时会报无法反序列化异常。</p><p>而且，比较让人崩溃的是，真的无法找到兼容两者的版本，那就只能利用 <code>maven-shade-plugin</code> 插件了。</p><p>我这里的项目本身使用的 <code>maven-shade-plugin</code> 插件是为了把所有的依赖都打包在一起，形成 <code>uber jar</code>，需要启动 <code>Spark</code> 任务时一起提交到集群。这样做主要是因为 <code>Spark</code> 集群的 <code>libs</code> 中没有存放任何公共依赖包，比较纯净，所以需要提交任务的客户端自己打包携带，这样也可以避免很多业务方共同使用同一个 <code>Spark</code> 集群产生依赖冲突问题。</p><p>无奈，最终只好决定使用 <code>maven-shade-plugin</code> 插件的高级功能：影子别名，直接变更类名，就不怕再冲突了。</p><p>使用 <code>maven-shade-plugin</code> 插件制作影子的相关类配置【把类的包名替换掉，避免冲突，根据项目的实际冲突情况而配置，这里仅供参考】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;relocations&gt;</span><br><span class="line">        &lt;relocation&gt;</span><br><span class="line">            &lt;pattern&gt;com.google&lt;/pattern&gt;</span><br><span class="line">            &lt;shadedPattern&gt;iplaypi.com.google&lt;/shadedPattern&gt;</span><br><span class="line">        &lt;/relocation&gt;</span><br><span class="line">        &lt;relocation&gt;</span><br><span class="line">            &lt;pattern&gt;io.netty&lt;/pattern&gt;</span><br><span class="line">            &lt;shadedPattern&gt;iplaypi.io.netty&lt;/shadedPattern&gt;</span><br><span class="line">        &lt;/relocation&gt;</span><br><span class="line">        &lt;relocation&gt;</span><br><span class="line">            &lt;pattern&gt;org.apache.curator&lt;/pattern&gt;</span><br><span class="line">            &lt;shadedPattern&gt;iplaypi.org.apache.curator&lt;/shadedPattern&gt;</span><br><span class="line">        &lt;/relocation&gt;</span><br><span class="line">        &lt;relocation&gt;</span><br><span class="line">            &lt;pattern&gt;com.esotericsoftware&lt;/pattern&gt;</span><br><span class="line">            &lt;shadedPattern&gt;iplaypi.com.esotericsoftware&lt;/shadedPattern&gt;</span><br><span class="line">        &lt;/relocation&gt;</span><br><span class="line">        &lt;relocation&gt;</span><br><span class="line">            &lt;pattern&gt;de.javakaffee&lt;/pattern&gt;</span><br><span class="line">            &lt;shadedPattern&gt;iplaypi.de.javakaffee&lt;/shadedPattern&gt;</span><br><span class="line">        &lt;/relocation&gt;</span><br><span class="line">    &lt;/relocations&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure><p>我这里把 <code>guava</code>、<code>netty</code>、<code>curator</code>、<code>kryo</code> 全部制作影子了，仅供参考。</p><h2 id="抽象简化问题"><a href="# 抽象简化问题" class="headerlink" title="抽象简化问题"></a>抽象简化问题 </h2><p> 下面就用模型简化一下我遇到的这类场景，使用 <code>guava</code> 包冲突做示例。</p><p><code>Maven</code> 项目中有 <code>a</code>、<code>b</code>、<code>c</code> 三个模块【分散为三个模块读者更容易理解，解决问题思路也更清晰】，<code>a</code> 同时依赖了 <code>b</code>、<code>c</code>。其中，<code>b</code> 依赖了低版本 <code>guava</code> 并调用了一个低版本独有的方法，<code>c</code> 依赖了高版本 <code>guava</code> 并调用了高版本独有的方法【当然引用特有的类也行】。</p><p>它们之间的关系如下图：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209000340.png" alt="项目依赖关系" title="项目依赖关系"></p><p>在这个 <code>Maven</code> 项目中，发生 <code>jar</code> 包冲突很明显是因为，项目中依赖了同一个 <code>jar</code> 包的多个版本，而且分别调用了高低版本特有的方法，或者引用了高低版本特有的类。面对此类问题，一般的解决思路是只保留一个版本，排除掉不需要的版本，但是上面这种情况太特殊了，排除 <code>jar</code> 包不能解决问题。</p><p>可以试想一下，排除掉低版本 <code>guava</code> 的话 <code>b</code> 会报错，排掉高版本 <code>guava</code> 的话 <code>c</code> 会报错，所以希望在项目中同时使用低版本 <code>guava</code> 和高版本 <code>guava</code>。</p><p>那就只能使用 <code>maven-shade-plugin</code> 插件来构建影子 <code>jar</code> 包，替换类路径，制作影子的效果如下图【思路就是构建 <code>c</code> 时替换掉 <code>guava</code> 的包名】：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209000438.png" alt="shade 插件替换类路径" title="shade 插件替换类路径"></p><p>这样就可以非常优雅地解决问题，但是会导致打包的 <code>jar</code> 比以前大一点。如果 <code>Maven</code> 项目本身没有那么多模块，只有一个大模块，建议拆分，至少把有冲突的部分单独拆出来构建影子模块。</p><p>这个方案的详细说明以及代码演示读者可以参考我的另外一篇博文：<a href="https://www.playpi.org/2019120101.html">解决 jar 包冲突的神器：maven-shade-plugin</a> 。</p><h1 id="问题总结"><a href="# 问题总结" class="headerlink" title="问题总结"></a>问题总结 </h1><p> 在制作影子时，<code>Maven</code> 的子模块是必不可少的帮手，否则还需要下载源码自己重新打包，麻烦而且做法不合适。</p><p><code>Java</code> 项目拆分为子模块的好处之一，遇到依赖冲突时，可以很方便地使用 <code>maven-shade-plugin</code> 插件，分分钟就可以制作影子。例如上面的抽象简化例子，如果 <code>a</code>、<code>b</code>、<code>c</code> 没有拆分，一直是一个模块，遇到这种依赖冲突就没办法解决，怎么排除都是不行的，只能单独构建一个子模块用来制作影子。</p><p>当然，如果上面的 <code>c</code> 本身就依赖了很多 <code>jar</code> 包，它们之间在 <code>c</code> 模块中就有冲突，也不好制作影子，还是单独新建一个纯净的子模块比较好【例如把类似 <code>guava</code> 冲突的 <code>jar</code> 包以及代码抽出来，单独创建 <code>c-sub-shade</code> 模块，在里面制作影子，这个模块给 <code>c</code> 引用】。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20200209000650.png" alt="shade 子模块" title="shade 子模块"></p><p>注意，上图和我本文中遇到的例子有一点不同，我的 <code>jar</code> 包在 <code>c</code> 模块中并没有冲突，所以可以直接利用 <code>c</code> 制作影子模块 <code>c-shade</code>，当然不怕麻烦也可以制作 <code>c-sub-shade</code>。</p><p>此外，我在一年前也遇到过一种简单的场景：<a href="https://www.playpi.org/2018100801.html">Spark Kryo 异常</a>，当时直接通过排除依赖就解决问题了，但是这次的场景太复杂，只能启用 <code>maven-shade-plugin</code> 插件了。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;今天遇到一个常见的依赖冲突问题，在一个 &lt;code&gt;Spark&lt;/code&gt; 项目中，引用了多个其它项目的公共包【例如公共 &lt;code&gt;elt&lt;/code&gt; 模块、算法模块】，在提交运行 &lt;code&gt;Spark&lt;/code&gt; 任务时，由于依赖冲突而失败，高低版本无法兼容。&lt;/p&gt;&lt;p&gt;本文记录问题解决过程以及经验总结，重要开发环境说明：&lt;code&gt;Spark v1.6&lt;/code&gt;、&lt;code&gt;es-hadoop v5.6.8&lt;/code&gt;、&lt;code&gt;kafka v0.9.x&lt;/code&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="踩坑系列" scheme="https://www.playpi.org/categories/series-of-fixbug/"/>
    
    
      <category term="Spark" scheme="https://www.playpi.org/tags/Spark/"/>
    
      <category term="Maven" scheme="https://www.playpi.org/tags/Maven/"/>
    
      <category term="shade" scheme="https://www.playpi.org/tags/shade/"/>
    
  </entry>
  
  <entry>
    <title>使用海龟绘图绘制一些植物</title>
    <link href="https://www.playpi.org/2019110201.html"/>
    <id>https://www.playpi.org/2019110201.html</id>
    <published>2019-11-02T13:58:18.000Z</published>
    <updated>2019-11-02T13:58:18.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>在 2019 年 10 月 1 日的时候，我尝试使用海龟绘图绘制了一面五星红旗，参考我的另外一篇博文：<a href="https://www.playpi.org/2019100101.html">使用海龟绘图绘制一面五星红旗 </a> ，我觉得挺好玩的，还想进一步了解一下相关知识。后来，我又探索了一些绘图内容，发现可以绘制一些植物，例如树木、花草，核心就是要定义好绘制曲线。本文记录几个常见的植物：樱花树、火树银花、玫瑰花。</p><a id="more"></a><p> 提前声明，下文中涉及的 <code>Python</code> 脚本已经被我上传至 <code>GitHub</code>，读者可以提前下载查看：<a href="https://github.com/iplaypi/iplaypipython/tree/master/iplaypipython/20191102" target="_blank" rel="noopener">绘制植物脚本 </a> ，脚本命名使用英文单词作为前缀。</p><h1 id="樱花树"><a href="# 樱花树" class="headerlink" title="樱花树"></a> 樱花树 </h1><p> 画樱花树的整体思路就是先绘制樱花树，再绘制地上的落叶。</p><p>其中，绘制樱花树使用了递归的方式，从主干开始绘制，绘制主干完成后分为左右两侧的枝干，不停递归绘制，对于长度比较长的枝干，仍旧按照主干的方式绘制，直到长度比较短的枝干，作为树枝末端存在，会有不同的颜色、粗细。</p><p>代码示例如下，里面包含了注释，很容易就能看懂：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/env python</span><br><span class="line"># coding=utf-8</span><br><span class="line"># 画一棵樱花树（模拟）</span><br><span class="line"># 导入 turtle 模块 </span><br><span class="line">import turtle</span><br><span class="line"># 导入 random 模块，每次绘制的樱花树形状随机 </span><br><span class="line">import random</span><br><span class="line">from turtle import *</span><br><span class="line">from time import sleep</span><br><span class="line"></span><br><span class="line"># 画樱花的躯干，传入躯干长度、画布 </span><br><span class="line"># 这里面会有递归调用 </span><br><span class="line"># 先画主干，然后递归画树枝，树枝越来越短，颜色会随机生成 </span><br><span class="line">def draw_tree (branchLen, t):</span><br><span class="line">    sleep (0.0005)</span><br><span class="line">    if branchLen &gt; 3:</span><br><span class="line">        # 末端的树枝 </span><br><span class="line">        if 8 &lt;= branchLen &lt;= 12:</span><br><span class="line">            # 随机生成画笔的颜色，用来画末端的树枝 </span><br><span class="line">            if random.randint (0,2) == 0:</span><br><span class="line">                # 白色 </span><br><span class="line">                t.color (&apos;snow&apos;)</span><br><span class="line">            else:</span><br><span class="line">                # 淡珊瑚色 </span><br><span class="line">                t.color (&apos;lightcoral&apos;)</span><br><span class="line">            # 画笔的线条粗细 </span><br><span class="line">            t.pensize (branchLen / 3)</span><br><span class="line">        elif branchLen &lt; 8:</span><br><span class="line">            if random.randint (0,1) == 0:</span><br><span class="line">                t.color (&apos;snow&apos;)</span><br><span class="line">            else:</span><br><span class="line">                t.color (&apos;lightcoral&apos;) # 淡珊瑚色 </span><br><span class="line">            t.pensize (branchLen / 2)</span><br><span class="line">        else:</span><br><span class="line">            # 树干的颜色赭 (zhě) 色、粗细 6</span><br><span class="line">            t.color (&apos;sienna&apos;)</span><br><span class="line">            t.pensize (branchLen / 10)</span><br><span class="line">        # 向前移动 branchLen 个像素 </span><br><span class="line">        t.forward (branchLen)</span><br><span class="line">        # 随机生成右转的角度 </span><br><span class="line">        a = 1.5 * random.random ()</span><br><span class="line">        t.right (20 * a)</span><br><span class="line">        # 递归画樱花树 </span><br><span class="line">        b = 1.5 * random.random ()</span><br><span class="line">        draw_tree (branchLen - 10 * b, t)</span><br><span class="line">        # 左转，递归画樱花树 </span><br><span class="line">        t.left (40 * a)</span><br><span class="line">        draw_tree (branchLen - 10 * b, t)</span><br><span class="line">        # 画笔回正方向，向前移动 </span><br><span class="line">        t.right (20 * a)</span><br><span class="line">        t.up ()</span><br><span class="line">        t.backward (branchLen)</span><br><span class="line">        t.down ()</span><br><span class="line"> </span><br><span class="line"># 掉落的花瓣，传入个数、画布 </span><br><span class="line">def draw_petal (m, t):</span><br><span class="line">    # 循环绘制 m 个花瓣 </span><br><span class="line">    for i in range (m):</span><br><span class="line">        # 生成随机的移动像素个数，a 用来控制左右的移动，b 用来控制上下的移动 </span><br><span class="line">        # a 大一点，b 小一点，总体可以让花瓣看起来有透视立体感 </span><br><span class="line">        a = 200 - 400 * random.random ()</span><br><span class="line">        b = 10 - 20 * random.random ()</span><br><span class="line">        # 以下就是到达花瓣位置 </span><br><span class="line">        # 提起画笔 </span><br><span class="line">        t.up ()</span><br><span class="line">        # 向前移动 b 个像素 </span><br><span class="line">        t.forward (b)</span><br><span class="line">        # 左转 90 度角度 </span><br><span class="line">        t.left (90)</span><br><span class="line">        # 向前移动 a 个像素 </span><br><span class="line">        t.forward (a)</span><br><span class="line">        # 放下画笔 </span><br><span class="line">        t.down ()</span><br><span class="line">        # 淡珊瑚色，花瓣的颜色 </span><br><span class="line">        t.color (&apos;lightcoral&apos;)</span><br><span class="line">        # 以下是绘制一个花瓣 </span><br><span class="line">        # 绘制一个圆 </span><br><span class="line">        t.circle (1)</span><br><span class="line">        # 以下就是回到中心点 </span><br><span class="line">        # 提起画笔 </span><br><span class="line">        t.up ()</span><br><span class="line">        # 向后移动 a 个像素 </span><br><span class="line">        t.backward (a)</span><br><span class="line">        # 右转 90 度角度 </span><br><span class="line">        t.right (90)</span><br><span class="line">        # 向后移动 b 个像素 </span><br><span class="line">        t.backward (b)</span><br><span class="line"></span><br><span class="line">def draw_cherry ():</span><br><span class="line">    # 海龟绘图区域 </span><br><span class="line">    t = turtle.Turtle ()</span><br><span class="line">    # 画布 </span><br><span class="line">    w = turtle.Screen ()</span><br><span class="line">    # 设置大小，4 个参数：宽度、高度、起始值 x 轴、起始值 y 轴 </span><br><span class="line">    w.setup (1000, 600, 200, 100)</span><br><span class="line">    # 设置背景为小麦颜色 </span><br><span class="line">    w.bgcolor (&apos;wheat&apos;)</span><br><span class="line">    # 隐藏画笔 </span><br><span class="line">    t.hideturtle ()</span><br><span class="line">    # 获取屏幕，并追踪 </span><br><span class="line">    t.getscreen ().tracer (5, 0)</span><br><span class="line">    t.left (90)</span><br><span class="line">    t.up ()</span><br><span class="line">    t.backward (200)</span><br><span class="line">    t.down ()</span><br><span class="line">    # 1、画樱花的躯干 </span><br><span class="line">    draw_tree (60, t)</span><br><span class="line">    # 2、画掉落的花瓣 </span><br><span class="line">    draw_petal (200, t)</span><br><span class="line">    # 3、点击退出 </span><br><span class="line">    w.exitonclick ()</span><br><span class="line"></span><br><span class="line"># 程序入口 </span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    print (&apos; 开始绘制樱花树 & apos;)</span><br><span class="line">    draw_cherry ()</span><br><span class="line">    print (&apos; 结束绘制樱花树 & apos;)</span><br><span class="line">    # input (&apos; 暂停，等待输入（输入任意内容按回车键可退出）：&apos;)</span><br></pre></td></tr></table></figure><p>运行结果如下，由于角度是随机生成的，所以每次运行结果都会不一样：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191105000726.png" alt="运行结果 1" title="运行结果 1"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191105000732.png" alt="运行结果 2" title="运行结果 2"></p><h1 id="火树银花"><a href="# 火树银花" class="headerlink" title="火树银花"></a>火树银花 </h1><p> 绘制火树银花的思路和上面的樱花树一致，只不过火树银花这个名字比较酷，树枝没有区分粗细，只区分长度、颜色，整个画面采用黑色背景，看起来非常闪耀。</p><p>需要注意的是，运行一次耗时比较长，大概需要 4-5 分钟。</p><p>代码内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"># !/usr/bin/python3</span><br><span class="line"># -*-coding:UTF-8-*-</span><br><span class="line"># 火树银花 </span><br><span class="line"></span><br><span class="line"># 导入海龟作图模块 </span><br><span class="line">import turtle</span><br><span class="line"># 导入随机数模块 </span><br><span class="line">import random as rm</span><br><span class="line"></span><br><span class="line"># 角度 </span><br><span class="line">angle = [15, 5, 10, 20, 25, 30]</span><br><span class="line"># 颜色数组，多种颜色供绘制时随机选择，青、红、粉、蓝、绿、黄 </span><br><span class="line">color = [&apos;yellow&apos;, &apos;green&apos;, &apos;blue&apos;, &apos;red&apos;, &apos;pink&apos;, &apos;cyan&apos;]</span><br><span class="line"></span><br><span class="line"># 绘制树干，传入长度、画布对象 </span><br><span class="line"># 绘制思路：根据长度的不同，生成的角度不同，树干会分为 2 个树枝，然后树枝再递归分叉 </span><br><span class="line"># 直到树枝的长度过小，变为树枝末梢，不再分叉 </span><br><span class="line">def draw_tree (branch_len, t, cr):</span><br><span class="line">    # 树干颜色 </span><br><span class="line">    t.color (cr)</span><br><span class="line">    # 设置画笔的粗细 </span><br><span class="line">    t.pensize (1)</span><br><span class="line">    # 随机选择颜色，不等于树干颜色，用于分叉树枝 </span><br><span class="line">    new_color = color [:]</span><br><span class="line">    new_color.remove (cr)</span><br><span class="line">    new_cr = rm.choice (new_color)</span><br><span class="line">    # 随机转动角度，用于分叉树枝 </span><br><span class="line">    ag1 = rm.choice (angle)</span><br><span class="line">    ag2 = rm.choice (angle)</span><br><span class="line">    # 分叉树枝的长度，默认等于树干的长度 </span><br><span class="line">    new_branch_len = branch_len</span><br><span class="line">    # 分叉树枝的长度重新计算，越来越短 </span><br><span class="line">    if branch_len &gt; 120:</span><br><span class="line">        new_branch_len = branch_len - 20</span><br><span class="line">    elif branch_len &gt;= 60:</span><br><span class="line">        new_branch_len = branch_len - 15</span><br><span class="line">    elif branch_len &gt;= 20:</span><br><span class="line">        new_branch_len = branch_len - 10</span><br><span class="line">    else:</span><br><span class="line">        new_branch_len = branch_len - 5</span><br><span class="line">    # 开始绘制 </span><br><span class="line">    if 10 &gt;= branch_len:</span><br><span class="line">        # 树枝太短，无需绘制，递归结束 </span><br><span class="line">        pass</span><br><span class="line">    else:</span><br><span class="line">        # 向前移动，绘制树干 </span><br><span class="line">        t.forward (branch_len)</span><br><span class="line">        # 右转指定角度 1，分叉 </span><br><span class="line">        t.right (ag1)</span><br><span class="line">        # 递归画树干，可以理解成子树 </span><br><span class="line">        draw_tree (new_branch_len, t, new_cr)</span><br><span class="line">        # 左转指定角度 2，分叉 </span><br><span class="line">        t.left (ag1 + ag2)</span><br><span class="line">        draw_tree (new_branch_len, t, new_cr)</span><br><span class="line">        # 角度回正，右转指定角度 2</span><br><span class="line">        t.right (ag2)</span><br><span class="line">        # 恢复颜色并后退 </span><br><span class="line">        t.color (cr)</span><br><span class="line">        t.backward (branch_len)</span><br><span class="line"></span><br><span class="line"># 开始绘制整棵树 </span><br><span class="line">def draw_fire_cilver ():</span><br><span class="line">    t = turtle.Turtle ()</span><br><span class="line">    w = turtle.Screen ()</span><br><span class="line">    # 设置背景为黑色 </span><br><span class="line">    w.bgcolor (&apos;black&apos;)</span><br><span class="line">    # 设置弹框大小，4 个参数：宽度、高度、起始值 x 轴、起始值 y 轴 </span><br><span class="line">    w.setup (1200, 800, 200, 50)</span><br><span class="line">    # 加快速度 </span><br><span class="line">    t.speed (10)</span><br><span class="line">    # 调整画笔的位置，开始的位置在中间偏下方 </span><br><span class="line">    t.left (90)</span><br><span class="line">    t.up ()</span><br><span class="line">    t.backward (400)</span><br><span class="line">    t.down ()</span><br><span class="line">    # 跟踪画笔，可以看到整个绘制轨迹 </span><br><span class="line">    turtle.tracer (5)</span><br><span class="line">    # 垂直位置绘制 1 棵，左右边再各绘制 3 棵，共 7 棵 </span><br><span class="line">    t.left (15)</span><br><span class="line">    for i in range (0,7):</span><br><span class="line">        # 绘制 1 棵，右转 5 度 </span><br><span class="line">        print (&apos;==== 绘制第 [&apos; + str (i + 1) + &apos;] 棵树 & apos;)</span><br><span class="line">        draw_tree (150, t, &apos;cyan&apos;)</span><br><span class="line">        t.right (5)</span><br><span class="line">    # 单机退出 </span><br><span class="line">    w.exitonclick ()</span><br><span class="line"></span><br><span class="line"># 主程序入口 </span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">    print (&apos; 开始绘制火树银花 & apos;)</span><br><span class="line">    draw_fire_cilver ()</span><br><span class="line">    print (&apos; 结束绘制火树银花 & apos;)</span><br><span class="line">    # input (&apos; 暂停，等待输入（输入任意内容按回车键可退出）：&apos;)</span><br></pre></td></tr></table></figure><p>运行结果如下图，由于角度、颜色也是随机生成的，所以每次运行结果是不一致的。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191105001116.png" alt="运行结果" title="运行结果"></p><p>在网络上找到的示例，看起来更好看一些。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191105001233.png" alt="网络上找到的示例" title="网络上找到的示例"></p><h1 id="玫瑰花"><a href="# 玫瑰花" class="headerlink" title="玫瑰花"></a>玫瑰花 </h1><p> 玫瑰花比较有意思，会涉及到非规则图形，花瓣的形状怎么绘制、绿叶的形状怎么绘制等。</p><p>简单思路：</p><ul><li>先绘制花瓣的边框，包括填充颜色 </li><li> 再绘制花瓣中的线条，凸显出花瓣的层次 </li><li> 绘制花枝主干 </li><li> 绘制两片绿叶，包括绿叶的枝条 </li></ul><p> 代码如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br></pre></td><td class="code"><pre><span class="line"># 导入海龟绘图模块 </span><br><span class="line">import turtle as t</span><br><span class="line"></span><br><span class="line"># 定义一个曲线绘制函数 </span><br><span class="line"># 思路就是画多个小圆弧，构成曲线 </span><br><span class="line"># n 表示画多少次圆弧，n 越大画的曲线越长 </span><br><span class="line"># r 表示圆弧半径，r 越大则曲线越平滑 </span><br><span class="line"># d=1 则是左弯的圆弧，d=-1 则是右弯的圆弧（由于屏幕的分辨率不同，有时候看不出来明显的弯度）</span><br><span class="line">def degree_curve (n, r, d=1):</span><br><span class="line">    for i in range (n):</span><br><span class="line">        t.left (d)</span><br><span class="line">        # r 是半径，abs (d) 是夹角 </span><br><span class="line">        t.circle (r, abs (d))</span><br><span class="line"></span><br><span class="line"># 绘制玫瑰花 </span><br><span class="line">def draw_rose (s):</span><br><span class="line">    # 设置画笔速度 </span><br><span class="line">    t.speed (100)</span><br><span class="line">    # 提起画笔，移动到指定位置 </span><br><span class="line">    t.penup ()</span><br><span class="line">    t.goto (0, 900 * s)</span><br><span class="line">    # 放下画笔 </span><br><span class="line">    t.pendown ()</span><br><span class="line"></span><br><span class="line">    # 开始填充，并绘制花朵形状 </span><br><span class="line">    t.begin_fill ()</span><br><span class="line">    # 起步花蕊的曲线，30 度圆弧，一层椭圆下侧 </span><br><span class="line">    t.circle (200 * s, 30)</span><br><span class="line">    # 左弯曲线，60 次半径为 10 的圆弧，一层椭圆右侧 </span><br><span class="line">    degree_curve (60, 50 * s)</span><br><span class="line">    # 和起步花蕊的曲线对称，30 度圆弧 </span><br><span class="line">    t.circle (200 * s, 30)</span><br><span class="line">    # 左弯曲线，4 次半径为 20 的圆弧，为了调整角度 </span><br><span class="line">    degree_curve (4, 100 * s)</span><br><span class="line">    # 50 度圆弧，一层椭圆上侧 </span><br><span class="line">    t.circle (200 * s, 50)</span><br><span class="line">    # 左弯曲线，50 次半径为 10 的圆弧，一层椭圆左侧下侧 </span><br><span class="line">    degree_curve (50, 50 * s)</span><br><span class="line">    # 65 度圆弧，一层椭圆下侧 </span><br><span class="line">    t.circle (350 * s, 65)</span><br><span class="line">    # 左弯曲线，40 次半径为 14 的圆弧，二层椭圆右侧 </span><br><span class="line">    degree_curve (40, 70 * s)</span><br><span class="line">    # 50 度圆弧，二层椭圆右侧上侧 </span><br><span class="line">    t.circle (150 * s, 50)</span><br><span class="line">    # 右弯曲线，20 次半径为 10 的圆弧，二层椭圆上侧 </span><br><span class="line">    degree_curve (20, 50 * s, -1)</span><br><span class="line">    # 60 度圆弧，二层椭圆上侧 </span><br><span class="line">    t.circle (400 * s, 60)</span><br><span class="line">    # 右弯曲线，18 次半径为 10 的圆弧，二层椭圆左侧 </span><br><span class="line">    degree_curve (18, 50 * s)</span><br><span class="line">    # 前进 125，直线，二层椭圆左侧连接处 </span><br><span class="line">    t.fd (250 * s)</span><br><span class="line">    # 右转 150 度 </span><br><span class="line">    t.right (150)</span><br><span class="line">    # 12 度圆弧，顺时针画圆，右弯曲线 </span><br><span class="line">    t.circle (-500 * s, 12)</span><br><span class="line">    # 左转 140 度 </span><br><span class="line">    t.left (140)</span><br><span class="line">    # 110 度圆弧，左侧花瓣边缘 </span><br><span class="line">    t.circle (550 * s, 110)</span><br><span class="line">    # 左转 27 度 </span><br><span class="line">    t.left (27)</span><br><span class="line">    # 100 度圆弧，右侧花瓣边缘 </span><br><span class="line">    t.circle (650 * s, 100)</span><br><span class="line">    # 左转 130 度 </span><br><span class="line">    t.left (130)</span><br><span class="line">    # 20 度圆弧，顺时针画圆 </span><br><span class="line">    t.circle (-300 * s, 20)</span><br><span class="line">    # 右转 123 度 </span><br><span class="line">    t.right (123)</span><br><span class="line">    # 57 度圆弧，连接到二层椭圆右侧 </span><br><span class="line">    t.circle (220 * s, 57)</span><br><span class="line">    # 至此图形封闭，颜色填充完成 </span><br><span class="line">    t.end_fill ()</span><br><span class="line"></span><br><span class="line">    # 绘制花枝形状，包括勾勒花瓣中间的线条 </span><br><span class="line">    # 左转 120 度 </span><br><span class="line">    t.left (120)</span><br><span class="line">    # 前进 140</span><br><span class="line">    t.fd (280 * s)</span><br><span class="line">    # 左转 115 度 </span><br><span class="line">    t.left (115)</span><br><span class="line">    # 33 度圆弧，连接到右侧花瓣边缘 </span><br><span class="line">    t.circle (300 * s, 33)</span><br><span class="line">    # 左转 180 度 </span><br><span class="line">    t.left (180)</span><br><span class="line">    # 33 度圆弧，顺时针，为了回到上一步画圆弧之前的位置 </span><br><span class="line">    t.circle (-300 * s, 33)</span><br><span class="line">    # 右弯曲线，70 次半径为 113 的圆弧，右侧花瓣线条 </span><br><span class="line">    degree_curve (70, 225 * s, -1)</span><br><span class="line">    # 104 度圆弧，右侧花瓣线条 </span><br><span class="line">    t.circle (350 * s, 104)</span><br><span class="line">    # 左转 90 度 </span><br><span class="line">    t.left (90)</span><br><span class="line">    # 105 度圆弧，左侧花瓣线条 </span><br><span class="line">    t.circle (200 * s, 105)</span><br><span class="line">    # 63 度弧度，顺时针，左侧花瓣线条，至此花瓣线条完成 </span><br><span class="line">    t.circle (-500 * s, 63)</span><br><span class="line">    # 提起画笔，移动到指定位置，花瓣与花枝连接处 </span><br><span class="line">    t.penup ()</span><br><span class="line">    t.goto (170 * s, -30 * s)</span><br><span class="line">    # 放下画笔 </span><br><span class="line">    t.pendown ()</span><br><span class="line">    # 左转 160 度，朝向调整为朝下 </span><br><span class="line">    t.left (160)</span><br><span class="line">    # 左弯曲线，20 次半径为 1250 的圆弧，花枝 </span><br><span class="line">    degree_curve (20, 2500 * s)</span><br><span class="line">    # 右弯曲线，220 次半径为 125 的圆弧，花枝 </span><br><span class="line">    degree_curve (220, 250 * s, -1)</span><br><span class="line"></span><br><span class="line">    # 下面开始绘制 2 片绿叶 </span><br><span class="line">    # 绘制一个绿色叶子，上方的 </span><br><span class="line">    t.fillcolor (&apos;green&apos;)</span><br><span class="line">    # 提起画笔移动到指定位置，叶尖 </span><br><span class="line">    t.penup ()</span><br><span class="line">    t.goto (670 * s, -180 * s)</span><br><span class="line">    t.pendown ()</span><br><span class="line">    # 右转 140 度，调整角度 </span><br><span class="line">    t.right (140)</span><br><span class="line">    # 开始填充 </span><br><span class="line">    t.begin_fill ()</span><br><span class="line">    # 120 度弧度，绿叶上侧 </span><br><span class="line">    t.circle (300 * s, 120)</span><br><span class="line">    # 左转 60 度 </span><br><span class="line">    t.left (60)</span><br><span class="line">    # 120 度弧度，绿叶下侧 </span><br><span class="line">    t.circle (300 * s, 120)</span><br><span class="line">    # 完成填充 </span><br><span class="line">    t.end_fill ()</span><br><span class="line">    t.penup ()</span><br><span class="line">    # 移动到绿叶枝条起始处 </span><br><span class="line">    t.goto (180 * s, -550 * s)</span><br><span class="line">    t.pendown ()</span><br><span class="line">    # 右转 85 度 </span><br><span class="line">    t.right (85)</span><br><span class="line">    # 40 度圆弧，绿叶枝条 </span><br><span class="line">    t.circle (600 * s, 40)</span><br><span class="line">    </span><br><span class="line">    # 绘制另一个绿色叶子，下方的 </span><br><span class="line">    # 提笔，移动到叶尖 </span><br><span class="line">    t.penup ()</span><br><span class="line">    t.goto (-150 * s, -1000 * s)</span><br><span class="line">    t.pendown ()</span><br><span class="line">    t.begin_fill ()</span><br><span class="line">    # 右转 120 度，调整角度 </span><br><span class="line">    t.rt (120)</span><br><span class="line">    # 115 度圆弧，叶子下侧 </span><br><span class="line">    t.circle (300 * s, 115)</span><br><span class="line">    # 左转 75 度 </span><br><span class="line">    t.left (75)</span><br><span class="line">    # 100 度弧度，叶子上侧 </span><br><span class="line">    t.circle (300 * s, 100)</span><br><span class="line">    t.end_fill ()</span><br><span class="line">    t.penup ()</span><br><span class="line">    # 移动到绿叶枝条起始处 </span><br><span class="line">    t.goto (430 * s, -1070 * s)</span><br><span class="line">    t.pendown ()</span><br><span class="line">    # 右转 30 度，调整角度 </span><br><span class="line">    t.right (30)</span><br><span class="line">    # 35 度圆弧，右弯，叶子枝条 </span><br><span class="line">    t.circle (-600 * s, 35)</span><br><span class="line">    # 等待退出 </span><br><span class="line">    t.exitonclick ()</span><br><span class="line"></span><br><span class="line"># 程序入口 </span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    print (&apos; 开始绘制玫瑰花 & apos;)</span><br><span class="line">    # 比例设定 </span><br><span class="line">    s = 0.2</span><br><span class="line">    # 设置弹窗大小 </span><br><span class="line">    t.setup (500 * 5 * s, 750 * 5 * s)</span><br><span class="line">    # 背景颜色，小麦色 </span><br><span class="line">    t.bgcolor (&apos;wheat&apos;)</span><br><span class="line">    # 设置画笔颜色，黑色 </span><br><span class="line">    t.pencolor (&quot;black&quot;)</span><br><span class="line">    # 设置填充颜色为红色，绘制花朵 </span><br><span class="line">    t.fillcolor (&quot;red&quot;)</span><br><span class="line">    draw_rose (s)</span><br><span class="line">    print (&apos; 结束绘制玫瑰花 & apos;)</span><br><span class="line">    # input (&apos; 暂停，等待输入（输入任意内容按回车键可退出）：&apos;)</span><br></pre></td></tr></table></figure><p>这里面的重点就是 <code>degree_curve (n, r, d=1)</code> 方法，它是为了绘制不规则图形而定义的。此外用的次数比较多的就是海龟绘图内置的 <code>circle</code> 方法，用来绘制标准的圆弧。</p><p>运行结果如下图，包含花瓣、绿叶。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191117003518.png" alt="玫瑰花运行结果" title="玫瑰花运行结果"></p><h1 id="参考"><a href="# 参考" class="headerlink" title="参考"></a>参考</h1><p><code>Python</code> 官方文档：<a href="https://docs.python.org/zh-cn/3/library/turtle.html" target="_blank" rel="noopener">Python3 文档说明</a></p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在 2019 年 10 月 1 日的时候，我尝试使用海龟绘图绘制了一面五星红旗，参考我的另外一篇博文：&lt;a href=&quot;https://www.playpi.org/2019100101.html&quot;&gt;使用海龟绘图绘制一面五星红旗&lt;/a&gt; ，我觉得挺好玩的，还想进一步了解一下相关知识。后来，我又探索了一些绘图内容，发现可以绘制一些植物，例如树木、花草，核心就是要定义好绘制曲线。本文记录几个常见的植物：樱花树、火树银花、玫瑰花。&lt;/p&gt;
    
    </summary>
    
      <category term="知识改变生活" scheme="https://www.playpi.org/categories/knowledge-for-life/"/>
    
    
      <category term="Python" scheme="https://www.playpi.org/tags/Python/"/>
    
      <category term="Turtle" scheme="https://www.playpi.org/tags/Turtle/"/>
    
      <category term="cherry" scheme="https://www.playpi.org/tags/cherry/"/>
    
      <category term="tree" scheme="https://www.playpi.org/tags/tree/"/>
    
      <category term="rose" scheme="https://www.playpi.org/tags/rose/"/>
    
  </entry>
  
  <entry>
    <title>重装系统后软件环境清单</title>
    <link href="https://www.playpi.org/2019102401.html"/>
    <id>https://www.playpi.org/2019102401.html</id>
    <published>2019-10-23T16:37:51.000Z</published>
    <updated>2019-10-23T16:37:51.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>最近又重装电脑系统了，<code>Windows 10</code>，需要搞很多开发工具的初始化，整个流程操作下来感觉挺麻烦的，而且显得很混乱。因此，我整理这一份文档，把基础环境的安装过程记录下来，需要安装哪些工具、配置哪些参数，都一一列举，为以后的重装系统做指导，同时也给部分读者参考。</p><p>以后如果有新增内容会持续补充。</p><a id="more"></a><h1 id="基础环境"><a href="# 基础环境" class="headerlink" title="基础环境"></a>基础环境 </h1><ol><li><p><code>JDK</code>，<code>Java</code> 开发基础工具，包含虚拟机、依赖包等。</p></li><li><p><code>Maven</code>，项目管理工具，非常流行。</p></li><li><p><code>Git</code>，版本控制系统，非常流行。</p></li><li><p><code>Nodejs</code>，一个流行的 <code>js</code> 库。</p></li><li><p><code>Python</code>，<code>Python</code> 开发基础工具。</p></li><li><p><code>Shadiwsocks</code>，一款代理客户端。</p></li><li><p><code>Chrome</code> 浏览器，一款流行全球的浏览器。</p></li><li><p><code>Gradle</code>，项目管理工具。</p></li><li><p><code>Scala</code>，<code>Scala</code> 开发基础工具。</p></li><li><p><code>Memory Analyzer</code>，一款内存诊断分析工具。</p></li></ol><h1 id="开发工具"><a href="# 开发工具" class="headerlink" title="开发工具"></a> 开发工具 </h1><ol><li><p><code>IDEA</code>，<code>IntelliJ</code> 系列的 <code>Java</code> 开发工具，非常流行。</p></li><li><p><code>PyCharm</code>，<code>IntelliJ</code> 系列的 <code>Python</code> 开发工具，非常流行。</p></li><li><p><code>Notepad++</code>，文本编辑器，小巧好用【但是作者是 <code>td</code>，可以废弃】。</p></li><li><p><code>Sublime</code>，文本编辑器，好用。</p></li><li><p><code>Navicat</code>，一款 <code>MySQL</code> 可视化管理工具，好用。</p></li></ol><p> 曾经在某个论坛发现有人贡献了一枚注册码：<code>NAVN-LNXG-XHHX-5NOO</code>，用户名可以任意指定。</p><ol start="6"><li><p><code>Aria2</code>，一款下载工具，支持多种协议，线程数可以自定义。</p></li><li><p><code>Typora</code>，一款 <code>Markdown</code> 编辑器，简洁好用。</p></li><li><p><code>Xshell</code>，一款 <code>ssh</code> 工具。</p></li><li><p><code>Xftp</code>，一款 <code>ftp</code> 工具，用来传输文件。</p></li><li><p><code>Everything</code>，文件快速搜索工具，索引创建成功后，基本秒出，比 <code>Windows</code> 自带的文件浏览器快得多。</p></li><li><p><code>Android Studio</code>，一款安卓开发工具，非常流行。</p></li><li><p><code>AndroidKiller</code>，一款安卓逆向工具。</p></li><li><p><code>IDAPro</code>，一款逆向开发工具。</p></li><li><p><code>Wireshark</code>，一款网络抓包工具。</p></li><li><p><code>PostMan</code>，一款 <code>HTTP</code> 调试工具，非常好用。</p></li><li><p><code>xMind</code>，一款脑图工具。</p></li><li><p><code>RedisDesktop</code>，一款 <code>Redis</code> 可视化管理工具。</p></li><li><p><code>picGo</code>，图床上传工具。</p></li><li><p><code>ProcessExplorer</code>，<code>Windows</code> 进程查看工具。</p></li><li><p><code>Imagine</code>，一款图片压缩工具，压缩大小而不失真。</p></li><li><p><code>EditPlus</code>，文本编辑器。</p></li></ol><h1 id="附加工具"><a href="# 附加工具" class="headerlink" title="附加工具"></a>附加工具 </h1><ol><li><p> 迅雷下载，一款下载工具。</p></li><li><p>迅雷看看，一款视频播放工具。</p></li><li><p><code>PhotoShop</code>，即大家所说的 <code>PS</code>，用来修图。</p></li><li><p><code>Premiere</code>，即大家所说的 <code>PR</code>，用来剪辑视频。</p></li><li><p><code>ffmpeg</code>，视频剪辑命令行工具，小巧好用，例如截取、拼接、格式转换。</p></li><li><p><code>EmEditor</code>，专为 <code>csv</code> 文件开发，可以打开超大文件，例如几 <code>GB</code>、几十万行的文件可以被轻松打开【当然，操作系统的内存要大一点，例如 <code>8GB</code>、<code>16GB</code>】。</p></li></ol><p>曾经逛相关论坛，发现有人贡献了几个注册码，有效期到 2021 年 6 月份，可以试用一下【注册码可以开启一些高级功能】：</p><ul><li>DEAZV-27TFM-BL52D-PVN9L-ADULD，2021-06-11 到期 </li><li>DEAZW-38TGM-HH52D-XG5WR-FX4QW，2021-06-11 到期</li><li>DMAZW-48TGM-LQ52C-G82V6-2JJUC，2021-06-10 到期</li><li>DMAZW-4ATGM-QL52D-M6XEM-TCFCS，2021-06-11 到期</li></ul><ol start="7"><li><p><code>IDM</code>，即 <code>Internet Download Manager</code>，网络下载工具，支持多种协议，例如可以下载 <code>Youtube</code> 的视频</p></li><li><p><code>Tomcat</code>，一款服务器。</p></li><li><p><code>Nginx</code>，一款服务器。</p></li><li><p><code>Ditto</code>，好用的粘贴板工具，高效便捷。</p></li><li><p><code>HandShaker</code>，锤子科技的手机文件管理器，可以方便把手机连接至电脑。</p></li><li><p><code>Office</code>，微软系列的办公软件，例如 <code>Word</code>、<code>Excel</code>、<code>PPT</code> 等。</p></li><li><p><code>openVPN</code>，<code>VPN</code> 连接工具。</p></li><li><p><code>UItraCompare</code>，一款文件比较器，<code>UItra</code> 系列，另外还有一个 <code>UItraEdit</code>，文本编辑器。</p></li><li><p><code>WinRAR</code>，一款解压、压缩工具。</p></li><li><p> 易我数据恢复，一款数据恢复工具。</p></li><li><p><code>WPS</code>，金山系列的办公软件，例如 <code>Word</code>、<code>Excel</code>、<code>PPT</code> 等。</p></li><li><p><code>youtubu-dl</code>，一款下载工具，使用命令行操作，需要搭配 <code>Aria2</code> 使用</p></li></ol><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;最近又重装电脑系统了，&lt;code&gt;Windows 10&lt;/code&gt;，需要搞很多开发工具的初始化，整个流程操作下来感觉挺麻烦的，而且显得很混乱。因此，我整理这一份文档，把基础环境的安装过程记录下来，需要安装哪些工具、配置哪些参数，都一一列举，为以后的重装系统做指导，同时也给部分读者参考。&lt;/p&gt;&lt;p&gt;以后如果有新增内容会持续补充。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="https://www.playpi.org/categories/essay/"/>
    
    
      <category term="Windows" scheme="https://www.playpi.org/tags/Windows/"/>
    
      <category term="init" scheme="https://www.playpi.org/tags/init/"/>
    
  </entry>
  
  <entry>
    <title>HBase 错误：The node hbase is not in ZooKeeper</title>
    <link href="https://www.playpi.org/2019101901.html"/>
    <id>https://www.playpi.org/2019101901.html</id>
    <published>2019-10-19T12:15:11.000Z</published>
    <updated>2019-10-19T12:15:11.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>使用 <code>phoenix</code> 向 <code>HBase</code> 中导入数据，使用的是 <code>phoenix</code> 自带的脚本 <code>psql.py</code>，结果报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">19/10/18 11:47:29 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br></pre></td></tr></table></figure><p>看起来是 <code>ZooKeeper</code> 环境有问题，本文记录解决过程。</p><p>本文开发环境基于 <code>HBase v1.1.2</code>、<code>phoenix v4.2.0</code> 。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 使用 <code>phoenix</code> 自带的导数脚本 <code>psql.py</code>，执行导入操作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">psql.py -t YOUR_TABLE dev4:2181 ./content.csv</span><br></pre></td></tr></table></figure><p>其中，<code>dev4:2191</code> 是 <code>Zookeeper</code> 集群节点，<code>./content.csv</code> 是数据文件，结果出现异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">19/10/18 11:47:29 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br><span class="line">19/10/18 11:47:29 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br><span class="line">19/10/18 11:47:29 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br><span class="line">19/10/18 11:47:30 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br><span class="line">19/10/18 11:47:31 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &apos;zookeeper.znode.parent&apos;. There could be a mismatch with the one configured in the master.</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191019204203.png" alt="导入数据异常" title="导入数据异常"></p><p>看起来是 <code>Zookeeper</code> 中缺失 <code>/hbase</code> 节点目录。</p><h1 id="问题解决"><a href="# 问题解决" class="headerlink" title="问题解决"></a>问题解决 </h1><p> 从 <code>stackoverflow</code> 上面查到一条类似的问题，见备注链接。</p><p>表面原因的确是 <code>Zookeeper</code> 中缺失 <code>/hbase</code> 节点目录，因为 <code>phoenix</code> 需要从这个节点获取 <code>HBase</code> 集群的信息，例如表结构，节点目录缺失则无法获取。</p><p>查看 <code>conf/hbase-site.xml</code> 文件，找到配置项：<code>zookeeper.znode.parent</code>，它就是表示 <code>HBase</code> 在 <code>ZooKeeper</code> 中的管理目录，里面存储着关于 <code>HBase</code> 集群的各项重要信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">  &lt;name&gt;zookeeper.znode.parent&lt;/name&gt;</span><br><span class="line">  &lt;value&gt;/hbase-unsecure&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>再去查看 <code>conf/hbase-env.sh</code> 里面的配置信息：<code>HBASE_MANAGES_ZK</code>，这个参数是告诉 <code>HBase</code> 是否使用自带的 <code>ZooKeeper</code> 管理 <code>HBase</code> 集群。如果为 <code>true</code>，则使用自带的 <code>ZooKeeper</code>；如果为 <code>false</code>，则使用外部的 <code>ZooKeeper</code>。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191019204433.png" alt="查看 hbase-env.sh 文件" title="查看 hbase-env.sh 文件"></p><p>可以看到我这里的参数设置的是 <code>false</code>，也就是使用外部的 <code>ZooKeeper</code> 集群。</p><p>在这里多说一下这个参数的不同值的使用场景：</p><ul><li>默认值为 <code>true</code>，但是，自带的 <code>ZooKeeper</code> 只能为单机或伪分布模式下的 <code>HBase</code> 提供服务，一般用于学习场景或者测试环境，比较方便管理 </li><li> 如果设置为 <code>false</code>，则使用外部的 <code>ZooKeeper</code> 管理 <code>HBase</code>，此时 <code>HBase</code> 既可以是单机模式、伪分布式模式，也可以是分布式模式，重点只有一个，需要自己搭建一套 <code>ZooKeeper</code> 集群 </li><li> 如果设置为 <code>true</code>，并且 <code>HBase</code> 使用伪分布式模式，则在启动 <code>HBase</code> 时，<code>HBase</code> 将 <code>Zookeeper</code> 作为自身的一部分运行，进程变为 <code>HQuorumPeer</code></li><li>一般建议使用 <code>false</code>，然后自己再单独搭建一套 <code>ZooKeeper</code>，这才是真生的分布式环境；当然，如果觉得复杂，只是自己学习、测试的时候使用，可以设置为 <code>true</code></li></ul><p>言归正传，既然使用的是外部的 <code>ZooKeeper</code>，也就是我这里指定的 <code>dev4:2181</code>，可见 <code>HBase</code> 集群已经设置了自己在 <code>Zookeeper</code> 中的元信息管理目录，而 <code>phoenix</code> 为什么要去另外一个目录 <code>/hbase</code> 获取呢。这里可能是 <code>phoenix</code> 的配置有问题。</p><p>不妨先去里面看一下是否存在 <code>/hbase</code> 节点即可，经过查看，没有这个节点。如果没有的话，也不妨先重新创建一个，使用：<code>create /hbase&quot;&quot;</code> 创建一个空内容节点，确保节点存在。</p><p>注意，这里只是创建了一个空节点，里面并没有任何信息，所以 <code>phoenix</code> 从里面是无法获取关于 <code>HBase</code> 集群的信息的。</p><p>测试了一下，果然，还是无法导入数据，抛出超时异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">19/10/19 20:47:12 WARN impl.MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-phoenix.properties,hadoop-metrics2.properties</span><br><span class="line">org.apache.phoenix.exception.PhoenixIOException: callTimeout=600000, callDuration=1024368: </span><br><span class="line">at org.apache.phoenix.util.ServerUtil.parseServerException (ServerUtil.java:108)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated (ConnectionQueryServicesImpl.java:840)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl.createTable (ConnectionQueryServicesImpl.java:1134)</span><br><span class="line">at org.apache.phoenix.query.DelegateConnectionQueryServices.createTable (DelegateConnectionQueryServices.java:110)</span><br><span class="line">at org.apache.phoenix.schema.MetaDataClient.createTableInternal (MetaDataClient.java:1591)</span><br><span class="line">at org.apache.phoenix.schema.MetaDataClient.createTable (MetaDataClient.java:569)</span><br><span class="line">at org.apache.phoenix.compile.CreateTableCompiler$2.execute (CreateTableCompiler.java:175)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixStatement$2.call (PhoenixStatement.java:271)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixStatement$2.call (PhoenixStatement.java:263)</span><br><span class="line">at org.apache.phoenix.call.CallRunner.run (CallRunner.java:53)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixStatement.executeMutation (PhoenixStatement.java:261)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixStatement.executeUpdate (PhoenixStatement.java:1043)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl$9.call (ConnectionQueryServicesImpl.java:1561)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl$9.call (ConnectionQueryServicesImpl.java:1530)</span><br><span class="line">at org.apache.phoenix.util.PhoenixContextExecutor.call (PhoenixContextExecutor.java:77)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl.init (ConnectionQueryServicesImpl.java:1530)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixDriver.getConnectionQueryServices (PhoenixDriver.java:162)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixEmbeddedDriver.connect (PhoenixEmbeddedDriver.java:126)</span><br><span class="line">at org.apache.phoenix.jdbc.PhoenixDriver.connect (PhoenixDriver.java:133)</span><br><span class="line">at java.sql.DriverManager.getConnection (DriverManager.java:664)</span><br><span class="line">at java.sql.DriverManager.getConnection (DriverManager.java:208)</span><br><span class="line">at org.apache.phoenix.util.PhoenixRuntime.main (PhoenixRuntime.java:182)</span><br><span class="line">Caused by: java.net.SocketTimeoutException: callTimeout=600000, callDuration=1024368: </span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries (RpcRetryingCaller.java:156)</span><br><span class="line">at org.apache.hadoop.hbase.client.HBaseAdmin.executeCallable (HBaseAdmin.java:3390)</span><br><span class="line">at org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor (HBaseAdmin.java:408)</span><br><span class="line">at org.apache.hadoop.hbase.client.HBaseAdmin.getTableDescriptor (HBaseAdmin.java:429)</span><br><span class="line">at org.apache.phoenix.query.ConnectionQueryServicesImpl.ensureTableCreated (ConnectionQueryServicesImpl.java:772)</span><br><span class="line">... 20 more</span><br><span class="line">Caused by: org.apache.hadoop.hbase.MasterNotRunningException: java.io.IOException: Can&apos;t get master address from ZooKeeper; znode data == null</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$StubMaker.makeStub (ConnectionManager.java:1671)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$MasterServiceStubMaker.makeStub (ConnectionManager.java:1697)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getKeepAliveMasterService (ConnectionManager.java:1914)</span><br><span class="line">at org.apache.hadoop.hbase.client.HBaseAdmin$MasterCallable.prepare (HBaseAdmin.java:3363)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries (RpcRetryingCaller.java:125)</span><br><span class="line">... 24 more</span><br><span class="line">Caused by: java.io.IOException: Can&apos;t get master address from ZooKeeper; znode data == null</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MasterAddressTracker.getMasterAddress (MasterAddressTracker.java:114)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$StubMaker.makeStubNoRetries (ConnectionManager.java:1597)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation$StubMaker.makeStub (ConnectionManager.java:1643)</span><br><span class="line">... 28 more</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191019211617.png" alt="导入数据再次出现异常" title="导入数据再次出现异常"></p><p>可以看到，里面有 <code>Can&#39;t get master address from ZooKeeper</code> 字样，也就是无法从 <code>Zookeeper</code> 指定的目录中获取关于 <code>HBase</code> 的主节点信息，可见，单纯在 <code>Zookeeper</code> 中创建一个 <code>/hbase</code> 目录是没用的。因此，源头应该在于 <code>phoenix</code> 为什么不去 <code>/hbase-unsecure</code> 目录中获取 <code>HBase</code> 集群信息【这才是 <code>HBase</code> 集群的信息所在地】，是哪里的配置出了问题。</p><p>经过排查，<code>phoenix</code> 脚本在加载 <code>hbase_conf_dir</code> 参数的时候，目录错误，因此没有获取到 <code>HBase</code> 相的配置文件，最终导致没有去 <code>Zookeeper</code> 的 <code>/hbase-unsecure</code> 目录读取数据。这里排查的是 <code>psql.py</code>、<code>phoenix_utils.py</code> 这两个文件，里面有关于加载 <code>HBase</code>、<code>Hadoop</code> 集群的配置目录的参数，如果赋值错误就会导致上述现象。</p><p>把 <code>hbase_conf_dir</code> 参数的加载过程梳理清楚，确保可以加载到 <code>HBASE_HOME/conf</code> 目录，接着就可以顺利导入数据了。</p><p>同时当然也需要 <code>HADOOP_HOME/conf</code>，但是我这里已经是正确的了，如果读者没有配置好，可能会遇到找不到 <code>hdfs</code> 的相关类，例如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: java.lang.ClassNotFoundException: Class org.apache.hadoop.hdfs.DistributedFileSystem not found</span><br></pre></td></tr></table></figure><p>最后一点需要注意，上传的 <code>csv</code> 文件内容列数要确保和 <code>HBase</code> 表的列数一致，并且不需要表头，否则无法成功导入【表头也会被当做内容】，日志也会报错提醒的。当然，字段也是有顺序的，<code>csv</code> 文件中字段的顺序要和 <code>HBase</code> 表中定义的一致。</p><p>顺利导入数据，导入成功，耗时 12 秒，导入 12000 条数据，从输出日志中可以看到详情。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191021215944.png" alt="数据导入成功" title="数据导入成功"></p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注</h1><p>1、参考：<a href="https://stackoverflow.com/questions/28605301/the-node-hbase-is-not-in-zookeeper" target="_blank" rel="noopener">HBase</a> ，这是个相似的问题。</p><p>2、如果数据量比较大的话，就不建议使用这种脚本导入的方式，反而可以使用 <code>xxx-client.jar</code> 包里面自带的处理类来执行，并提前把数据文件上传至 <code>hdfs</code>，然后后台会提交 <code>MapReduce</code> 任务来大批量导入数据。</p><p>3、数据导入、数据导出还可以使用 <code>pig</code> 这个工具。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;使用 &lt;code&gt;phoenix&lt;/code&gt; 向 &lt;code&gt;HBase&lt;/code&gt; 中导入数据，使用的是 &lt;code&gt;phoenix&lt;/code&gt; 自带的脚本 &lt;code&gt;psql.py&lt;/code&gt;，结果报错：&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;19/10/18 11:47:29 ERROR client.ConnectionManager$HConnectionImplementation: The node /hbase is not in ZooKeeper. It should have been written by the master. Check the value configured in &amp;apos;zookeeper.znode.parent&amp;apos;. There could be a mismatch with the one configured in the master.&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;看起来是 &lt;code&gt;ZooKeeper&lt;/code&gt; 环境有问题，本文记录解决过程。&lt;/p&gt;&lt;p&gt;本文开发环境基于 &lt;code&gt;HBase v1.1.2&lt;/code&gt;、&lt;code&gt;phoenix v4.2.0&lt;/code&gt; 。&lt;/p&gt;
    
    </summary>
    
      <category term="踩坑系列" scheme="https://www.playpi.org/categories/series-of-fixbug/"/>
    
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="Phoenix" scheme="https://www.playpi.org/tags/Phoenix/"/>
    
      <category term="Zookeeper" scheme="https://www.playpi.org/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>HBase 错误：NotServingRegionException</title>
    <link href="https://www.playpi.org/2019101201.html"/>
    <id>https://www.playpi.org/2019101201.html</id>
    <published>2019-10-12T12:19:31.000Z</published>
    <updated>2019-10-12T12:19:31.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>在使用 <code>SparkStreaming</code> 程序处理数据，结果写入 <code>HBase</code> 时，遇到异常 <code>NotServingRegionException</code>，只是突然出现一次，平时正常，怀疑是和开发环境有关，本文记录查找问题的过程。本文中涉及的开发环境为 <code>HBase v1.1.2</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p><code>SparkStreaming</code> 程序处理数据，结果写入 <code>HBase</code>，出现异常，并且一直持续：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">2019-10-13_16:40:31 [JobGenerator] INFO consumer.SimpleConsumer:68: Reconnect due to socket error: java.nio.channels.ClosedChannelException</span><br><span class="line">2019-10-13_16:40:32 [JobGenerator] INFO scheduler.JobScheduler:58: Added jobs for time 1570869630000 ms</span><br><span class="line">2019-10-13_16:40:33 [Executor task launch worker-0] INFO client.AsyncProcess:1656: #3, waiting for some tasks to finish. Expected max=0, tasksInProgress=29</span><br><span class="line">2019-10-13_16:40:34 [htable-pool3-t1] INFO client.AsyncProcess:1174: #3, table=YOUR_TABLE, attempt=29/35 failed=64ops, last exception: org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region YOUR_TABLE,f,1565318245911.a70001dfe6d9320600286510318bfeb6. is not online on dev6,16020,1570795214262</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName (HRegionServer.java:2898)</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion (RSRpcServices.java:947)</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.RSRpcServices.multi (RSRpcServices.java:1994)</span><br><span class="line">at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod (ClientProtos.java:32213)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcServer.call (RpcServer.java:2114)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.CallRunner.run (CallRunner.java:101)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop (RpcExecutor.java:130)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run (RpcExecutor.java:107)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line"> on dev6,16020,1569724523487, tracking started null, retrying after=20058ms, replay=64ops</span><br><span class="line">2019-10-13_16:40:46 [scheduled-rate-update] INFO streaming.ScheduledRateController:136: MinRateCondition, rateLimit = -1, minRate = 400</span><br><span class="line">2019-10-13_16:40:46 [stream-rate-update] INFO streaming.ScheduledRateController:155: MinRateCondition&apos;s execute, numOfBatches = 38 vs 80</span><br><span class="line">2019-10-13_16:40:54 [Executor task launch worker-0] INFO client.AsyncProcess:1656: #3, waiting for some tasks to finish. Expected max=0, tasksInProgress=30</span><br><span class="line">2019-10-13_16:40:54 [htable-pool3-t1] INFO client.AsyncProcess:1174: #3, table=YOUR_TABLE, attempt=30/35 failed=64ops, last exception: org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region YOUR_TABLE,f,1565318245911.a70001dfe6d9320600286510318bfeb6. is not online on dev6,16020,1570795214262</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegionByEncodedName (HRegionServer.java:2898)</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.RSRpcServices.getRegion (RSRpcServices.java:947)</span><br><span class="line">at org.apache.hadoop.hbase.regionserver.RSRpcServices.multi (RSRpcServices.java:1994)</span><br><span class="line">at org.apache.hadoop.hbase.protobuf.generated.ClientProtos$ClientService$2.callBlockingMethod (ClientProtos.java:32213)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcServer.call (RpcServer.java:2114)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.CallRunner.run (CallRunner.java:101)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcExecutor.consumerLoop (RpcExecutor.java:130)</span><br><span class="line">at org.apache.hadoop.hbase.ipc.RpcExecutor$1.run (RpcExecutor.java:107)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line"> on dev6,16020,1569724523487, tracking started null, retrying after=20050ms, replay=64ops</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204453.png" alt="HBase 错误日志" title="HBase 错误日志"></p><p> 留意重点信息：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">NotServingRegionException</span><br><span class="line">is not online on dev6,16020,1570795214262</span><br></pre></td></tr></table></figure><p>通过初步排查，发现只有一个数据表有此问题，更换其它表数据就可以正常写入，看来是和环境有关。</p><p>通过 <code>phoenix</code> 进行查询，发现也无法查询出数据，报错超时：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java.lang.RuntimeException: org.apache.phoenix.exception.PhoenixIOException: org.apache.phoenix.exception.PhoenixIOException: Failed after attempts=36, exceptions:</span><br><span class="line">Sat Oct 12 16:30:48 CST 2019, null, java.net.SocketTimeoutException: callTimeout=60000, callDuration=70197: row &apos;0&apos; on table &apos;YOUR_TABLE&apos; at region=YOUR_TABLE,0,1565318245911.157723c2d47bbae2226f6286a56f0256., hostname=dev6,15020,1569724523487, seqNum=1627</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204535.png" alt="phoenix 查询超时" title="phoenix 查询超时"></p><p>但是从这个超时异常中看不到有效的线索。</p><p>接着通过 <code>RegionServer</code> 查看 <code>Region</code> 的分布，尝试搜索日志中出现的 <code>Region YOUR_TABLE,f,1565318245911.a70001dfe6d9320600286510318bfeb6</code>，发现不存在，看来这个表的 <code>Region</code> 信息有异常。</p><p>通过搜索问题关键词，在 <code>stackoverflow</code> 上面找到一个例子，出现这种现象是因为这个表的 <code>Region</code> 损坏了，导致无法找到指定的 <code>Region</code>，但是可以手动修复。</p><h1 id="问题解决"><a href="# 问题解决" class="headerlink" title="问题解决"></a>问题解决 </h1><p> 找问题原因，并且进一步得到了建议的解决方案，准备实施。</p><p>首先使用 <code>hbase hbck&quot;YOUR_TABLE&quot;</code> 检测数据表的状态，等待几十秒，会陆续打印出集群的状态以及表的状态：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">2019-10-13 17:44:20,160 INFO  [main-SendThread (dev5:2181)] zookeeper.ClientCnxn: Opening socket connection to server dev5/172.18.5.205:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">2019-10-13 17:44:20,247 INFO  [main-SendThread (dev5:2181)] zookeeper.ClientCnxn: Socket connection established to dev5/172.18.5.205:2181, initiating session</span><br><span class="line">2019-10-13 17:44:20,359 INFO  [main-SendThread (dev5:2181)] zookeeper.ClientCnxn: Session establishment complete on server dev5/172.18.5.205:2181, sessionid = 0x26d539786987a13, negotiated timeout = 40000</span><br><span class="line">Version: 1.1.2.2.4.2.0-258</span><br><span class="line">Number of live region servers: 3</span><br><span class="line">Number of dead region servers: 0</span><br><span class="line">Master: dev6,16000,1563773138374</span><br><span class="line">Number of backup masters: 1</span><br><span class="line">Average load: 382.6666666666667</span><br><span class="line">Number of requests: 0</span><br><span class="line">Number of regions: 1148</span><br><span class="line">Number of regions in transition: 30</span><br><span class="line">2019-10-13 17:44:24,693 INFO  [main] util.HBaseFsck: Loading regionsinfo from the hbase:meta table</span><br><span class="line"></span><br><span class="line">Number of empty REGIONINFO_QUALIFIER rows in hbase:meta: 0</span><br><span class="line">2019-10-13 17:44:24,954 INFO  [main] util.HBaseFsck: getHTableDescriptors == tableNames =&gt; [YOUR_TABLE]</span><br><span class="line">...</span><br><span class="line">2019-10-13 17:44:26,621 INFO  [main] util.HBaseFsck: Checking and fixing region consistency</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,6,1565318245911.60686e402d3b0e25edc210190f8290c6., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/60686e402d3b0e25edc210190f8290c6, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,9,1565318245911.3dab1e5fc8211112c46041544c8cf6a1., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/3dab1e5fc8211112c46041544c8cf6a1, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,0,1565318245911.157723c2d47bbae2226f6286a56f0256., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/157723c2d47bbae2226f6286a56f0256, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,3,1565318245911.8e6507c0aa0ba2f7864fb6adbab58cd4., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/8e6507c0aa0ba2f7864fb6adbab58cd4, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,f,1565318245911.a70001dfe6d9320600286510318bfeb6., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/a70001dfe6d9320600286510318bfeb6, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">ERROR: Region &#123; meta =&gt; YOUR_TABLE,c,1565318245911.e247e3f852573308fd554e07452fbe93., hdfs =&gt; hdfs://dev-hdfs/apps/hbase/data/data/default/YOUR_TABLE/e247e3f852573308fd554e07452fbe93, deployed =&gt; , replicaId =&gt; 0 &#125; not deployed on any region server.</span><br><span class="line">2019-10-13 17:44:26,732 INFO  [main] util.HBaseFsck: Handling overlap merges in parallel. set hbasefsck.overlap.merge.parallel to false to run serially.</span><br><span class="line">ERROR: There is a hole in the region chain between 0 and 1.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.</span><br><span class="line">ERROR: There is a hole in the region chain between 3 and 4.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.</span><br><span class="line">ERROR: There is a hole in the region chain between 6 and 7.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.</span><br><span class="line">ERROR: There is a hole in the region chain between 9 and a.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.</span><br><span class="line">ERROR: There is a hole in the region chain between c and d.  You need to create a new .regioninfo and region dir in hdfs to plug the hole.</span><br><span class="line">ERROR: Last region should end with an empty key. You need to create a new region and regioninfo in HDFS to plug the hole.</span><br><span class="line">ERROR: Found inconsistency in table YOUR_TABLE</span><br><span class="line">2019-10-13 17:44:26,747 INFO  [main] util.HBaseFsck: Computing mapping of all store files</span><br><span class="line">...</span><br><span class="line">2019-10-13 17:44:30,038 INFO  [main] util.HBaseFsck: Finishing hbck</span><br><span class="line">Summary:</span><br><span class="line">Table YOUR_TABLE is inconsistent.</span><br><span class="line">    Number of regions: 11</span><br><span class="line">    Deployed on:  dev4,16020,1570795191487 dev5,16020,1570795198827</span><br><span class="line">Table hbase:meta is okay.</span><br><span class="line">    Number of regions: 1</span><br><span class="line">    Deployed on:  dev5,16020,1570795198827</span><br><span class="line">12 inconsistencies detected.</span><br><span class="line">Status: INCONSISTENT</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204717.png" alt="hbase hbck 查看输出日志 - 1" title="hbase hbck 查看输出日志 - 1"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204734.png" alt="hbase hbck 查看输出日志 - 2" title="hbase hbck 查看输出日志 - 2"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204747.png" alt="hbase hbck 查看输出日志 - 3" title="hbase hbck 查看输出日志 - 3"></p><p>首先注意到前面那个不存在的 <code>Region</code> <code>a70001dfe6d9320600286510318bfeb6</code> 处于未部署状态，<code>RegionServer</code> 当然无法找到了。</p><p>可以看到最终的结论：<code>INCONSISTENT</code>，就是数据不一致。并且在输出日志里面还有说明出现了 <code>Region</code> 空洞【<code>Region hole</code>】。</p><p>那怎么解决呢，可以先尝试使用 <code>hbase hbck -fix&quot;YOUR_TABLE&quot;</code> 解决。</p><p>这里如果遇到操作 <code>HDFS</code> 无权限，记得切换用户 <code>export HADOOP_USER_NAME=hbase</code>，当然最好还是直接使用管理员权限操作：<br><code>sudo -u hbase hbase hbck -fix&quot;YOUR_TABLE&quot;</code>。</p><p>在修复过程中，仍旧会不断输出日志，如果看到：<br><code>util.HBaseFsck: Sleeping 10000ms before re-checking after fix...</code><br>则说明修复完成，为了验证修复结果，<code>HBase</code> 还会自动检测一次。</p><p>再次检测后，如果看到如下信息，说明修复成功：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Summary:</span><br><span class="line">Table YOUR_TABLE is okay.</span><br><span class="line">    Number of regions: 17</span><br><span class="line">2019-10-13 18:20:02,145 INFO  [main-EventThread] zookeeper.ClientCnxn: EventThread shut down</span><br><span class="line">    Deployed on:  dev4,16020,1570795191487 dev5,16020,1570795198827</span><br><span class="line">Table hbase:meta is okay.</span><br><span class="line">    Number of regions: 1</span><br><span class="line">    Deployed on:  dev5,16020,1570795198827</span><br><span class="line">0 inconsistencies detected.</span><br><span class="line">Status: OK</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191012204942.png" alt="hbase hbck fix 修复完成" title="hbase hbck fix 修复完成"></p><p>接着就可以继续正常写入数据了。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p> 参考 <code>stackoverflow</code> 上面的例子：<a href="https://stackoverflow.com/questions/37507878/hbase-fails-with-org-apache-hadoop-hbase-notservingregionexception-region-is-not" target="_blank" rel="noopener">notservingregionexception</a> 。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在使用 &lt;code&gt;SparkStreaming&lt;/code&gt; 程序处理数据，结果写入 &lt;code&gt;HBase&lt;/code&gt; 时，遇到异常 &lt;code&gt;NotServingRegionException&lt;/code&gt;，只是突然出现一次，平时正常，怀疑是和开发环境有关，本文记录查找问题的过程。本文中涉及的开发环境为 &lt;code&gt;HBase v1.1.2&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="SparkStreaming" scheme="https://www.playpi.org/tags/SparkStreaming/"/>
    
  </entry>
  
  <entry>
    <title>Git 异常之 Unlink of file</title>
    <link href="https://www.playpi.org/2019100801.html"/>
    <id>https://www.playpi.org/2019100801.html</id>
    <published>2019-10-08T12:48:39.000Z</published>
    <updated>2019-10-08T12:48:39.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:17 GMT+0800 (China Standard Time) --><p>在使用 <code>Git</code> 的时候，出现错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Unlink of file &apos;.git/objects/pack/pack-xx.idx&apos; failed. Should I try again? (y/n)</span><br></pre></td></tr></table></figure><p>连续出现几十次，看起来像是 <code>Git</code> 在操作索引文件时被拒绝了，可能是文件权限问题，或者文件被占用。</p><p>本文内容中涉及的 <code>Git</code> 版本为：<code>2.18.0.windows.1</code>，操作系统为：<code>Windows 7x64</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 在对一个普通的 <code>Git</code> 项目进行 <code>git pull</code> 操作的时候，出现错误，显示如下的交互询问内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) </span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-670222495fa872c140e7e231e36cb2701d76c86b.idx&apos; failed. Should I try again? (y/n) </span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6acdf7d3bbb7394f39b68e0e40b47ca0116fbfa2.idx&apos; failed. Should I try again? (y/n) </span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6ffde68d8af2eafb0803063b895291418ed5f465.idx&apos; failed. Should I try again? (y/n)</span><br></pre></td></tr></table></figure><p>尝试手动输入 <code>y</code> 或者 <code>n</code>，并没有什么效果，输入 <code>y</code> 后同样的错误会继续出现，输入 <code>n</code> 会接着提示下一个类似的文件错误。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; iled. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-61113bb66bb6a4dcc0893ee5e0b36bf30cf917e6.idx&apos; failed. Should I try again? (y/n) n</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-670222495fa872c140e7e231e36cb2701d76c86b.idx&apos; failed. Should I try again? (y/n) n</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6acdf7d3bbb7394f39b68e0e40b47ca0116fbfa2.idx&apos; failed. Should I try again? (y/n) n</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6ffde68d8af2eafb0803063b895291418ed5f465.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6ffde68d8af2eafb0803063b895291418ed5f465.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6ffde68d8af2eafb0803063b895291418ed5f465.idx&apos; failed. Should I try again? (y/n) y</span><br><span class="line">Unlink of file &apos;.git/objects/pack/pack-6ffde68d8af2eafb0803063b895291418ed5f465.idx&apos; failed. Should I try again? (y/n) y</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191008210804.png" alt="Git 文件被占用" title="Git 文件被占用"></p><p>可见是要把所有同类型的文件全部询问一次，看起来问题没那么简单。</p><p>如果有耐心的话，连续输入几十次 <code>n</code>，可能会把所有的文件都忽略掉，提示也就结束了，或者直接使用 <code>ctrl + c</code> 结束操作，强制退出，但是这样操作并没有从根本上解决这个问题。</p><h1 id="分析解决"><a href="# 分析解决" class="headerlink" title="分析解决"></a>分析解决 </h1><p> 经过查询分析，这个问题的根本原因是 <code>Git</code> 项目的文件被其它程序占用，导致 <code>Git</code> 没有权限变更这些文件。这些文件是 <code>Git</code> 产生的临时文件，需要从 <code>Git</code> 的工作区移除。</p><p>上面提及的其它程序极有可能是 <code>IDEA</code>、<code>Eclipse</code>、<code>Visual Studio</code> 等常用的开发工具。</p><p>参考：<a href="https://stackoverflow.com/questions/4389833/unlink-of-file-failed-should-i-try-again" target="_blank" rel="noopener">stackoverflow.com</a> 。</p><p>解决方案也很简单，把占用文件的程序关闭就行。但是有时候找不到是哪个程序占用了文件，怎么办，可以利用微软的 <code>Process Explorer</code> 工具，具体介绍参考备注内容。</p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注 </h1><p>1、<code>Process Explorer</code> 是一个任务管理器，目前由微软开发，仅用于 <code>Windows</code> 操作系统平台，可以查看系统的进程信息、资源占用信息、文件占用信息，官网地址：<a href="https://docs.microsoft.com/en-us/sysinternals/downloads/process-explorer" target="_blank" rel="noopener">Process Explorer</a> 。</p><p> 同时，这个工具目前在 <code>GitHub</code> 上已经开源，并重新命名为：<code>sysinternals</code>，<code>GitHub</code> 的地址：<a href="https://github.com/MicrosoftDocs/sysinternals/tree/live" target="_blank" rel="noopener">sysinternals</a> 。</p><p>使用时无需安装，解压后直接可以运行，在主界面依次选择 <code>Find</code> -&gt; <code>Find Handle or DLL</code>，在搜索框中输入程序的名字、文件的名字，点击搜索，就可以看到搜索结果了，例如正在运行的进程、文件的使用情况等。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191008210836.png" alt="使用 Process Explorer 查看文件占用情况" title="使用 Process Explorer 查看文件占用情况"></p><p>2、我留意到在上述的 <code>stackoverflow</code> 链接中，也有人建议先使用 <code>git gc</code> 来手动执行一下垃圾清理，把临时文件给清理掉，然才进行 <code>git pull</code> 操作。我没有测试过，但感觉也有道理，读者可以试试。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:17 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在使用 &lt;code&gt;Git&lt;/code&gt; 的时候，出现错误：&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;Unlink of file &amp;apos;.git/objects/pack/pack-xx.idx&amp;apos; failed. Should I try again? (y/n)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;连续出现几十次，看起来像是 &lt;code&gt;Git&lt;/code&gt; 在操作索引文件时被拒绝了，可能是文件权限问题，或者文件被占用。&lt;/p&gt;&lt;p&gt;本文内容中涉及的 &lt;code&gt;Git&lt;/code&gt; 版本为：&lt;code&gt;2.18.0.windows.1&lt;/code&gt;，操作系统为：&lt;code&gt;Windows 7x64&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="基础技术知识" scheme="https://www.playpi.org/categories/basic-technical-knowledge/"/>
    
    
      <category term="Git" scheme="https://www.playpi.org/tags/Git/"/>
    
  </entry>
  
  <entry>
    <title>使用海龟绘图绘制一面五星红旗</title>
    <link href="https://www.playpi.org/2019100101.html"/>
    <id>https://www.playpi.org/2019100101.html</id>
    <published>2019-10-01T14:57:58.000Z</published>
    <updated>2019-10-23T14:57:58.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:17 GMT+0800 (China Standard Time) --><p>今天是国庆节，中国正在举行建国七十周年大阅兵，很多人都在观看，我在家里也看了直播片段。在刷微博的过程中，无意中看到有人在介绍 <strong>海龟绘图 </strong>这个 <code>Python</code> 库，可以非常方便地绘制各种图形，其中有人提到可以绘制出一面五星红旗。</p><p>后来我查了一下，的确是可以，难度不大，只要理解基本的绘制流程即可，于是我尝试了一下，并成功绘制出一面五星红旗。本文记录过程，开发环境基于 <code>Python v3.8</code>、<code>Windows 10 x64</code>。</p><a id="more"></a><h1 id="绘制思路"><a href="# 绘制思路" class="headerlink" title="绘制思路"></a>绘制思路 </h1><p> 绘制思路很简单，不过在这里需要先理解坐标轴、画笔的颜色、背景色、角度等基础概念。</p><p>1、先设置弹框大小，也就是五星红旗的长、宽，单位是像素。</p><p>2、设置背景颜色为红色，设置五角星的线条、填充颜色都为黄色。</p><p>3、绘制中心的 1 个大五角星。</p><p>4、绘制边上的 4 个小五角星。</p><h1 id="绘制代码"><a href="# 绘制代码" class="headerlink" title="绘制代码"></a>绘制代码 </h1><p> 需要注意除了 <code>Python</code> 环境，还需要安装海龟绘图库，我使用的 <code>Python v3.8</code> 已经自带了这个库，如果读者有使用这个版本的 <code>Python</code> 则不需要再单独安装。如果是其它版本的 <code>Python</code>，可能缺失这个库，可以使用 <code>pip</code> 工具安装，参考安装命令：<code>pip install turtle</code>。</p><p>当然，可能还会有其它依赖缺失问题，不属于本文讨论的范围，请读者自行解决。</p><p>提醒读者，这里涉及到的代码已经被我上传至 <code>Github</code>，命名为：<code>__main__.py</code>，读者可以提前下载查看：<a href="https://github.com/iplaypi/iplaypipython/blob/master/iplaypipython/20191001/__main__.py" target="_blank" rel="noopener">main.py</a> 。</p><p>下面给出代码清单，包含注释，读者很容易看懂：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"># 从海龟绘图模块中导入全部函数 </span><br><span class="line"># 在 Python v3.8 中已经内置此模块，如果其它 Python 版本没有内置，需要使用 pip 安装 </span><br><span class="line">from turtle import *</span><br><span class="line"></span><br><span class="line"># 开始绘制五星红旗 </span><br><span class="line">def daw_flag ():</span><br><span class="line">    # 设置大小，4 个参数：宽度、高度、起始值 x 轴、起始值 y 轴 </span><br><span class="line">    setup (600, 400, 0, 0)</span><br><span class="line">    # 设置背景为红色 </span><br><span class="line">    bgcolor (&apos;red&apos;)</span><br><span class="line">    # 线条、填充颜色设置为黄色 </span><br><span class="line">    fillcolor (&apos;yellow&apos;)</span><br><span class="line">    color (&apos;yellow&apos;)</span><br><span class="line">    # 画笔运行速度 </span><br><span class="line">    speed (10)</span><br><span class="line"></span><br><span class="line">    # 大五角星绘制 </span><br><span class="line">    draw_star (-280, 100, 0, 150, 144, 0)</span><br><span class="line"></span><br><span class="line">    # 4 个小五角星绘制 </span><br><span class="line">    draw_star (-100, 180, 305, 50, 0, 144)</span><br><span class="line">    draw_star (-50, 110, 30, 50, 144, 0)</span><br><span class="line">    draw_star (-40, 50, 5, 50, 144, 0)</span><br><span class="line">    draw_star (-100, 10, 300, 50, 0, 144)</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"># 绘制五角星的方法，根据实际情况传入参数 </span><br><span class="line">def draw_star (gotox_val, gotoy_val, heading_val=0, fd_val=50, rt_val=0, lt_val=0):</span><br><span class="line">    # 开始填充 </span><br><span class="line">    begin_fill ()</span><br><span class="line">    # 提起画笔，此时可以任意移动画笔位置 </span><br><span class="line">    up ()</span><br><span class="line">    # 移动至指定坐标 </span><br><span class="line">    goto (gotox_val, gotoy_val)</span><br><span class="line">    # 设置朝向角度 </span><br><span class="line">    if (0 != heading):</span><br><span class="line">        setheading (heading_val)</span><br><span class="line">    # 放下画笔，此时再移动就开始绘制 </span><br><span class="line">    down ()</span><br><span class="line">    # for 循环，绘制 5 条边 </span><br><span class="line">    for i in range (5):</span><br><span class="line">        # forward，向前移动画笔指定单位，像素 </span><br><span class="line">        fd (fd_val)</span><br><span class="line">        if (0 != rt_val):</span><br><span class="line">            # right，向右旋转指定单位，度数 </span><br><span class="line">            rt (rt_val)</span><br><span class="line">        if (0 != lt_val):</span><br><span class="line">            # left，向左旋转指定单位，度数 </span><br><span class="line">            lt (lt_val)</span><br><span class="line">    # 结束填充 </span><br><span class="line">    end_fill ()</span><br><span class="line"></span><br><span class="line"># 程序入口 </span><br><span class="line">if __name__==&quot;__main__&quot;:</span><br><span class="line">    print (&apos; 开始绘制五星红旗 & apos;)</span><br><span class="line">    daw_flag ()</span><br><span class="line">    print (&apos; 结束绘制五星红旗 & apos;)</span><br><span class="line">    exitonclick ()</span><br><span class="line">    # input (&apos; 暂停，等待输入（输入任意内容按回车键可退出）：&apos;)</span><br></pre></td></tr></table></figure><p>可以看到，代码中有 3 个函数：主函数 <code>__main__</code>、绘制五星红旗函数 <code>draw_flag</code>、绘制五角星函数 <code>draw_star</code>，这 3 个函数存在调用关系，共同绘制出一面五星红旗。</p><p>我这里故意把绘制速度设置小一点，读者在运行过程中可以清楚地看到绘制的过程，点的移动、线的绘制可以看得很清楚。</p><p>运行结果。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20191031005634.png" alt="运行结果" title="运行结果"></p><h1 id="备注"><a href="# 备注" class="headerlink" title="备注"></a>备注</h1><p>1、<code>Python</code> 官方网站参考：<a href="https://www.python.org" target="_blank" rel="noopener">Python</a> 。</p><p>2、在 <code>Windows</code> 平台安装 <code>Python</code> 需要注意版本的选择，是 32 位还是 64 位要搞清楚，不然后续会引发一系列麻烦，哪怕卸载重装也会有麻烦。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:17 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;今天是国庆节，中国正在举行建国七十周年大阅兵，很多人都在观看，我在家里也看了直播片段。在刷微博的过程中，无意中看到有人在介绍 &lt;strong&gt;海龟绘图 &lt;/strong&gt;这个 &lt;code&gt;Python&lt;/code&gt; 库，可以非常方便地绘制各种图形，其中有人提到可以绘制出一面五星红旗。&lt;/p&gt;&lt;p&gt;后来我查了一下，的确是可以，难度不大，只要理解基本的绘制流程即可，于是我尝试了一下，并成功绘制出一面五星红旗。本文记录过程，开发环境基于 &lt;code&gt;Python v3.8&lt;/code&gt;、&lt;code&gt;Windows 10 x64&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="知识改变生活" scheme="https://www.playpi.org/categories/knowledge-for-life/"/>
    
    
      <category term="Python" scheme="https://www.playpi.org/tags/Python/"/>
    
      <category term="Turtle" scheme="https://www.playpi.org/tags/Turtle/"/>
    
      <category term="national" scheme="https://www.playpi.org/tags/national/"/>
    
  </entry>
  
  <entry>
    <title>HBase 错误之 NoClassDefFoundError：ProtobufUtil</title>
    <link href="https://www.playpi.org/2019093001.html"/>
    <id>https://www.playpi.org/2019093001.html</id>
    <published>2019-09-30T12:34:04.000Z</published>
    <updated>2019-10-01T12:34:04.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>背景说明：通过 <code>dubbo</code> 部署一个服务，服务中的业务逻辑会查询 <code>HBase</code> 表的数据，但是 <code>dubbo</code> 服务在初始化注册时，<code>HBase</code> 初始化的过程中会报错：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil</span><br></pre></td></tr></table></figure><p>本文涉及的开发环境，基于 <code>HBase v1.1.2</code>、<code>Zookeeper v3.4.6</code>、<code>dubbo v2.8.4</code>、<code>Hadoop v2.7.1</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 通过 <code>k8s</code> 多节点发布服务，但是只有在某一台机器上面出现错误【其它节点日志显示正常，也可以提供正常的服务】，发布后 <code>dubbo</code> 服务注册初始化时出现的错误如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">2019-09-19_18:03:49 [http-nio-28956-exec-2-SendThread (192.168.20.101:2181)] INFO zookeeper.ClientCnxn:852: Socket connection established to 192.168.20.101/192.168.20.101:2181, initiating session</span><br><span class="line">2019-09-19_18:03:49 [http-nio-28956-exec-2-SendThread (192.168.20.101:2181)] INFO zookeeper.ClientCnxn:1235: Session establishment complete on server 192.168.20.101/192.168.20.101:2181, sessionid = 0x36af032f505e830, negotiated timeout = 90000</span><br><span class="line">2019-09-19_18:03:50 [http-nio-28956-exec-2] WARN hdfs.DFSUtil:689: Namenode for hdfs-cluster remains unresolved for ID nn1.  Check your hdfs-site.xml file to ensure namenodes are configured properly.</span><br><span class="line">2019-09-19_18:03:50 [http-nio-28956-exec-9] ERROR filter.ExceptionFilter:87:  [DUBBO] Got unchecked and undeclared exception which called by 10.200.0.2. service: com.yyy.zzz.service.es.weibo.IXxxService, method: search, exception: java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil, dubbo version: 2.8.4, current host: 127.0.0.1</span><br><span class="line">java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState (MetaTableLocator.java:482)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation (MetaTableLocator.java:167)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:598)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:579)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:558)</span><br><span class="line">at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation (ZooKeeperRegistry.java:61)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta (ConnectionManager.java:1192)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1159)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.relocateRegion (ConnectionManager.java:1133)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta (ConnectionManager.java:1338)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1162)</span><br><span class="line">at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.findAllLocationsOrFail (AsyncProcess.java:940)</span><br><span class="line">at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.groupAndSendMultiAction (AsyncProcess.java:857)</span><br><span class="line">at org.apache.hadoop.hbase.client.AsyncProcess$AsyncRequestFutureImpl.access$100 (AsyncProcess.java:575)</span><br><span class="line">at org.apache.hadoop.hbase.client.AsyncProcess.submitAll (AsyncProcess.java:557)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.batch (HTable.java:933)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.batch (HTable.java:950)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:911)</span><br><span class="line">at com.yyy.zzz.commons.search.reader.hbase.BaseHBaseReader.batchGet (BaseHBaseReader.java:94)</span><br><span class="line">at com.yyy.zzz.commons.search.reader.hbase.weibo.WeiboContentHbaseReader.batchGet (WeiboContentHbaseReader.java:98)</span><br><span class="line">at com.yyy.zzz.commons.search.searcher.AbstractBaseSearcher.getContent (AbstractBaseSearcher.java:269)</span><br><span class="line">at com.yyy.zzz.commons.search.searcher.AbstractBaseSearcher.getInfo (AbstractBaseSearcher.java:188)</span><br><span class="line">at com.yyy.zzz.runner.search.BaseSearchRunner.search (BaseSearchRunner.java:89)</span><br><span class="line">at com.yyy.zzz.api.weibo.WeiboContentServiceImpl.search (WeiboContentServiceImpl.java:33)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.Wrapper3.invokeMethod (Wrapper3.java)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory$1.doInvoke (JavassistProxyFactory.java:46)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.AbstractProxyInvoker.invoke (AbstractProxyInvoker.java:72)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.InvokerWrapper.invoke (InvokerWrapper.java:53)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ExceptionFilter.invoke (ExceptionFilter.java:64)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.monitor.support.MonitorFilter.invoke (MonitorFilter.java:75)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.TimeoutFilter.invoke (TimeoutFilter.java:42)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.dubbo.filter.TraceFilter.invoke (TraceFilter.java:78)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ContextFilter.invoke (ContextFilter.java:70)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.GenericFilter.invoke (GenericFilter.java:132)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ClassLoaderFilter.invoke (ClassLoaderFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.EchoFilter.invoke (EchoFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.InvokerInvocationHandler.invoke (InvokerInvocationHandler.java:52)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.proxy1.search (proxy1.java)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke (Method.java:498)</span><br><span class="line">at org.jboss.resteasy.core.MethodInjectorImpl.invoke (MethodInjectorImpl.java:137)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget (ResourceMethodInvoker.java:288)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:242)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:229)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:356)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:179)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.ServletContainerDispatcher.service (ServletContainerDispatcher.java:220)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:56)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:51)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.rest.DubboHttpServer$RestHandler.handle (DubboHttpServer.java:86)</span><br><span class="line">at com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet.service (DispatcherServlet.java:64)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:291)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter (ApplicationFilterChain.java:206)</span><br><span class="line">at org.apache.catalina.core.StandardWrapperValve.invoke (StandardWrapperValve.java:219)</span><br><span class="line">at org.apache.catalina.core.StandardContextValve.invoke (StandardContextValve.java:106)</span><br><span class="line">at org.apache.catalina.authenticator.AuthenticatorBase.invoke (AuthenticatorBase.java:504)</span><br><span class="line">at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:142)</span><br><span class="line">at org.apache.catalina.valves.ErrorReportValve.invoke (ErrorReportValve.java:79)</span><br><span class="line">at org.apache.catalina.core.StandardEngineValve.invoke (StandardEngineValve.java:88)</span><br><span class="line">at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:534)</span><br><span class="line">at org.apache.coyote.http11.AbstractHttp11Processor.process (AbstractHttp11Processor.java:1081)</span><br><span class="line">at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process (AbstractProtocol.java:658)</span><br><span class="line">at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process (Http11NioProtocol.java:222)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun (NioEndpoint.java:1566)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run (NioEndpoint.java:1523)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run (TaskThread.java:61)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br></pre></td></tr></table></figure><p>注意查看重点的内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">2019-09-19_18:03:50 [http-nio-28956-exec-2] WARN hdfs.DFSUtil:689: Namenode for hdfs-cluster remains unresolved for ID nn1.  Check your hdfs-site.xml file to ensure namenodes are configured properly.</span><br><span class="line">java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil</span><br></pre></td></tr></table></figure><p>第一行是 <code>hdfs</code> 无法解析 <code>HA</code> 的域名，应该是系统环境问题；第二行是 <code>HBase</code> 初始化环境失败，看起来像是缺失依赖包或者依赖包冲突导致的 <code>NoClassDefFoundError</code>。</p><p>同时还出现了未知主机名异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line">com.yyy.zzz.exception.es.EsConnException: java.net.UnknownHostException: host40: Temporary failure in name resolution</span><br><span class="line">at com.yyy.zzz.commons.infrastructure.client.EsClient.&lt;init&gt;(EsClient.java:46)</span><br><span class="line">at com.yyy.zzz.commons.infrastructure.client.EsClient.getInstance (EsClient.java:57)</span><br><span class="line">at com.yyy.zzz.commons.search.searcher.AbstractBaseSearcher.&lt;init&gt;(AbstractBaseSearcher.java:69)</span><br><span class="line">at com.yyy.zzz.commons.search.searcher.weibo.WeiboContentSearcher.&lt;init&gt;(WeiboContentSearcher.java:14)</span><br><span class="line">at com.yyy.zzz.commons.search.searcher.weibo.WeiboContentSearcher.getInstance (WeiboContentSearcher.java:22)</span><br><span class="line">at com.yyy.zzz.runner.search.weibo.WeiboContentSearchRunner.&lt;init&gt;(WeiboContentSearchRunner.java:26)</span><br><span class="line">at com.yyy.zzz.runner.search.weibo.WeiboContentSearchRunner.&lt;init&gt;(WeiboContentSearchRunner.java:20)</span><br><span class="line">at com.yyy.zzz.api.weibo.WeiboContentServiceImpl.search (WeiboContentServiceImpl.java:32)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.Wrapper3.invokeMethod (Wrapper3.java)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory$1.doInvoke (JavassistProxyFactory.java:46)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.AbstractProxyInvoker.invoke (AbstractProxyInvoker.java:72)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.InvokerWrapper.invoke (InvokerWrapper.java:53)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ExceptionFilter.invoke (ExceptionFilter.java:64)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.monitor.support.MonitorFilter.invoke (MonitorFilter.java:75)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.TimeoutFilter.invoke (TimeoutFilter.java:42)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.dubbo.filter.TraceFilter.invoke (TraceFilter.java:78)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ContextFilter.invoke (ContextFilter.java:70)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.GenericFilter.invoke (GenericFilter.java:132)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ClassLoaderFilter.invoke (ClassLoaderFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.EchoFilter.invoke (EchoFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.InvokerInvocationHandler.invoke (InvokerInvocationHandler.java:52)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.proxy1.search (proxy1.java)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke (Method.java:498)</span><br><span class="line">at org.jboss.resteasy.core.MethodInjectorImpl.invoke (MethodInjectorImpl.java:137)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget (ResourceMethodInvoker.java:288)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:242)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:229)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:356)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:179)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.ServletContainerDispatcher.service (ServletContainerDispatcher.java:220)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:56)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:51)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.rest.DubboHttpServer$RestHandler.handle (DubboHttpServer.java:86)</span><br><span class="line">at com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet.service (DispatcherServlet.java:64)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:291)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter (ApplicationFilterChain.java:206)</span><br><span class="line">at org.apache.catalina.core.StandardWrapperValve.invoke (StandardWrapperValve.java:219)</span><br><span class="line">at org.apache.catalina.core.StandardContextValve.invoke (StandardContextValve.java:106)</span><br><span class="line">at org.apache.catalina.authenticator.AuthenticatorBase.invoke (AuthenticatorBase.java:504)</span><br><span class="line">at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:142)</span><br><span class="line">at org.apache.catalina.valves.ErrorReportValve.invoke (ErrorReportValve.java:79)</span><br><span class="line">at org.apache.catalina.core.StandardEngineValve.invoke (StandardEngineValve.java:88)</span><br><span class="line">at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:534)</span><br><span class="line">at org.apache.coyote.http11.AbstractHttp11Processor.process (AbstractHttp11Processor.java:1081)</span><br><span class="line">at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process (AbstractProtocol.java:658)</span><br><span class="line">at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process (Http11NioProtocol.java:222)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun (NioEndpoint.java:1566)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run (NioEndpoint.java:1523)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run (TaskThread.java:61)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">Caused by: java.net.UnknownHostException: host40: Temporary failure in name resolution</span><br><span class="line">at java.net.Inet6AddressImpl.lookupAllHostAddr (Native Method)</span><br><span class="line">at java.net.InetAddress$2.lookupAllHostAddr (InetAddress.java:928)</span><br><span class="line">at java.net.InetAddress.getAddressesFromNameService (InetAddress.java:1323)</span><br><span class="line">at java.net.InetAddress.getAllByName0 (InetAddress.java:1276)</span><br><span class="line">at java.net.InetAddress.getAllByName (InetAddress.java:1192)</span><br><span class="line">at java.net.InetAddress.getAllByName (InetAddress.java:1126)</span><br><span class="line">at java.net.InetAddress.getByName (InetAddress.java:1076)</span><br><span class="line">at com.yyy.zzz.commons.infrastructure.client.EsClient.&lt;init&gt;(EsClient.java:43)</span><br><span class="line">... 64 more</span><br></pre></td></tr></table></figure><p>同时，在之后的请求中，只要是转发到这个服务节点的请求，就会出现如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">com.yyy.zzz.exception.hbase.HBaseException: java.lang.reflect.InvocationTargetException</span><br><span class="line">Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection (ConnectionFactory.java:240)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.createConnection (ConnectionManager.java:433)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.createConnection (ConnectionManager.java:426)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.getConnectionInternal (ConnectionManager.java:304)</span><br><span class="line">    at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:185)</span><br><span class="line">    at org.apache.hadoop.hbase.client.HTableFactory.createHTableInterface (HTableFactory.java:41)</span><br><span class="line">    ... 18 more</span><br></pre></td></tr></table></figure><p>通过排查代码，这个异常是在业务逻辑代码连接 <code>HBase</code> 表取数时出现的：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hTableInterface.get (List&lt;Get&gt;)</span><br></pre></td></tr></table></figure><p>每一次连接 <code>HBase</code> 取数，都会有这个异常出现。</p><h1 id="问题排查"><a href="# 问题排查" class="headerlink" title="问题排查"></a>问题排查 </h1><p> 首先怀疑的是 <code>protobuf</code> 版本冲突问题，但是通过对比，发现只有一个确定版本的 <code>jar</code> 包，而且对比其它节点，并没有这个问题出现，最终否定了这个猜测。</p><p>接着尝试发送多次请求，查看日志，以下错误不再出现【也很合理，这些异常是在服务注册初始化时只出现一次】：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil</span><br><span class="line">com.yyy.zzz.exception.es.EsConnException: java.net.UnknownHostException: host40: Temporary failure in name resolution</span><br></pre></td></tr></table></figure><p>反而出现的全部是 <code>HBase</code> 取数异常：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">com.yyy.zzz.exception.hbase.HBaseException: java.lang.reflect.InvocationTargetException</span><br><span class="line">Caused by: java.io.IOException: java.lang.reflect.InvocationTargetException</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionFactory.createConnection (ConnectionFactory.java:240)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.createConnection (ConnectionManager.java:433)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.createConnection (ConnectionManager.java:426)</span><br><span class="line">    at org.apache.hadoop.hbase.client.ConnectionManager.getConnectionInternal (ConnectionManager.java:304)</span><br><span class="line">    at org.apache.hadoop.hbase.client.HTable.&lt;init&gt;(HTable.java:185)</span><br><span class="line">    at org.apache.hadoop.hbase.client.HTableFactory.createHTableInterface (HTableFactory.java:41)</span><br><span class="line">    ... 18 more</span><br></pre></td></tr></table></figure><p>更神奇的是，只在一台节点上面有问题，其它相同功能的节点没问题。</p><p>最终，通过运维排查，从 <code>NoClassDefFoundError</code> 以及 <code>UnknownHostException</code> 发现了异常原因：在某个时间点发布服务时，恰好此时机器负载过高，导致 <code>DNS</code> 解析异常，于是 <code>dubbo</code> 服务在注册时无法获取 <code>hdfs</code> 信息。而 <code>HBase</code> 在初始化时需要依赖 <code>hdfs</code> 上面的某个 <code>hbase.version</code> 文件【用来确定 <code>HBase</code> 的版本】，导致 <code>HBase</code> 在初始化时无法找到这个文件，也就无法确定版本，最终没有加载 <code>ProtobufUtil</code> 类文件。</p><p><code>hdfs-site.xml</code> 配置文件中的重要内容如下，<code>nn1</code> 节点无法被识别：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;dfs.ha.namenodes.hdfs-cluster&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;nn1,nn2&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p><code>hbase-site.xml</code> 配置文件中的重要内容如下，对于 <code>HBase</code> 来说，这个 <code>hdfs</code> 路径里面存放着重要的信息，如果无法读取它也就无法成功初始化 <code>HBase</code> 环境：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;hbase.rootdir&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;hdfs://hdfs-cluster/apps/hbase/data&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure><p>所以此后所有的请求需要连接 <code>HBase</code> 取数时，都会出现 <code>java.lang.reflect.InvocationTargetException</code> 异常。</p><p>这里会进一步引发一个严重的问题，由于 <code>dubbo</code> 服务在注册时出现问题没有退出，仍旧提供服务，但是这个服务是有问题的，每次需要连接 <code>HBase</code> 取数时都会出现异常，由于没有处理好异常，导致大量的 <code>Zookeeper</code> 连接没有关闭。</p><p>进一步导致当前机器的 <code>Zookeeper</code> 连接数接近 1000 个，严重影响了其它业务连接 <code>Zookeeper</code>，一律是等待、超时重试。</p><h1 id="问题解决"><a href="# 问题解决" class="headerlink" title="问题解决"></a>问题解决 </h1><p> 找到问题原因，就很容易解决了，重启对应的服务，观察初始化日志，一切正常。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;背景说明：通过 &lt;code&gt;dubbo&lt;/code&gt; 部署一个服务，服务中的业务逻辑会查询 &lt;code&gt;HBase&lt;/code&gt; 表的数据，但是 &lt;code&gt;dubbo&lt;/code&gt; 服务在初始化注册时，&lt;code&gt;HBase&lt;/code&gt; 初始化的过程中会报错：&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;java.lang.NoClassDefFoundError: Could not initialize class org.apache.hadoop.hbase.protobuf.ProtobufUtil&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;本文涉及的开发环境，基于 &lt;code&gt;HBase v1.1.2&lt;/code&gt;、&lt;code&gt;Zookeeper v3.4.6&lt;/code&gt;、&lt;code&gt;dubbo v2.8.4&lt;/code&gt;、&lt;code&gt;Hadoop v2.7.1&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Hadoop" scheme="https://www.playpi.org/tags/Hadoop/"/>
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="Zookeeper" scheme="https://www.playpi.org/tags/Zookeeper/"/>
    
      <category term="dubbo" scheme="https://www.playpi.org/tags/dubbo/"/>
    
  </entry>
  
  <entry>
    <title>HBase 错误之 ConnectionLoss for hbase-unsecure</title>
    <link href="https://www.playpi.org/2019092901.html"/>
    <id>https://www.playpi.org/2019092901.html</id>
    <published>2019-09-29T13:11:53.000Z</published>
    <updated>2019-09-30T13:11:53.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --><p>在当前的业务中，需要连接 <code>HBase</code> 获取数据，但是最近在某一台节点上面的进程总是出现连接异常，类似下面：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">2019-09-20_18:54:44 [http-nio-28956-exec-5] WARN zookeeper.ZKUtil:629: hconnection-0x8a9f6680x0, quorum=host1:2181,host10:2181,host11:2181,host61:2181,host62:2181, baseZNode=/hbase-unsecure Unable to get data of znode /hbase-unsecure/meta-region-server</span><br><span class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/meta-region-server</span><br><span class="line">at org.apache.zookeeper.KeeperException.create (KeeperException.java:99)</span><br></pre></td></tr></table></figure><p>看起来是连接超时，然后重试，日志中持续了多次。本文开发环境基于 <code>HBase v1.1.2</code>、<code>Zookeeper v3.4.6</code>、<code>Hadoop v2.7.1</code>。</p><a id="more"></a><h1 id="问题出现"><a href="# 问题出现" class="headerlink" title="问题出现"></a>问题出现 </h1><p> 一个正常的连接 <code>HBase</code> 取数的服务，在某个节点上出现大量的异常日志，无法连接到 <code>HBase</code>，一直在重试，同时观察到在其它节点上相同的服务却是正常的。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br></pre></td><td class="code"><pre><span class="line">2019-09-20_18:54:44 [http-nio-28956-exec-5] ERROR zookeeper.RecoverableZooKeeper:277: ZooKeeper getData failed after 4 attempts</span><br><span class="line">2019-09-20_18:54:44 [http-nio-28956-exec-5] WARN zookeeper.ZKUtil:629: hconnection-0x8a9f6680x0, quorum=host1:2181,host10:2181,host11:2181,host61:2181,host62:2181, baseZNode=/hbase-unsecure Unable to get data of znode /hbase-unsecure/meta-region-server</span><br><span class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/meta-region-server</span><br><span class="line">at org.apache.zookeeper.KeeperException.create (KeeperException.java:99)</span><br><span class="line">at org.apache.zookeeper.KeeperException.create (KeeperException.java:51)</span><br><span class="line">at org.apache.zookeeper.ZooKeeper.getData (ZooKeeper.java:1155)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData (RecoverableZooKeeper.java:359)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData (ZKUtil.java:621)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState (MetaTableLocator.java:481)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation (MetaTableLocator.java:167)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:598)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:579)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:558)</span><br><span class="line">at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation (ZooKeeperRegistry.java:61)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta (ConnectionManager.java:1192)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1159)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations (RpcRetryingCallerWithReadReplicas.java:300)</span><br><span class="line">at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call (ScannerCallableWithReplicas.java:152)</span><br><span class="line">at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call (ScannerCallableWithReplicas.java:60)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries (RpcRetryingCaller.java:200)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.loadCache (ClientSmallReversedScanner.java:211)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.next (ClientSmallReversedScanner.java:185)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta (ConnectionManager.java:1256)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1162)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1146)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1103)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getRegionLocation (ConnectionManager.java:938)</span><br><span class="line">at org.apache.hadoop.hbase.client.HRegionLocator.getRegionLocation (HRegionLocator.java:83)</span><br><span class="line">at org.apache.hadoop.hbase.client.RegionServerCallable.prepare (RegionServerCallable.java:79)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries (RpcRetryingCaller.java:124)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:889)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:855)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:908)</span><br><span class="line">at com.xxx.yyy.commons.search.reader.hbase.BaseHBaseReader.batchGet (BaseHBaseReader.java:94)</span><br><span class="line">at com.xxx.yyy.commons.search.searcher.AbstractBaseSearcher.getContent (AbstractBaseSearcher.java:269)</span><br><span class="line">at com.xxx.yyy.commons.search.searcher.AbstractBaseSearcher.getInfo (AbstractBaseSearcher.java:194)</span><br><span class="line">at com.xxx.yyy.runner.search.BaseSearchRunner.combinaSearch (BaseSearchRunner.java:139)</span><br><span class="line">at com.xxx.yyy.api.newsforum.NewsForumPostServiceImpl.combinaSearch (NewsForumPostServiceImpl.java:53)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.Wrapper9.invokeMethod (Wrapper9.java)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory$1.doInvoke (JavassistProxyFactory.java:46)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.AbstractProxyInvoker.invoke (AbstractProxyInvoker.java:72)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.InvokerWrapper.invoke (InvokerWrapper.java:53)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ExceptionFilter.invoke (ExceptionFilter.java:64)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.monitor.support.MonitorFilter.invoke (MonitorFilter.java:75)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.TimeoutFilter.invoke (TimeoutFilter.java:42)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.dubbo.filter.TraceFilter.invoke (TraceFilter.java:78)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ContextFilter.invoke (ContextFilter.java:70)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.GenericFilter.invoke (GenericFilter.java:132)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ClassLoaderFilter.invoke (ClassLoaderFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.EchoFilter.invoke (EchoFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.InvokerInvocationHandler.invoke (InvokerInvocationHandler.java:52)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.proxy4.combinaSearch (proxy4.java)</span><br><span class="line">at sun.reflect.GeneratedMethodAccessor87.invoke (Unknown Source)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke (Method.java:498)</span><br><span class="line">at org.jboss.resteasy.core.MethodInjectorImpl.invoke (MethodInjectorImpl.java:137)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget (ResourceMethodInvoker.java:288)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:242)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:229)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:356)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:179)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.ServletContainerDispatcher.service (ServletContainerDispatcher.java:220)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:56)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:51)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.rest.DubboHttpServer$RestHandler.handle (DubboHttpServer.java:86)</span><br><span class="line">at com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet.service (DispatcherServlet.java:64)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:291)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter (ApplicationFilterChain.java:206)</span><br><span class="line">at org.apache.catalina.core.StandardWrapperValve.invoke (StandardWrapperValve.java:219)</span><br><span class="line">at org.apache.catalina.core.StandardContextValve.invoke (StandardContextValve.java:106)</span><br><span class="line">at org.apache.catalina.authenticator.AuthenticatorBase.invoke (AuthenticatorBase.java:504)</span><br><span class="line">at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:142)</span><br><span class="line">at org.apache.catalina.valves.ErrorReportValve.invoke (ErrorReportValve.java:79)</span><br><span class="line">at org.apache.catalina.core.StandardEngineValve.invoke (StandardEngineValve.java:88)</span><br><span class="line">at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:534)</span><br><span class="line">at org.apache.coyote.http11.AbstractHttp11Processor.process (AbstractHttp11Processor.java:1081)</span><br><span class="line">at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process (AbstractProtocol.java:658)</span><br><span class="line">at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process (Http11NioProtocol.java:222)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun (NioEndpoint.java:1566)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run (NioEndpoint.java:1523)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run (TaskThread.java:61)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">2019-09-20_18:54:45 [http-nio-28956-exec-5] ERROR zookeeper.ZooKeeperWatcher:655: hconnection-0x8a9f6680x0, quorum=host1:2181,host10:2181,host11:2181,host61:2181,host62:2181, baseZNode=/hbase-unsecure Received unexpected KeeperException, re-throwing exception</span><br><span class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/meta-region-server</span><br><span class="line">at org.apache.zookeeper.KeeperException.create (KeeperException.java:99)</span><br><span class="line">at org.apache.zookeeper.KeeperException.create (KeeperException.java:51)</span><br><span class="line">at org.apache.zookeeper.ZooKeeper.getData (ZooKeeper.java:1155)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.getData (RecoverableZooKeeper.java:359)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.ZKUtil.getData (ZKUtil.java:621)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionState (MetaTableLocator.java:481)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.getMetaRegionLocation (MetaTableLocator.java:167)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:598)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:579)</span><br><span class="line">at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable (MetaTableLocator.java:558)</span><br><span class="line">at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation (ZooKeeperRegistry.java:61)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateMeta (ConnectionManager.java:1192)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1159)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCallerWithReadReplicas.getRegionLocations (RpcRetryingCallerWithReadReplicas.java:300)</span><br><span class="line">at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call (ScannerCallableWithReplicas.java:152)</span><br><span class="line">at org.apache.hadoop.hbase.client.ScannerCallableWithReplicas.call (ScannerCallableWithReplicas.java:60)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithoutRetries (RpcRetryingCaller.java:200)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.loadCache (ClientSmallReversedScanner.java:211)</span><br><span class="line">at org.apache.hadoop.hbase.client.ClientSmallReversedScanner.next (ClientSmallReversedScanner.java:185)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegionInMeta (ConnectionManager.java:1256)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1162)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1146)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion (ConnectionManager.java:1103)</span><br><span class="line">at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getRegionLocation (ConnectionManager.java:938)</span><br><span class="line">at org.apache.hadoop.hbase.client.HRegionLocator.getRegionLocation (HRegionLocator.java:83)</span><br><span class="line">at org.apache.hadoop.hbase.client.RegionServerCallable.prepare (RegionServerCallable.java:79)</span><br><span class="line">at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries (RpcRetryingCaller.java:124)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:889)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:855)</span><br><span class="line">at org.apache.hadoop.hbase.client.HTable.get (HTable.java:908)</span><br><span class="line">at com.xxx.yyy.commons.search.reader.hbase.BaseHBaseReader.batchGet (BaseHBaseReader.java:94)</span><br><span class="line">at com.xxx.yyy.commons.search.searcher.AbstractBaseSearcher.getContent (AbstractBaseSearcher.java:269)</span><br><span class="line">at com.xxx.yyy.commons.search.searcher.AbstractBaseSearcher.getInfo (AbstractBaseSearcher.java:194)</span><br><span class="line">at com.xxx.yyy.runner.search.BaseSearchRunner.combinaSearch (BaseSearchRunner.java:139)</span><br><span class="line">at com.xxx.yyy.api.newsforum.NewsForumPostServiceImpl.combinaSearch (NewsForumPostServiceImpl.java:53)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.Wrapper9.invokeMethod (Wrapper9.java)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.javassist.JavassistProxyFactory$1.doInvoke (JavassistProxyFactory.java:46)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.AbstractProxyInvoker.invoke (AbstractProxyInvoker.java:72)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.InvokerWrapper.invoke (InvokerWrapper.java:53)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ExceptionFilter.invoke (ExceptionFilter.java:64)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.monitor.support.MonitorFilter.invoke (MonitorFilter.java:75)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.TimeoutFilter.invoke (TimeoutFilter.java:42)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.dubbo.filter.TraceFilter.invoke (TraceFilter.java:78)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ContextFilter.invoke (ContextFilter.java:70)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.GenericFilter.invoke (GenericFilter.java:132)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.ClassLoaderFilter.invoke (ClassLoaderFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.filter.EchoFilter.invoke (EchoFilter.java:38)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.ProtocolFilterWrapper$1.invoke (ProtocolFilterWrapper.java:91)</span><br><span class="line">at com.alibaba.dubbo.rpc.proxy.InvokerInvocationHandler.invoke (InvokerInvocationHandler.java:52)</span><br><span class="line">at com.alibaba.dubbo.common.bytecode.proxy4.combinaSearch (proxy4.java)</span><br><span class="line">at sun.reflect.GeneratedMethodAccessor87.invoke (Unknown Source)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke (Method.java:498)</span><br><span class="line">at org.jboss.resteasy.core.MethodInjectorImpl.invoke (MethodInjectorImpl.java:137)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invokeOnTarget (ResourceMethodInvoker.java:288)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:242)</span><br><span class="line">at org.jboss.resteasy.core.ResourceMethodInvoker.invoke (ResourceMethodInvoker.java:229)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:356)</span><br><span class="line">at org.jboss.resteasy.core.SynchronousDispatcher.invoke (SynchronousDispatcher.java:179)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.ServletContainerDispatcher.service (ServletContainerDispatcher.java:220)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:56)</span><br><span class="line">at org.jboss.resteasy.plugins.server.servlet.HttpServletDispatcher.service (HttpServletDispatcher.java:51)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at com.alibaba.dubbo.rpc.protocol.rest.DubboHttpServer$RestHandler.handle (DubboHttpServer.java:86)</span><br><span class="line">at com.alibaba.dubbo.remoting.http.servlet.DispatcherServlet.service (DispatcherServlet.java:64)</span><br><span class="line">at javax.servlet.http.HttpServlet.service (HttpServlet.java:790)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter (ApplicationFilterChain.java:291)</span><br><span class="line">at org.apache.catalina.core.ApplicationFilterChain.doFilter (ApplicationFilterChain.java:206)</span><br><span class="line">at org.apache.catalina.core.StandardWrapperValve.invoke (StandardWrapperValve.java:219)</span><br><span class="line">at org.apache.catalina.core.StandardContextValve.invoke (StandardContextValve.java:106)</span><br><span class="line">at org.apache.catalina.authenticator.AuthenticatorBase.invoke (AuthenticatorBase.java:504)</span><br><span class="line">at org.apache.catalina.core.StandardHostValve.invoke (StandardHostValve.java:142)</span><br><span class="line">at org.apache.catalina.valves.ErrorReportValve.invoke (ErrorReportValve.java:79)</span><br><span class="line">at org.apache.catalina.core.StandardEngineValve.invoke (StandardEngineValve.java:88)</span><br><span class="line">at org.apache.catalina.connector.CoyoteAdapter.service (CoyoteAdapter.java:534)</span><br><span class="line">at org.apache.coyote.http11.AbstractHttp11Processor.process (AbstractHttp11Processor.java:1081)</span><br><span class="line">at org.apache.coyote.AbstractProtocol$AbstractConnectionHandler.process (AbstractProtocol.java:658)</span><br><span class="line">at org.apache.coyote.http11.Http11NioProtocol$Http11ConnectionHandler.process (Http11NioProtocol.java:222)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.doRun (NioEndpoint.java:1566)</span><br><span class="line">at org.apache.tomcat.util.net.NioEndpoint$SocketProcessor.run (NioEndpoint.java:1523)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor.runWorker (ThreadPoolExecutor.java:1149)</span><br><span class="line">at java.util.concurrent.ThreadPoolExecutor$Worker.run (ThreadPoolExecutor.java:624)</span><br><span class="line">at org.apache.tomcat.util.threads.TaskThread$WrappingRunnable.run (TaskThread.java:61)</span><br><span class="line">at java.lang.Thread.run (Thread.java:748)</span><br><span class="line">2019-09-20_18:54:45 [http-nio-28956-exec-10-SendThread (host1:2181)] INFO zookeeper.ClientCnxn:975: Opening socket connection to server host1/192.168.20.101:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">2019-09-20_18:54:45 [http-nio-28956-exec-10-SendThread (host1:2181)] INFO zookeeper.ClientCnxn:852: Socket connection established to host1/192.168.20.101:2181, initiating session</span><br><span class="line">2019-09-20_18:54:45 [http-nio-28956-exec-10-SendThread (host1:2181)] WARN zookeeper.ClientCnxn:1102: Session 0x0 for server host1/192.168.20.101:2181, unexpected error, closing socket connection and attempting reconnect</span><br><span class="line">java.io.IOException: Connection reset by peer</span><br><span class="line">at sun.nio.ch.FileDispatcherImpl.read0 (Native Method)</span><br><span class="line">at sun.nio.ch.SocketDispatcher.read (SocketDispatcher.java:39)</span><br><span class="line">at sun.nio.ch.IOUtil.readIntoNativeBuffer (IOUtil.java:223)</span><br><span class="line">at sun.nio.ch.IOUtil.read (IOUtil.java:192)</span><br><span class="line">at sun.nio.ch.SocketChannelImpl.read (SocketChannelImpl.java:380)</span><br><span class="line">at org.apache.zookeeper.ClientCnxnSocketNIO.doIO (ClientCnxnSocketNIO.java:68)</span><br><span class="line">at org.apache.zookeeper.ClientCnxnSocketNIO.doTransport (ClientCnxnSocketNIO.java:366)</span><br><span class="line">at org.apache.zookeeper.ClientCnxn$SendThread.run (ClientCnxn.java:1081)</span><br></pre></td></tr></table></figure><p>注意查看重点内容：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2019-09-20_18:54:44 [http-nio-28956-exec-5] ERROR zookeeper.RecoverableZooKeeper:277: ZooKeeper getData failed after 4 attempts</span><br><span class="line">2019-09-20_18:54:44 [http-nio-28956-exec-5] WARN zookeeper.ZKUtil:629: hconnection-0x8a9f6680x0, quorum=host1:2181,host10:2181,host11:2181,host61:2181,host62:2181, baseZNode=/hbase-unsecure Unable to get data of znode /hbase-unsecure/meta-region-server</span><br><span class="line">org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/meta-region-server</span><br><span class="line">...</span><br><span class="line">2019-09-20_18:54:45 [http-nio-28956-exec-10-SendThread (host1:2181)] WARN zookeeper.ClientCnxn:1102: Session 0x0 for server host1/192.168.20.101:2181, unexpected error, closing socket connection and attempting reconnect</span><br></pre></td></tr></table></figure><p>看起来是当前节点网络有问题，或者 <code>Zookeeper</code> 连接资源紧张。</p><p>与此同时，还有大量类似下面这种连接重试出现：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-8-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e055f894, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-2-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e040a3c2, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-8-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e040a720, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-6-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x46d5ce1483b88cf, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-2-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e040a349, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-6-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e03f75aa, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br><span class="line">2019-09-30_00:24:56 [http-nio-28956-exec-4-SendThread (alps61:2181)] INFO zookeeper.ClientCnxn:1098: Unable to read additional data from server sessionid 0x16af866e040a8f0, likely server has closed socket, closing socket connection and </span><br><span class="line">attempting reconnect</span><br></pre></td></tr></table></figure><p>其实就是有进程在占用过多的 <code>Zookeeper</code> 连接，导致 <code>Zookeeper</code> 的 <code>Server</code> 端拒绝响应。</p><h1 id="问题排查"><a href="# 问题排查" class="headerlink" title="问题排查"></a>问题排查 </h1><p> 由于没有 <code>root</code> 权限，只能请运维帮忙排查，通过排查，发现当前主机创建的 <code>Zookeeper</code> 连接数过多，超过了设置的最大值。</p><p>使用 <code>netstat -antp | grep 2181 | wc -l</code> 命令，注意需要 <code>root</code> 用户的权限。这个命令统计的是所有 <code>Zookeeper</code> 连接【通过使用 2181 端口过滤】，包含等待的和正在通信的，如果查看正在通信的，加上一个 <code>grep ESTABLISHED</code> 过滤即可。</p><p>当然，如果机器开放了部分可以使用高级用户执行的命令【例如使用 <code>root</code> 用户执行 <code>pwdx</code>】，则更方便查看，操作时指定用户即可，例如：<code>sudo -u root netstat -antp | grep 2181 | wc -l</code>，可以查看端口 2181 占用的情况。</p><p>局部截图如下：</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190930214400.png" alt="zk 连接进程查看" title="zk 连接进程查看"></p><p>由于直接发现了问题，所以也不用进一步查看 <code>Zookeeper</code> 的日志了。</p><p>至于为什么 <code>Zookeeper</code> 的连接数会这么多，罪魁祸首请读者参考我的另外一篇博客：<br><a href="https://www.playpi.org/2019093001.html">HBase 错误之 NoClassDefFoundError：ProtobufUtil</a> 。</p><p>由于当前节点创建的 <code>Zookeeper</code> 连接数过多，所以再创建新连接时无法顺利连接通信，一直等待重试。</p><h1 id="问题解决"><a href="# 问题解决" class="headerlink" title="问题解决"></a>问题解决 </h1><p> 问题排查出来，解决就简单了，直接找到问题程序，修复资源泄漏问题，然后重启，保证合理的 <code>Zookeeper</code> 连接数量，不要因为某一个程序的失误而影响到其它业务。</p><p>另外如果有必要查看 <code>Zookeeper</code> 日志，需要特别留意 <code>Zookeeper</code> 查看日志的方法，日志文件是不能被直接打开的，需要工具转换为文本日志，然后才能查看分析。</p><p>关于 <code>Zookeeper</code> 日志的转换查看方法，可以参考我的另外一篇博客内容：<a href="https://www.playpi.org/2019092001.html">Zookeeper - 日志查看</a> 。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:18 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在当前的业务中，需要连接 &lt;code&gt;HBase&lt;/code&gt; 获取数据，但是最近在某一台节点上面的进程总是出现连接异常，类似下面：&lt;/p&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;2019-09-20_18:54:44 [http-nio-28956-exec-5] WARN zookeeper.ZKUtil:629: hconnection-0x8a9f6680x0, quorum=host1:2181,host10:2181,host11:2181,host61:2181,host62:2181, baseZNode=/hbase-unsecure Unable to get data of znode /hbase-unsecure/meta-region-server&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase-unsecure/meta-region-server&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	at org.apache.zookeeper.KeeperException.create (KeeperException.java:99)&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;&lt;p&gt;看起来是连接超时，然后重试，日志中持续了多次。本文开发环境基于 &lt;code&gt;HBase v1.1.2&lt;/code&gt;、&lt;code&gt;Zookeeper v3.4.6&lt;/code&gt;、&lt;code&gt;Hadoop v2.7.1&lt;/code&gt;。&lt;/p&gt;
    
    </summary>
    
      <category term="大数据技术知识" scheme="https://www.playpi.org/categories/big-data-technical-knowledge/"/>
    
    
      <category term="Hadoop" scheme="https://www.playpi.org/tags/Hadoop/"/>
    
      <category term="HBase" scheme="https://www.playpi.org/tags/HBase/"/>
    
      <category term="Zookeeper" scheme="https://www.playpi.org/tags/Zookeeper/"/>
    
  </entry>
  
  <entry>
    <title>爬山徒步：竹海丛林 - 广州第二峰鸡枕山</title>
    <link href="https://www.playpi.org/2019092201.html"/>
    <id>https://www.playpi.org/2019092201.html</id>
    <published>2019-09-22T08:03:49.000Z</published>
    <updated>2019-09-22T08:03:49.000Z</updated>
    
    <content type="html"><![CDATA[<!-- build time:Thu Jun 04 2020 22:25:17 GMT+0800 (China Standard Time) --><p>在 2019 年 9 月 21 日，我们几个小伙伴相约去广州鸡枕山登山徒步，给久坐的身体放松一下。<strong> 鸡枕山 </strong>位于广东省广州市从化区良口镇，海拔 1175 米，为广州市的第二高峰，仅次于天堂顶。另外有一点比较好的地方在于，鸡枕山虽然是广州第二高峰，但是它全程都被树木、竹林遮挡，基本不会被晒到，而且山路比较平缓，除了几处陡一点的路段，整体来说走起来非常舒适。</p><p>因此，这条线路非常适合没有登山经验的人，或者是平时缺少锻炼的人，或者体能差的人，这是一个比较好的入门路线。本文记录这次徒步的过程，给读者一个观察参考。</p><a id="more"></a><h1 id="集合"><a href="# 集合" class="headerlink" title="集合"></a>集合 </h1><p> 我们规定 07:00 在 <strong>客村 </strong>地铁站集合，签到、吃早餐、准备食物水等必需品。这时候有些人没有买够水的去买，没有买够食物的也去买，保证登山过程中能量补给。</p><h1 id="启程"><a href="# 启程" class="headerlink" title="启程"></a>启程 </h1><p>07:40，所有人到齐，准备出发，一个大巴装了 40 多人。</p><h1 id="到达"><a href="# 到达" class="headerlink" title="到达"></a> 到达 </h1><p>08:50 到达服务区，大家下车休息，还可以买水，09:00 继续出发。</p><p> 由于领队的麦克风坏掉了，所以没有进行互动，在过了服务区快要到达的时候，领队讲了一下注意事项。</p><p>在 09:50 到达山脚，做了一下热身，稍微休息几分钟，准备登山。</p><p>为了保留体力，避免运动过量，我们决定慢慢走，全程都是最后一批，其中一个领队就在我们后面。</p><h1 id="登山"><a href="# 登山" class="headerlink" title="登山"></a>登山 </h1><p>10:00 开始登山。</p><p> 在登山入口，我们的小队伍合影。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923005645.jpg" alt="山脚小合照" title="山脚小合照"></p><p>登山路过竹林，有人仰望天空，发现的心形竹林空隙。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923005740.jpg" alt="心形的竹林空隙" title="心形的竹林空隙"></p><p>走过竹林间的小路，看光影婆娑。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923005830.jpg" alt="竹林间的小路" title="竹林间的小路"></p><p>路过小水坝，领队抓拍到的如画风景，我是倒数第二个人。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923005941.jpg" alt="路过小水坝" title="路过小水坝"></p><p>这个小水坝是很小的，看看小水坝的蓄水池就知道了。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923010019.jpg" alt="小水坝的蓄水池" title="小水坝的蓄水池"></p><p>总是有人善于观察，看看路边的小花，斑驳的台阶。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923010102.jpg" alt="路边的小花" title="路边的小花"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923010113.jpg" alt="斑驳的台阶" title="斑驳的台阶"></p><p>路上少有的开阔视野，可以看看风景，远处的蓝蓝天空。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923010213.jpg" alt="蓝蓝的天空" title="蓝蓝的天空"></p><h1 id="午餐休息"><a href="# 午餐休息" class="headerlink" title="午餐休息"></a>午餐休息 </h1><p>12:00 到达登顶前的平台，可以休息吃午餐，养好体力准备登顶。</p><p> 领队竟然带了一个西瓜，背上来整整一个大西瓜，切开大家分了，竟然是冰冻的，爽口解渴。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011540.jpg" alt="切西瓜" title="切西瓜"></p><p>吃瓜群众，阳光洒在我全身，像个孩子。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011556.jpg" alt="吃瓜群众" title="吃瓜群众"></p><p>我带的西红柿，都给队友吃了。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011609.jpg" alt="吃西红柿" title="吃西红柿"></p><h1 id="登顶"><a href="# 登顶" class="headerlink" title="登顶"></a>登顶 </h1><p>12:40 开始登顶。</p><p> 最后有一段稍微陡峭的登顶路段，全程是暴露在阳光下的，只有灌木丛，慢慢登上去大概需要 20 分钟。</p><p>登顶大合照之一。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011649.jpg" alt="登顶大合照之一" title="登顶大合照之一"></p><p>登顶大合照之二。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011702.jpg" alt="登顶大合照之二" title="登顶大合照之二"></p><p>登顶小合照，我站在最高的石头上，眼睛被阳光照射，只能眯得很小。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011715.jpg" alt="登顶小合照" title="登顶小合照"></p><p>可以看到鸡枕山卫峰。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011731.jpg" alt="鸡枕山卫峰" title="鸡枕山卫峰"></p><p>山顶的平台被阳光直射，还是有点晒的，不过还好有风，感觉不是那么炎热，在上面遥看远方，风景如画，谈笑风生。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011904.jpg" alt="登顶远眺之一" title="登顶远眺之一"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011915.jpg" alt="登顶远眺之二" title="登顶远眺之二"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011924.jpg" alt="登顶远眺之三" title="登顶远眺之三"></p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923011934.jpg" alt="登顶远眺之四" title="登顶远眺之四"></p><h1 id="下山"><a href="# 下山" class="headerlink" title="下山"></a>下山 </h1><p>13:30 开始下山，一路小树林、竹林、灌木丛、缓坡。</p><p> 下山会有几段灌木丛的路，大概是下图这样，感觉像是玉米地，每次路过时双手挡脸，要防止被叶子刮伤。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923012043.jpg" alt="灌木丛" title="灌木丛"></p><p>下山遇到的竹林，不过此时已经很累，无心观赏。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923012346.jpg" alt="下山遇到的竹林" title="下山遇到的竹林"></p><p>仰望竹林。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923012433.jpg" alt="仰望竹林" title="仰望竹林"></p><p>阳光洒下，一片片翠绿金黄的竹叶；微风袭来，一阵阵扑面而来的凉爽。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923012128.jpg" alt="翠绿金黄的竹林" title="翠绿金黄的竹林"></p><p>竟然在山脚遇到了一群走地鸡，公鸡的羽毛特别漂亮。</p><p><img src="https://raw.githubusercontent.com/iplaypi/img-playpi/master/img/2019/20190923012211.jpg" alt="走地鸡" title="走地鸡"></p><h1 id="集合返程"><a href="# 集合返程" class="headerlink" title="集合返程"></a>集合返程 </h1><p> 在 16:30 到达山脚。</p><p>返程到山脚，有一些人很早就到了，等了两个多小时。</p><p>大家换衣服、洗脸、去厕所，休息一会，然后喝水、吃东西，聊聊天。</p><p>一切准备就绪后，大家集合，准备返程。</p><h1 id="归来"><a href="# 归来" class="headerlink" title="归来"></a>归来 </h1><p>17:00 开始返程。</p><p> 在下山的路上，由于是曲折蜿蜒的 S 型路线，而且路比较窄，所以堵车很严重，特别在转弯处，还需要下去人去指挥，毕竟安全第一。</p><p>本来 10 分钟可以走完的路花了将近 1 小时。</p><p>最后在 20:00 到达广州。</p><!-- rebuild by neat -->]]></content>
    
    <summary type="html">
    
      &lt;!-- build time:Thu Jun 04 2020 22:25:17 GMT+0800 (China Standard Time) --&gt;&lt;p&gt;在 2019 年 9 月 21 日，我们几个小伙伴相约去广州鸡枕山登山徒步，给久坐的身体放松一下。&lt;strong&gt; 鸡枕山 &lt;/strong&gt;位于广东省广州市从化区良口镇，海拔 1175 米，为广州市的第二高峰，仅次于天堂顶。另外有一点比较好的地方在于，鸡枕山虽然是广州第二高峰，但是它全程都被树木、竹林遮挡，基本不会被晒到，而且山路比较平缓，除了几处陡一点的路段，整体来说走起来非常舒适。&lt;/p&gt;&lt;p&gt;因此，这条线路非常适合没有登山经验的人，或者是平时缺少锻炼的人，或者体能差的人，这是一个比较好的入门路线。本文记录这次徒步的过程，给读者一个观察参考。&lt;/p&gt;
    
    </summary>
    
      <category term="游玩" scheme="https://www.playpi.org/categories/have-for-fun/"/>
    
    
      <category term="Guangzhou" scheme="https://www.playpi.org/tags/Guangzhou/"/>
    
      <category term="hike" scheme="https://www.playpi.org/tags/hike/"/>
    
      <category term="climbing" scheme="https://www.playpi.org/tags/climbing/"/>
    
  </entry>
  
</feed>
